<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>科学上网-Vultr搭建服务</title>
    <url>/2021/05/17/other/%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/</url>
    <content><![CDATA[<p>访问google在国内是被墙的，所以为了使用google搜索，不得不想办法翻墙上网。方法1：使用翻墙软件</p>
<p>方法2：自建VPN</p>
<p>方法1很不稳定，所以选择方法2</p>
<a id="more"></a>

<h3 id="购买Vultr服务器"><a href="#购买Vultr服务器" class="headerlink" title="购买Vultr服务器"></a>购买Vultr服务器</h3><p>自己上网使用的话，不需要太多流量内存，所以1月5$足够，1核/1G内存/25G硬盘存储。</p>
<p>要开服务器首先需要充值，最低10美元，支持支付宝。</p>
<p>充值后选择服务器。</p>
<p>Choose Server =&gt; Cloud Compute</p>
<p>Server Location(服务器地点) =&gt; New York(NJ)</p>
<p>Server Type =&gt; CentOS</p>
<p>Service Size =&gt; $5</p>
<p>部署即可</p>
<p>在Products部分的Server Information中，即可看到分配的IP地址，用户名，用户密码等信息</p>
<h3 id="服务器端安装软件"><a href="#服务器端安装软件" class="headerlink" title="服务器端安装软件"></a>服务器端安装软件</h3><p>这种自搭梯子的方式过程：</p>
<p>本机访问外网=&gt;本机Shadowsocks转发到购买的服务器=&gt;服务器访问外网=&gt;服务器通过Shadowsocks将内容返回本机</p>
<p>所以在本机和服务器上都要安装Shadowsocks软件</p>
<p>服务器上安装：</p>
<p>通过SecureCRT等远程连接软件登陆购买的服务器，然后安装</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget --no-check-certificate https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;teddysun&#x2F;shadowsocks_install&#x2F;master&#x2F;shadowsocksR.sh</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 修改权限</span><br><span class="line">chmod +x shadowsocksR.sh</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 安装并打日志</span><br><span class="line">.&#x2F;shadowsocksR.sh 2&gt;&amp;1 | tee shadowsocksR.log</span><br></pre></td></tr></table></figure>

<p>安装时选择如下选项：<br>Your Server IP : IP<br>Your Server Port : 端口（刚才设置的端口）<br>Your Password : 密码<br>Your Protocol : origin<br>Your obfs : plain<br>Your Encryption Method: aes-256-cfb（加密协议）</p>
<p>安装完成后，可以使用如下命令控制shadowsocks服务是否启动</p>
<p>启动：/etc/init.d/shadowsocks start<br>停止：/etc/init.d/shadowsocks stop<br>重启：/etc/init.d/shadowsocks restart<br>状态：/etc/init.d/shadowsocks status<br>配置文件路径：/etc/shadowsocks.json<br>日志文件路径：/var/log/shadowsocks.log<br>代码安装目录：/usr/local/shadowsocks</p>
<h3 id="客户端安装软件-ubuntu"><a href="#客户端安装软件-ubuntu" class="headerlink" title="客户端安装软件-ubuntu"></a>客户端安装软件-ubuntu</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo pip install shadowsocks</span><br></pre></td></tr></table></figure>

<p>然后为shadowsocks添加配置文件</p>
<p>/etc/shadowsocks/shadowsocks.json，内容如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;server&quot;:&quot;xx.xx.xx.xx&quot;,</span><br><span class="line">  &quot;server_port&quot;:17766,</span><br><span class="line">  &quot;local_address&quot;: &quot;127.0.0.1&quot;,</span><br><span class="line">  &quot;local_port&quot;:1080,</span><br><span class="line">  &quot;password&quot;:&quot;xx&quot;,</span><br><span class="line">  &quot;timeout&quot;:300,</span><br><span class="line">  &quot;method&quot;:&quot;aes-256-cfb&quot;,</span><br><span class="line">  &quot;fast_open&quot;: false</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>后台运行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo &#x2F;usr&#x2F;local&#x2F;bin&#x2F;sslocal -c &#x2F;etc&#x2F;shadowsocks&#x2F;shadowsocks.json -d start</span><br></pre></td></tr></table></figure>



<h3 id="客户端安装软件-mac"><a href="#客户端安装软件-mac" class="headerlink" title="客户端安装软件-mac"></a>客户端安装软件-mac</h3><p>可以直接安装代理软件**sock</p>
<h3 id="浏览器配置代理规则"><a href="#浏览器配置代理规则" class="headerlink" title="浏览器配置代理规则"></a>浏览器配置代理规则</h3><p>首先安装<a href="chrome-extension://padekgcemlokbadohgkifijomclgjgif/options.html#!/about">SwitchyOmega</a>软件。如果不能直接访问google扩展，可以先到github下载最新的chrome插件，然后安装。Mac直接拖拽安装如果产生错误，可以解压后放到一个文件夹，然后到chrome扩展下打开开发者模式，然后加载文件夹内容即可。</p>
<p>新建情景模式，代理协议选择SOCKS5, 代理服务器为本机，转发到本机配置的1080端口</p>
<p><img src="https://github.com/zcenao21/photos-blog/raw/main/other/vpn.png" alt="vpn.png"></p>
<p>保存后选择情景模式为vpn即可。</p>
<p>选择后因为所有流量都走了vpn，为了减少不必要的访问流量，部分网站可以使用原网络，配置方式是在<a href="chrome-extension://padekgcemlokbadohgkifijomclgjgif/options.html#!/about">SwitchyOmega</a>上配置auto switch，添加规则列表，选择AutoProxy，规则列表网址填写内容如下：</p>
<p><a href="https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt" target="_blank" rel="noopener">https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt</a></p>
<h3 id="问题解决"><a href="#问题解决" class="headerlink" title="问题解决"></a>问题解决</h3><p>1.shadowsocks服务端安装失败，不能运行python</p>
<p>因为shadowsocks需要python，所以要先安装python</p>
<p>2.[Errno 113] No route to host</p>
<p>出现如上错误，通过关闭服务器防火墙可以解决</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 查看防火墙状态</span><br><span class="line">systemctl status firewalld</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; 若上一个命令得到状态为Active，则通过下面的命令关闭</span><br><span class="line">systemctl stop firewalld</span><br></pre></td></tr></table></figure>





<blockquote>
<p>参考</p>
<p><a href="https://segmentfault.com/a/1190000009922582" target="_blank" rel="noopener">https://segmentfault.com/a/1190000009922582</a></p>
<p><a href="https://meiwencun.com/shadowsocksr_ssr_an_zhuang_387/" target="_blank" rel="noopener">https://meiwencun.com/shadowsocksr_ssr_an_zhuang_387/</a></p>
<p><a href="https://www.nb-fk.com/1047.html" target="_blank" rel="noopener">https://www.nb-fk.com/1047.html</a></p>
</blockquote>
]]></content>
      <categories>
        <category>other</category>
      </categories>
      <tags>
        <tag>科学上网</tag>
        <tag>Vultr</tag>
        <tag>翻墙</tag>
      </tags>
  </entry>
  <entry>
    <title>软件使用</title>
    <url>/2021/04/29/other/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<p>常用软件小技巧总结（包括操作系统）</p>
<a id="more"></a>


<h1 id="Unbuntu"><a href="#Unbuntu" class="headerlink" title="Unbuntu"></a>Unbuntu</h1><h3 id="锁屏失效"><a href="#锁屏失效" class="headerlink" title="锁屏失效"></a>锁屏失效</h3><p>桌面右上角打开设置</p>
<p><img src="https://github.com/zcenao21/photos-blog/raw/main/other/setting.png" alt="setting.png"></p>
<p>然后选择键盘，打开后</p>
<p><img src="https://github.com/zcenao21/photos-blog/raw/main/other/lock-screen.png" alt="lock-screen.png"></p>
<p>自定义快捷键，命令为</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">gnome-screensaver-command -l</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>other</category>
      </categories>
      <tags>
        <tag>软件</tag>
        <tag>Ubuntu</tag>
        <tag>办公软件</tag>
        <tag>EXCEL</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark使用</title>
    <url>/2021/04/21/spark/spark%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h3 id="Spark运行模式"><a href="#Spark运行模式" class="headerlink" title="Spark运行模式"></a>Spark运行模式</h3><p>Spark运行方式分为3种</p>
<ul>
<li>本地运行模式(单机)</li>
<li>本地伪集群运行模式(单机模拟集群)</li>
<li>集群运行模式</li>
</ul>
<p>本地模式一般用于初步调试或者学习，本地伪集群模式提供了真实集群运行模式的模拟环境。<a id="more"></a></p>
<h3 id="本地运行模式下的任务提交"><a href="#本地运行模式下的任务提交" class="headerlink" title="本地运行模式下的任务提交"></a>本地运行模式下的任务提交</h3><p>比如运行Spark提供的计算PI的例子</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;spark-submit --class org.apache.spark.examples.SparkPi --master &quot;local[2]&quot; ..&#x2F;examples&#x2F;target&#x2F;original-spark-examples_2.12-3.2.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>

<p>Spark 提供了 Web UI 来对 Spark 应用进行监控。每个 SparkContext 会启动一个 web UI，默认是在 4040 端口。它显示了应用相关的有用的信息，包括：</p>
<ul>
<li>一系列的 scheduler stage 和 task</li>
<li>RDD 大小和内存占用的概要</li>
<li>环境信息</li>
<li>正在运行的 executor 的一些信息</li>
</ul>
<p>但是由于上面的程序执行过程很快，可能来不及打开。或者执行完了怎么在Web上查看呢？我们可以使用spark history server的功能</p>
<p>首先设置conf/spark-defaults.conf(如果没有，拷贝conf/spark-defaults.conf.template)，参数如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spark.eventLog.enabled           true</span><br><span class="line">spark.eventLog.dir               &#x2F;home&#x2F;will&#x2F;study&#x2F;projects&#x2F;spark&#x2F;tmp&#x2F;spark-history-server</span><br><span class="line">spark.history.fs.logDirectory    file:&#x2F;home&#x2F;will&#x2F;study&#x2F;projects&#x2F;spark&#x2F;tmp&#x2F;spark-history-server</span><br></pre></td></tr></table></figure>

<p>设置成功后运行history server，具体操作是执行sbin/start-history-server.sh，启动history server。启动后访问<a href="http://localhost:18080/即可看到执行历史" target="_blank" rel="noopener">http://localhost:18080/即可看到执行历史</a></p>
<p><img src="https://github.com/zcenao21/photos-blog/raw/main/spark/history-server.png" alt=""></p>
<h3 id="client模式"><a href="#client模式" class="headerlink" title="client模式"></a>client模式</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;spark-submit --class org.apache.spark.examples.SparkPi --master spark:&#x2F;&#x2F;localhost:7077 ..&#x2F;examples&#x2F;target&#x2F;original-spark-examples_2.12-3.2.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>



<h3 id="Debug"><a href="#Debug" class="headerlink" title="Debug"></a>Debug</h3><p>启动之后如果想看进行单步调试，可以进行如下操作</p>
<p>首先启动时设置远程监听</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;spark-submit --class org.apache.spark.examples.SparkPi --master spark:&#x2F;&#x2F;localhost:7077 --conf spark.driver.extraJavaOptions&#x3D;&quot;-agentlib:jdwp&#x3D;transport&#x3D;dt_socket,server&#x3D;y,suspend&#x3D;y,address&#x3D;7777&quot; ..&#x2F;examples&#x2F;target&#x2F;original-spark-examples_2.12-3.2.0-SNAPSHOT.jar</span><br></pre></td></tr></table></figure>

<p>然后在IDEA中设置调试参数</p>
<p><img src="https://github.com/zcenao21/photos-blog/raw/main/spark/spark-submit-debug.png" alt="spark-submit-debug.png"></p>
<p>-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=7777</p>
<p>再在需要的地方打断点</p>
<p><img src="https://github.com/zcenao21/photos-blog/raw/main/spark/debug-breakpoint.png" alt="debug-breakpoint.png"></p>
<h3 id="任务提交"><a href="#任务提交" class="headerlink" title="任务提交"></a>任务提交</h3><p>通过spark-submit提交任务，spark-submit所做的工作是设置一些环境参数，比如检测Java安装目录，设定SPARK_HOME变量等，然后提交运行任务</p>
<p>spark 3.2 版本spark-submit内容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;usr&#x2F;bin&#x2F;env bash</span><br><span class="line"></span><br><span class="line"># 若没有设定SPARK_HOME则使用脚本查找设置</span><br><span class="line">if [ -z &quot;$&#123;SPARK_HOME&#125;&quot; ]; then</span><br><span class="line">  source &quot;$(dirname &quot;$0&quot;)&quot;&#x2F;find-spark-home</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"># disable randomized hash for string in Python 3.3+</span><br><span class="line">export PYTHONHASHSEED&#x3D;0</span><br><span class="line"></span><br><span class="line">exec &quot;$&#123;SPARK_HOME&#125;&quot;&#x2F;bin&#x2F;spark-class org.apache.spark.deploy.SparkSubmit &quot;$@&quot;</span><br></pre></td></tr></table></figure>

<p>设置SPARK_HOME目录后运行脚本spark-class，这个脚本主要用于设置JAVA_HOME(Java环境)及LAUNCH_CLASSPATH(spark运行依赖的jar包)。</p>
<p>最终调用org.apache.spark.deploy.SparkSubmit这个类，提交任务。</p>
<p>在org.apache.spark.deploy.SparkSubmit这个类中，调用doSubmit方法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">override def main(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">    val submit &#x3D; new SparkSubmit() &#123;</span><br><span class="line">      self &#x3D;&gt;</span><br><span class="line">      ... &#x2F;&#x2F; 省略部分代码</span><br><span class="line">      override def doSubmit(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">        try &#123;</span><br><span class="line">          super.doSubmit(args)</span><br><span class="line">        &#125; catch &#123;</span><br><span class="line">          case e: SparkUserAppException &#x3D;&gt;</span><br><span class="line">            exitFn(e.exitCode)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    submit.doSubmit(args)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>在doSubmit方法中,再选择调用submit(appArgs, uninitLog)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def doSubmit(args: Array[String]): Unit &#x3D; &#123;</span><br><span class="line">    &#x2F;&#x2F; Initialize logging if it hasn&#39;t been done yet. Keep track of whether logging needs to</span><br><span class="line">    &#x2F;&#x2F; be reset before the application starts.</span><br><span class="line">    val uninitLog &#x3D; initializeLogIfNecessary(true, silent &#x3D; true)</span><br><span class="line"></span><br><span class="line">    val appArgs &#x3D; parseArguments(args)</span><br><span class="line">    if (appArgs.verbose) &#123;</span><br><span class="line">      logInfo(appArgs.toString)</span><br><span class="line">    &#125;</span><br><span class="line">    appArgs.action match &#123;</span><br><span class="line">      case SparkSubmitAction.SUBMIT &#x3D;&gt; submit(appArgs, uninitLog)</span><br><span class="line">      case SparkSubmitAction.KILL &#x3D;&gt; kill(appArgs)</span><br><span class="line">      case SparkSubmitAction.REQUEST_STATUS &#x3D;&gt; requestStatus(appArgs)</span><br><span class="line">      case SparkSubmitAction.PRINT_VERSION &#x3D;&gt; printVersion()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>在submit方法中，又选择调用了doRunMain()方法，接着调用runMain(args, uninitLog)方法。接下来</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val app: SparkApplication &#x3D; if (classOf[SparkApplication].isAssignableFrom(mainClass)) &#123;</span><br><span class="line">   mainClass.getConstructor().newInstance().asInstanceOf[SparkApplication]</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">      new JavaMainApplication(mainClass)</span><br><span class="line">    &#125;</span><br><span class="line">... &#x2F;&#x2F; 省略代码</span><br><span class="line">try &#123;</span><br><span class="line">	app.start(childArgs.toArray, sparkConf)</span><br><span class="line">&#125; catch &#123;</span><br><span class="line">	case t: Throwable &#x3D;&gt; throw findCause(t)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在app.start中，反射调用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mainMethod.invoke(null, args)</span><br></pre></td></tr></table></figure>

<p>于是调用了我们自己的spark程序，以PI计算程序为例</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">val spark &#x3D; SparkSession</span><br><span class="line">                .builder</span><br><span class="line">                .appName(&quot;Spark Pi&quot;)</span><br><span class="line">                .getOrCreate()</span><br></pre></td></tr></table></figure>

<p>在getOrCreate()方法中</p>
<h3 id="Master程序调试"><a href="#Master程序调试" class="headerlink" title="Master程序调试"></a>Master程序调试</h3><p>版本：３.2.0-SNAPSHOT</p>
<p>在start-master.sh文件中添加如下参数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export SPARK_MASTER_OPTS&#x3D;&quot;-Xdebug -Xrunjdwp:transport&#x3D;dt_socket,server&#x3D;y,suspend&#x3D;y,address&#x3D;10000&quot;</span><br></pre></td></tr></table></figure>

<p>然后在运行start-master.sh，运行后在命令行执行<code>jps</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">8817 Main</span><br><span class="line">753 Jps</span><br><span class="line">623 -- main class information unavailable</span><br></pre></td></tr></table></figure>

<p>再在idea端增加remote配置，端口配置为上述参数中的10000，use module classpath选择spark-core_2.12。之后在org.apache.spark.deploy.master.Master中的main函数上打断点。最后点击Debug开始调试</p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>Spark使用</tag>
      </tags>
  </entry>
  <entry>
    <title>RDD内幕</title>
    <url>/2021/04/17/spark/RDD%E5%86%85%E5%B9%95/</url>
    <content><![CDATA[<h1 id="RDD分区"><a href="#RDD分区" class="headerlink" title="RDD分区"></a>RDD分区</h1><p>RDD内部使用分区表示并行计算的一个计算单元，因此分区和并发计算有关系。RDD的分区源码：<a id="more"></a> </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">package org.apache.spark</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * An identifier for a partition in an RDD.</span><br><span class="line"> *&#x2F;</span><br><span class="line">trait Partition extends Serializable &#123;</span><br><span class="line">  &#x2F;**</span><br><span class="line">   * Get the partition&#39;s index within its parent RDD</span><br><span class="line">   *&#x2F;</span><br><span class="line">  def index: Int</span><br><span class="line"></span><br><span class="line">  &#x2F;&#x2F; A better default implementation of HashCode</span><br><span class="line">  override def hashCode(): Int &#x3D; index</span><br><span class="line"></span><br><span class="line">  override def equals(other: Any): Boolean &#x3D; super.equals(other)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在org.apache.spark.rdd.RDD中，定义了getPartitions方法，可以获取分区。</p>
<p>分区个数如何决定呢？</p>
<p>例如map操作得到的RDD分区由第一个父RDD分区决定</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">override def getPartitions: Array[Partition] &#x3D; firstParent[T].partitions</span><br></pre></td></tr></table></figure>

<p>再比如parallize操作的分区个数由用户指定，默认为集群核心数目</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;** Distribute a local Scala collection to form an RDD.</span><br><span class="line">   *</span><br><span class="line">   * @note Parallelize acts lazily. If &#96;seq&#96; is a mutable collection and is altered after the call</span><br><span class="line">   * to parallelize and before the first action on the RDD, the resultant RDD will reflect the</span><br><span class="line">   * modified collection. Pass a copy of the argument to avoid this.</span><br><span class="line">   * @note avoid using &#96;parallelize(Seq())&#96; to create an empty &#96;RDD&#96;. Consider &#96;emptyRDD&#96; for an</span><br><span class="line">   * RDD with no partitions, or &#96;parallelize(Seq[T]())&#96; for an RDD of &#96;T&#96; with empty partitions.</span><br><span class="line">   * @param seq Scala collection to distribute</span><br><span class="line">   * @param numSlices number of partitions to divide the collection into</span><br><span class="line">   * @return RDD representing distributed collection</span><br><span class="line">   *&#x2F;</span><br><span class="line">  def parallelize[T: ClassTag](</span><br><span class="line">      seq: Seq[T],</span><br><span class="line">      numSlices: Int &#x3D; defaultParallelism): RDD[T] &#x3D; withScope &#123;</span><br><span class="line">    assertNotStopped()</span><br><span class="line">    new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>defaultParallelism的设置，可以看LocalSchedulerBackend源码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">override def defaultParallelism(): Int &#x3D;</span><br><span class="line">    scheduler.k.getInt(&quot;spark.default.parallelism&quot;, totalCores)</span><br></pre></td></tr></table></figure>




<blockquote>
<p>参考<br><a href="https://ihainan.gitbooks.io/spark-source-code/content/section1/rddPartitions.html" target="_blank" rel="noopener">https://ihainan.gitbooks.io/spark-source-code/content/section1/rddPartitions.html</a></p>
</blockquote>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>RDD</tag>
        <tag>RDD内幕</tag>
      </tags>
  </entry>
  <entry>
    <title>算法常用模板</title>
    <url>/2021/04/13/other/%E7%AE%97%E6%B3%95%E5%B8%B8%E7%94%A8%E6%A8%A1%E6%9D%BF/</url>
    <content><![CDATA[<p>常用的算法模板，比如快排</p>
<a id="more"></a>

<h3 id="快排升序"><a href="#快排升序" class="headerlink" title="快排升序"></a>快排升序</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public void fastSort(int[] nums, int start, int end)&#123;</span><br><span class="line">    if(start&gt;&#x3D;end) return;</span><br><span class="line">    int mid &#x3D; start;</span><br><span class="line">    for(int i&#x3D;start+1;i&lt;&#x3D;end;i++)&#123;</span><br><span class="line">        if(nums[mid]&gt;nums[i]))&#123;</span><br><span class="line">            int temp &#x3D; nums[i];</span><br><span class="line">            for(int j&#x3D;i;j&gt;mid;j--)&#123;</span><br><span class="line">                nums[j] &#x3D; nums[j-1];</span><br><span class="line">            &#125;</span><br><span class="line">            nums[mid]&#x3D;temp;</span><br><span class="line">            mid &#x3D; mid+1;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    fastSort(nums, start, mid-1);</span><br><span class="line">    fastSort(nums, mid+1, end);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>other</category>
      </categories>
      <tags>
        <tag>算法模板</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>ReentrantLock</title>
    <url>/2021/03/27/java/Java%E5%B9%B6%E5%8F%91%EF%BC%8DReentrantLock/</url>
    <content><![CDATA[<h1 id="Synchronized"><a href="#Synchronized" class="headerlink" title="Synchronized"></a>Synchronized</h1><p>synchronized是java语言底层实现，我们不需要考虑因为它产生的异常，但是也存在不够灵活，比较重等缺点。<a id="more"></a></p>
<p>下面使用synchronized实现一个生产消费队列：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.Random;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WaitAndNotify</span> </span>&#123;</span><br><span class="line">    <span class="keyword">static</span> Random random = <span class="keyword">new</span> Random();</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Runnable target;</span><br><span class="line">        <span class="keyword">final</span> TaskQueueTest test = <span class="keyword">new</span> TaskQueueTest();</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;<span class="number">3</span>;i++)&#123;</span><br><span class="line">            <span class="keyword">final</span> <span class="keyword">int</span> finalI = i;</span><br><span class="line">            Thread producer = <span class="keyword">new</span> Thread()&#123;</span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span></span>&#123;</span><br><span class="line">                    <span class="keyword">try</span>&#123;</span><br><span class="line">                        <span class="keyword">while</span>(<span class="keyword">true</span>)&#123;</span><br><span class="line">                            test.putTask(finalI+<span class="string">""</span>);</span><br><span class="line">                            sleep(<span class="number">900</span>+random.nextInt(<span class="number">900</span>));</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;<span class="keyword">catch</span> (Exception e)&#123;&#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;;</span><br><span class="line">            producer.setDaemon(<span class="keyword">true</span>);</span><br><span class="line">            producer.start();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;<span class="number">3</span>;j++)&#123;</span><br><span class="line">            <span class="keyword">final</span> <span class="keyword">int</span> finalJ = j;</span><br><span class="line">            Thread consumer = <span class="keyword">new</span> Thread()&#123;</span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span></span>&#123;</span><br><span class="line">                    <span class="keyword">try</span>&#123;</span><br><span class="line">                        <span class="keyword">while</span>(<span class="keyword">true</span>)&#123;</span><br><span class="line">                            test.exeTask(finalJ);</span><br><span class="line">                            sleep(<span class="number">900</span>+random.nextInt(<span class="number">900</span>));</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;<span class="keyword">catch</span> (Exception e)&#123;&#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;;</span><br><span class="line">            consumer.setDaemon(<span class="keyword">true</span>);</span><br><span class="line">            consumer.start();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        Thread.sleep(<span class="number">5000</span>);</span><br><span class="line">        <span class="keyword">while</span>(test.getTasksCnt()!=<span class="number">0</span>)&#123;</span><br><span class="line">            Thread.sleep(<span class="number">100</span>);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        System.out.println(System.currentTimeMillis()+<span class="string">": main thread done"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TaskQueueTest</span></span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> cnt = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">private</span> ArrayList&lt;String&gt; taskList = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getTasksCnt</span><span class="params">()</span></span>&#123;</span><br><span class="line">        <span class="keyword">return</span> taskList.size();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">putTask</span><span class="params">(String taskName)</span></span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"User "</span> + taskName + <span class="string">" put task "</span> + cnt);</span><br><span class="line">        taskList.add(cnt++ + <span class="string">""</span>);</span><br><span class="line">        <span class="keyword">this</span>.notify();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">synchronized</span> <span class="keyword">void</span> <span class="title">exeTask</span><span class="params">(<span class="keyword">int</span> taskNumber)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">while</span>(taskList.isEmpty())&#123;</span><br><span class="line">            <span class="keyword">this</span>.wait();</span><br><span class="line">        &#125;</span><br><span class="line">        System.out.println(<span class="string">"Consumer "</span> + taskNumber + <span class="string">" consume task "</span> + taskList.get(<span class="number">0</span>));</span><br><span class="line">        taskList.remove(<span class="number">0</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">User 1 put task 0</span><br><span class="line">User 2 put task 1</span><br><span class="line">Consumer 0 consume task 0</span><br><span class="line">Consumer 1 consume task 1</span><br><span class="line">User 0 put task 2</span><br><span class="line">Consumer 2 consume task 2</span><br><span class="line">User 1 put task 3</span><br><span class="line">Consumer 0 consume task 3</span><br><span class="line">User 0 put task 4</span><br><span class="line">Consumer 1 consume task 4</span><br><span class="line">User 2 put task 5</span><br><span class="line">Consumer 2 consume task 5</span><br><span class="line">User 0 put task 6</span><br><span class="line">User 1 put task 7</span><br><span class="line">User 2 put task 8</span><br><span class="line">Consumer 0 consume task 6</span><br><span class="line">Consumer 2 consume task 7</span><br><span class="line">Consumer 1 consume task 8</span><br><span class="line">User 0 put task 9</span><br><span class="line">Consumer 1 consume task 9</span><br><span class="line">User 2 put task 10</span><br><span class="line">Consumer 0 consume task 10</span><br><span class="line">User 1 put task 11</span><br><span class="line">Consumer 2 consume task 11</span><br><span class="line">1616810856277: main thread done</span><br></pre></td></tr></table></figure>



<h1 id="ReentrantLock"><a href="#ReentrantLock" class="headerlink" title="ReentrantLock"></a>ReentrantLock</h1><p>ReentrantLock是java语言实现的，相对于synchronized较轻量，且更加灵活。同样实现上面的生产消费模型：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import java.util.ArrayList;</span><br><span class="line">import java.util.Random;</span><br><span class="line">import java.util.concurrent.locks.Condition;</span><br><span class="line">import java.util.concurrent.locks.ReentrantLock;</span><br><span class="line"></span><br><span class="line">public class WaitAndNotify2 &#123;</span><br><span class="line">    static Random random &#x3D; new Random();</span><br><span class="line">    public static void main(String[] args) throws InterruptedException &#123;</span><br><span class="line">        Runnable target;</span><br><span class="line">        final TaskQueueTest2 test &#x3D; new TaskQueueTest2();</span><br><span class="line">        for(int i&#x3D;0;i&lt;3;i++)&#123;</span><br><span class="line">            final int finalI &#x3D; i;</span><br><span class="line">            Thread producer &#x3D; new Thread()&#123;</span><br><span class="line">                public void run()&#123;</span><br><span class="line">                    try&#123;</span><br><span class="line">                        while(true)&#123;</span><br><span class="line">                            test.putTask(finalI+&quot;&quot;);</span><br><span class="line">                            sleep(900+random.nextInt(900));</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;catch (Exception e)&#123;&#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;;</span><br><span class="line">            producer.setDaemon(true);</span><br><span class="line">            producer.start();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        for(int j&#x3D;0;j&lt;3;j++)&#123;</span><br><span class="line">            final int finalJ &#x3D; j;</span><br><span class="line">            Thread consumer &#x3D; new Thread()&#123;</span><br><span class="line">                public void run()&#123;</span><br><span class="line">                    try&#123;</span><br><span class="line">                        while(true)&#123;</span><br><span class="line">                            test.exeTask(finalJ);</span><br><span class="line">                            sleep(900+random.nextInt(900));</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;catch (Exception e)&#123;&#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;;</span><br><span class="line">            consumer.setDaemon(true);</span><br><span class="line">            consumer.start();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        Thread.sleep(5000);</span><br><span class="line">        while(test.getTasksCnt()!&#x3D;0)&#123;</span><br><span class="line">            Thread.sleep(100);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        System.out.println(System.currentTimeMillis()+&quot;: main thread done&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">class TaskQueueTest2&#123;</span><br><span class="line">    private int cnt &#x3D; 0;</span><br><span class="line">    private ArrayList&lt;String&gt; taskList &#x3D; new ArrayList&lt;String&gt;();</span><br><span class="line">    ReentrantLock lock &#x3D; new ReentrantLock();</span><br><span class="line">    Condition condition &#x3D; lock.newCondition();</span><br><span class="line"></span><br><span class="line">    public int getTasksCnt()&#123;</span><br><span class="line">        return taskList.size();</span><br><span class="line">    &#125;</span><br><span class="line">    public void putTask(String taskName)&#123;</span><br><span class="line">        lock.lock();</span><br><span class="line">        try&#123;</span><br><span class="line">            System.out.println(&quot;User &quot; + taskName + &quot; put task &quot; + cnt);</span><br><span class="line">            taskList.add(cnt++ + &quot;&quot;);</span><br><span class="line">            condition.signal();</span><br><span class="line">        &#125;finally &#123;</span><br><span class="line">            lock.unlock();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public synchronized void exeTask(int taskNumber) throws InterruptedException &#123;</span><br><span class="line">        lock.lock();</span><br><span class="line">        try&#123;</span><br><span class="line">            while(taskList.isEmpty())&#123;</span><br><span class="line">                condition.await();</span><br><span class="line">            &#125;</span><br><span class="line">            System.out.println(&quot;Consumer &quot; + taskNumber + &quot; consume task &quot; + taskList.get(0));</span><br><span class="line">            taskList.remove(0);</span><br><span class="line">        &#125;finally &#123;</span><br><span class="line">            lock.unlock();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">User 0 put task 0</span><br><span class="line">User 1 put task 1</span><br><span class="line">User 2 put task 2</span><br><span class="line">Consumer 0 consume task 0</span><br><span class="line">Consumer 1 consume task 1</span><br><span class="line">Consumer 2 consume task 2</span><br><span class="line">User 1 put task 3</span><br><span class="line">Consumer 0 consume task 3</span><br><span class="line">User 0 put task 4</span><br><span class="line">Consumer 2 consume task 4</span><br><span class="line">User 2 put task 5</span><br><span class="line">Consumer 1 consume task 5</span><br><span class="line">User 1 put task 6</span><br><span class="line">Consumer 0 consume task 6</span><br><span class="line">User 2 put task 7</span><br><span class="line">Consumer 2 consume task 7</span><br><span class="line">User 0 put task 8</span><br><span class="line">Consumer 1 consume task 8</span><br><span class="line">User 1 put task 9</span><br><span class="line">Consumer 0 consume task 9</span><br><span class="line">User 0 put task 10</span><br><span class="line">Consumer 2 consume task 10</span><br><span class="line">User 2 put task 11</span><br><span class="line">Consumer 1 consume task 11</span><br><span class="line">1616812412381: main thread done</span><br></pre></td></tr></table></figure>





























]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>ReentrantLock</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop总结</title>
    <url>/2021/03/05/hadoop/hadoop/</url>
    <content><![CDATA[<h1 id="序言"><a href="#序言" class="headerlink" title="序言"></a>序言</h1><p>Hadoop框架包含计算和存储两部分，MapReduce实现计算部分，存储则是HDFS。大数据在map reduce思想基石上，形成了一整套Hadoop生态系统。了解map reduce还是相当重要的。<a id="more"></a></p>
<h1 id="Hadoop发展史"><a href="#Hadoop发展史" class="headerlink" title="Hadoop发展史"></a>Hadoop发展史</h1><p>为什么取名Hadoop，Hadoop其实是一个造出来的词，来源于Doug Cutting儿子毛绒玩具象。我们就看看Hadoop的发展历程。</p>
<ul>
<li>2003-2004 Google公布GFS和MapReduce思想的两篇论文，受此启发，Doug Cutting等人用2年时间实现了DFS和MapReduce机制。</li>
<li>2005　Hadoop作为Lucene的子项目Nutch的一部分正式引入Apache基金会。</li>
<li>2006.2 被分离.出来，成为一套完整独立的软件，起名为Hadoop</li>
<li>2012.5 发布２.0版本，抽象出单独的资源管理模块Yarn进行资源管理。</li>
</ul>
<p>所以从根源上说，MapReduce思想来源于Google的论文，Doug Cutting等人做了开源的工程实现。</p>
<h1 id="Hadoop架构"><a href="#Hadoop架构" class="headerlink" title="Hadoop架构"></a>Hadoop架构</h1><p><img src="https://raw.githubusercontent.com/zcenao21/photos-blog/main/hadoop/hadoop-structure.png" alt="hadoop-structure"></p>
<h3 id="NameNode"><a href="#NameNode" class="headerlink" title="NameNode"></a>NameNode</h3><p>NameNode管理文件系统的命名空间，维护文件系统树和整棵树内的所有文件和目录。这些信息是以两个文件形式永久保存在本地磁盘上，分别是命名空间镜像文件(fsimage)和编辑日志文件(edit logs)。</p>
<p>NameNode存储信息列表：</p>
<ul>
<li>文件系统的命名空间，文件名称，文件目录结构，文件的属性(权限/创建时间/副本数)</li>
<li>文件对应哪些数据块，数据块存储在哪些datanode节点上</li>
</ul>
<p>NameNode在启动时，会先将fsimage加载进内存，然后与edit logs合并更新到最新状态。在hadoop1.x版本中，启动之后产生的新的edit logs由Secondary NameNode定期和fsimage合并并更新到NameNode当中。在hadoop2.x之后，增加了NameNode HA（高可用）功能，合并的工作由Secondary NameNode转移到了Standy NameNode上。另外，Secondary NameNode可以在NameNode出现故障时快速切换。</p>
<h3 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h3><p>DataNode是实际的数据存储模块。NameNode仅用于存放元数据信息，实际的数据信息都放在DataNode上。DataNode存储数据块和数据块校验和，和客户端进行数据通信，DataNode之间也进行数据传输。</p>
<p>DataNode和NameNode之间通过心跳机制联系。每隔３秒DataNode发送一次心跳，心跳结果返回NameNode的命令如块的复制/块的删除等。</p>
<p>如果十分钟未收到DataNode的心跳，则认为DataNode丢失，它上面的block会拷贝到其他DataNode上。</p>
<p>#　Hadoop配置</p>
<h3 id="配置文件及其作用"><a href="#配置文件及其作用" class="headerlink" title="配置文件及其作用"></a>配置文件及其作用</h3><p><img src="https://github.com/zcenao21/photos-blog/raw/main/hadoop/file-func.jpg" alt="img"></p>
<p><strong>core-site.xml</strong></p>
<table>
<thead>
<tr>
<th>配置项</th>
<th>作用</th>
</tr>
</thead>
<tbody><tr>
<td>fs.defaultFS</td>
<td>配置namenode的dfs协议的文件系统通信地址</td>
</tr>
<tr>
<td>hadoop.tmp.dir</td>
<td>指定hadoop临时目录</td>
</tr>
</tbody></table>
<p><strong>hdfs-site.xml</strong></p>
<table>
<thead>
<tr>
<th>作用</th>
<th>配置项</th>
</tr>
</thead>
<tbody><tr>
<td>dfs.namenode.name.dir</td>
<td>namenode数据的存放地点</td>
</tr>
<tr>
<td>dfs.datanode.data.dir</td>
<td>datanode数据的存放地点</td>
</tr>
<tr>
<td>dfs.replication</td>
<td>hdfs副本数量设置</td>
</tr>
</tbody></table>
<p><strong>yarn-site.xml</strong></p>
<table>
<thead>
<tr>
<th>作用</th>
<th>配置项</th>
</tr>
</thead>
<tbody><tr>
<td>yarn.resourcemanager.address</td>
<td>ResourceManager 对客户端暴露的地址</td>
</tr>
<tr>
<td>yarn.resourcemanager.scheduler.address</td>
<td>ResourceManager 对ApplicationMaster暴露的访问地址</td>
</tr>
<tr>
<td>yarn.resourcemanager.resource-tracker.address</td>
<td>ResourceManager 对NodeManager暴露的地址</td>
</tr>
<tr>
<td>yarn.resourcemanager.admin.address</td>
<td>ResourceManager 对管理员暴露的访问地址</td>
</tr>
</tbody></table>
<p><strong>mapred-site.xml</strong></p>
<table>
<thead>
<tr>
<th>作用</th>
<th>配置项</th>
</tr>
</thead>
<tbody><tr>
<td>mapreduce.framework.name</td>
<td>指定mr框架为yarn的方式</td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>map</tag>
        <tag>reduce</tag>
        <tag>hadoop源码</tag>
        <tag>shuffle</tag>
      </tags>
  </entry>
  <entry>
    <title>Airflow安装使用</title>
    <url>/2021/03/04/other/airflow%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<p>airflow是一个优秀的开源调度软件，可视化做的很棒，操作方便。airflow本身是python编写，需要安装python环境。<a id="more"></a> 安装环境为CentOS7.3，work权限（非root）。具体安装的软件如下：</p>
<p><strong>Python 3.7.1</strong></p>
<p><strong>MySQL 5.7.33</strong></p>
<p><strong>Caddy 0.10.11</strong> </p>
<p><strong>airflow 1.10.5</strong></p>
<h2 id="Mysql安装"><a href="#Mysql安装" class="headerlink" title="Mysql安装"></a>Mysql安装</h2><h3 id="下载配置"><a href="#下载配置" class="headerlink" title="下载配置"></a>下载配置</h3><p>mysql下载完成后上传到当前普通用户目录下解压，依次执行以下命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ tar -zxvf mysql-5.7.29-linux-glibc2.12-x86_64.tar.gz         #解压文件包</span><br><span class="line">$ mv mysql-5.7.29-linux-glibc2.12-x86_64 &#x2F;home&#x2F;apps&#x2F;mysql      #移动到指定目录并重命名</span><br></pre></td></tr></table></figure>

<p>编辑my.cnf配置文件，放在当前mysql安装目录下，依次执行以下命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cd mysql	         #进入安装目录</span><br><span class="line">$ vim my.cnf	      #编辑配置文件</span><br></pre></td></tr></table></figure>

<p>编辑my.cnf文件，我这里mysql的路径是/home/apps/mysql，需根据自己的路径进行修改</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[client]</span><br><span class="line">port&#x3D;3306					#服务端口</span><br><span class="line">socket&#x3D;&#x2F;home&#x2F;apps&#x2F;mysql&#x2F;mysql.sock		#指定套接字文件</span><br><span class="line"></span><br><span class="line">[mysqld]</span><br><span class="line">port&#x3D;3306					#服务端口</span><br><span class="line">basedir&#x3D;&#x2F;home&#x2F;apps&#x2F;mysql			#mysql安装路径</span><br><span class="line">datadir&#x3D;&#x2F;home&#x2F;apps&#x2F;mysql&#x2F;data                   #数据目录</span><br><span class="line">pid-file&#x3D;&#x2F;home&#x2F;apps&#x2F;mysql&#x2F;mysql.pid		#指定pid文件</span><br><span class="line">socket&#x3D;&#x2F;home&#x2F;apps&#x2F;mysql&#x2F;mysql.sock		#指定套接字文件</span><br><span class="line">log_error&#x3D;&#x2F;home&#x2F;apps&#x2F;mysql&#x2F;error.log            #指定错误日志</span><br><span class="line">server-id&#x3D;100                                   #Mysql主从唯一标识</span><br></pre></td></tr></table></figure>



<h3 id="安装启动mysql"><a href="#安装启动mysql" class="headerlink" title="安装启动mysql"></a>安装启动mysql</h3><p>安装：依次执行以下命令，指定配置文件安装并初始化mysql，没有报错即安装成功</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cd bin</span><br><span class="line">$ .&#x2F;mysqld --defaults-file&#x3D;&#x2F;home&#x2F;apps&#x2F;mysql&#x2F;my.cnf --initialize --user&#x3D;apps --basedir&#x3D;&#x2F;home&#x2F;apps&#x2F;mysql --datadir&#x3D;&#x2F;home&#x2F;apps&#x2F;mysql&#x2F;data		#安装并初始化mysql</span><br></pre></td></tr></table></figure>

<p>启动：依次执行以下命令，没有报错并能成功监听3306端口即表示启动成功</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ .&#x2F;mysqld_safe --defaults-file&#x3D;&#x2F;home&#x2F;apps&#x2F;mysql&#x2F;my.cnf --user&#x3D;apps &amp;      #启动mysql</span><br><span class="line">$ netstat -tln | grep 3306		                              #查看是否成功监听3306端口</span><br></pre></td></tr></table></figure>



<h3 id="登入mysql"><a href="#登入mysql" class="headerlink" title="登入mysql"></a>登入mysql</h3><p>获取初始密码，初始密码在error.log日志文件内，执行以下命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ cd ..</span><br><span class="line">$ less error.log | grep root@localhost		#查找root用户的初始登录密码</span><br></pre></td></tr></table></figure>

<p>登录mysql，直接输入登录命令 bin/mysql -u root -p 有可能会报mysql没有找到/tmp/mysqk.sock文件<br>有两种解决方法<br>如果本机上没有其他数据库，可以通过软连接方式将寻找sock文件的路径指向我们mysql安装目录下的sock文件<br>也可以直接指定mysql.sock文件启动，执行以下命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ .&#x2F;mysql -u root -p -S &#x2F;home&#x2F;apps&#x2F;mysql&#x2F;mysql.sock 	#指定sock文件登录</span><br></pre></td></tr></table></figure>

<p>成功登入mysql后，修改登录密码，执行以下sql语句</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">SET PASSWORD FOR &#39;root&#39;@&#39;localhost&#39; &#x3D; PASSWORD(&#39;123456&#39;);	--设置登录密码为123456</span><br><span class="line">flush privileges;</span><br></pre></td></tr></table></figure>





<h2 id="Python安装"><a href="#Python安装" class="headerlink" title="Python安装"></a>Python安装</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;www.python.org&#x2F;ftp&#x2F;python&#x2F;3.7.1&#x2F;Python-3.7.1.tgz &#x2F;&#x2F; 下载</span><br><span class="line">tar -zxvf *.tgz &#x2F;&#x2F; 解压</span><br><span class="line">到解压目录中</span><br><span class="line">mkdir &#x2F;home&#x2F;work&#x2F;soft&#x2F;python37 &#x2F;&#x2F;创建目录，作为工作目录</span><br><span class="line">.&#x2F;configure --prefix&#x3D;&#x2F;home&#x2F;work&#x2F;soft&#x2F;python37</span><br><span class="line">make &amp;&amp; make install</span><br></pre></td></tr></table></figure>



<h2 id="Airflow安装"><a href="#Airflow安装" class="headerlink" title="Airflow安装"></a>Airflow安装</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install apache-airflow&#x3D;&#x3D;1.10.5</span><br></pre></td></tr></table></figure>

<p>这个过程中可能会出现依赖冲突等各种问题，逐一解决。</p>
<p>安装完成后</p>
<p>先export AIRFLOW_HOME=xxx，export AIRFLOW_GPL_UNIDECODE=yes</p>
<p>再到 /home/work/soft/python37/bin目录下，执行./airflow，是为了在AIRFLOW_HOME下生成airflow.cfg文件</p>
<h3 id="配置airflow"><a href="#配置airflow" class="headerlink" title="配置airflow"></a>配置airflow</h3><p>配置AIRFLOW_HOME目录下的airflow.cfg文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[core]</span><br><span class="line">dags_folder &#x3D; &#x2F;home&#x2F;work&#x2F;airflow&#x2F;dags # 配置dag文件夹</span><br><span class="line">executor &#x3D; CeleryExecutor  # celeryExecutor方便分布式部署</span><br><span class="line">sql_alchemy_conn &#x3D;  mysql:&#x2F;&#x2F;airflow:airflow@localhost&#x2F;airflow  # 配置mysql</span><br><span class="line">sql_engine_encoding &#x3D; utf-8</span><br><span class="line">task_runner &#x3D; BashTaskRunner</span><br><span class="line"></span><br><span class="line">[celery]</span><br><span class="line">broker_url &#x3D; sqla+mysql:&#x2F;&#x2F;airflow:airflow@localhost&#x2F;airflow</span><br><span class="line">result_backend &#x3D; db+mysql:&#x2F;&#x2F;airflow:airflow@localhost&#x2F;airflow</span><br></pre></td></tr></table></figure>



<h3 id="airflow添加用户登录"><a href="#airflow添加用户登录" class="headerlink" title="airflow添加用户登录"></a>airflow添加用户登录</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在 airflow.cfg 文件中 [webserver] 下添加如下配置</span><br><span class="line">[webserver]</span><br><span class="line">authenticate &#x3D; True</span><br><span class="line">auth_backend &#x3D; airflow.contrib.auth.backends.password_auth</span><br><span class="line"></span><br><span class="line">然后执行：</span><br><span class="line"></span><br><span class="line">import airflow</span><br><span class="line">from airflow import models, settings</span><br><span class="line">from airflow.contrib.auth.backends.password_auth import PasswordUser</span><br><span class="line">user &#x3D; PasswordUser(models.User())</span><br><span class="line">user.username &#x3D; &#39;in_airflow&#39;  # 用户名</span><br><span class="line">user.password &#x3D; &#39;mifi_in&#39;   # 用户密码</span><br><span class="line">session &#x3D; settings.Session()</span><br><span class="line">session.add(user)</span><br><span class="line">session.commit()</span><br><span class="line">session.close()</span><br><span class="line">exit()</span><br></pre></td></tr></table></figure>



<h3 id="替换UTC时间"><a href="#替换UTC时间" class="headerlink" title="替换UTC时间"></a>替换UTC时间</h3><p>airflow右上角显示的默认为UTC时间，需要更改源码</p>
<p>1.在airflow家目录下修改airflow.cfg，设置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">default_timezone &#x3D; Asia&#x2F;Shanghai</span><br></pre></td></tr></table></figure>

<p>2.进入airflow包的安装位置,也就是site-packages的位置,以下修改文件均为相对位置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;root&#x2F;.virtualenvs&#x2F;af&#x2F;lib&#x2F;python3.4&#x2F;site-packages&#x2F;</span><br></pre></td></tr></table></figure>

<p>3.修改airflow/utils/timezone.py</p>
<p>在 utc = pendulum.timezone(‘UTC’) 这行(第27行)代码下添加,</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from airflow import configuration as conf</span><br><span class="line">try:</span><br><span class="line">	tz &#x3D; conf.get(&quot;core&quot;, &quot;default_timezone&quot;)</span><br><span class="line">	if tz &#x3D;&#x3D; &quot;system&quot;:</span><br><span class="line">		utc &#x3D; pendulum.local_timezone()</span><br><span class="line">	else:</span><br><span class="line">		utc &#x3D; pendulum.timezone(tz)</span><br><span class="line">except Exception:</span><br><span class="line">	pass</span><br></pre></td></tr></table></figure>

<p>修改utcnow()函数 (在第69行)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">原代码 d &#x3D; dt.datetime.utcnow() </span><br><span class="line">修改为 d &#x3D; dt.datetime.now()</span><br></pre></td></tr></table></figure>

<p>4.修改airflow/utils/sqlalchemy.py<br>在utc = pendulum.timezone(‘UTC’) 这行(第37行)代码下添加</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from airflow import configuration as conf</span><br><span class="line">try:</span><br><span class="line">	tz &#x3D; conf.get(&quot;core&quot;, &quot;default_timezone&quot;)</span><br><span class="line">	if tz &#x3D;&#x3D; &quot;system&quot;:</span><br><span class="line">		utc &#x3D; pendulum.local_timezone()</span><br><span class="line">	else:</span><br><span class="line">		utc &#x3D; pendulum.timezone(tz)</span><br><span class="line">except Exception:</span><br><span class="line">	pass</span><br></pre></td></tr></table></figure>

<p>注释cursor.execute(“SET time_zone = ‘+00:00’”) (第124行)</p>
<p>5.修改airflow/www/templates/admin/master.html(第31行)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">把代码 var UTCseconds &#x3D; (x.getTime() + x.getTimezoneOffset()*60*1000); </span><br><span class="line">改为 var UTCseconds &#x3D; x.getTime();</span><br><span class="line"></span><br><span class="line">把代码 &quot;timeFormat&quot;:&quot;H:i:s %UTC%&quot;,</span><br><span class="line">改为  &quot;timeFormat&quot;:&quot;H:i:s&quot;,</span><br></pre></td></tr></table></figure>

<p>最后重启airflow-webserver即可</p>
<h3 id="初始化数据库"><a href="#初始化数据库" class="headerlink" title="初始化数据库"></a>初始化数据库</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">airflow resetdb</span><br><span class="line">airflow initdb</span><br></pre></td></tr></table></figure>

<h3 id="启动airflow"><a href="#启动airflow" class="headerlink" title="启动airflow"></a>启动airflow</h3><p>使用脚本检查，如果没有相应服务则启动。总共四个服务，webserver，scheduler，servelog，worker</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;sh</span><br><span class="line">source ~&#x2F;.bashrc</span><br><span class="line">time&#x3D;&#96;date &quot;+%Y-%m-%d-%H-%M-%S&quot;&#96;</span><br><span class="line">echo &quot;start check webserver&quot; $time</span><br><span class="line">ps -fe|grep &quot;airflow webserver\\|airflow-webserver&quot; |grep -v grep</span><br><span class="line">#不存在</span><br><span class="line">if [ $? -ne 0 ]</span><br><span class="line">then</span><br><span class="line">    echo $time &quot;不存在&quot;</span><br><span class="line">    cd &#x2F;home&#x2F;work&#x2F;soft&#x2F;airflow</span><br><span class="line">    rm -rf &#x2F;home&#x2F;work&#x2F;soft&#x2F;airflow&#x2F;airflow-webserver.*</span><br><span class="line">    nohup airflow webserver &gt; &#x2F;home&#x2F;work&#x2F;soft&#x2F;airflow&#x2F;logs&#x2F;airflow-webserver_$&#123;time&#125;.log &amp;</span><br><span class="line">    ps -fe|grep &quot;airflow webserver\\|airflow-webserver&quot; |grep -v grep</span><br><span class="line">#存在</span><br><span class="line">else</span><br><span class="line">    echo $time &quot;存在&quot;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;sh</span><br><span class="line">source ~&#x2F;.bashrc</span><br><span class="line">time&#x3D;&#96;date &quot;+%Y-%m-%d-%H-%M-%S&quot;&#96;</span><br><span class="line">echo &quot;start check scheduler&quot; $time</span><br><span class="line">ps -fe|grep &quot;airflow scheduler&quot; |grep -v grep</span><br><span class="line">#不存在</span><br><span class="line">if [ $? -ne 0 ]</span><br><span class="line">then</span><br><span class="line">    echo $time &quot;不存在&quot;</span><br><span class="line">    cd &#x2F;home&#x2F;work&#x2F;soft&#x2F;airflow</span><br><span class="line">    pwd</span><br><span class="line">    nohup &#x2F;home&#x2F;work&#x2F;soft&#x2F;python37&#x2F;bin&#x2F;airflow scheduler &gt; &#x2F;home&#x2F;work&#x2F;soft&#x2F;airflow&#x2F;logs&#x2F;airflow-scheduler_$&#123;time&#125;.log &amp;</span><br><span class="line">    ps -fe|grep &quot;airflow scheduler&quot; |grep -v grep</span><br><span class="line">#存在</span><br><span class="line">else</span><br><span class="line">    echo $time &quot;存在&quot;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;sh</span><br><span class="line">source ~&#x2F;.bashrc</span><br><span class="line">time&#x3D;&#96;date &quot;+%Y-%m-%d-%H-%M-%S&quot;&#96;</span><br><span class="line">echo &quot;start check airflow servelog&quot; $time</span><br><span class="line">ps -fe|grep &quot;airflow serve_logs&quot; |grep -v grep |grep -v root</span><br><span class="line">#不存在</span><br><span class="line">if [ $? -ne 0 ]</span><br><span class="line">then</span><br><span class="line">    echo $time &quot;不存在&quot;</span><br><span class="line">    cd &#x2F;home&#x2F;work&#x2F;soft&#x2F;airflow</span><br><span class="line">    nohup airflow serve_logs&gt; &#x2F;home&#x2F;work&#x2F;soft&#x2F;airflow&#x2F;logs&#x2F;airflow-servelog_$&#123;time&#125;.log &amp;</span><br><span class="line">    ps -fe|grep &quot;airflow serve_logs&quot; |grep -v grep |grep -v root</span><br><span class="line">#存在</span><br><span class="line">else</span><br><span class="line">    echo $time &quot;存在&quot;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;sh</span><br><span class="line">source ~&#x2F;.bashrc</span><br><span class="line">time&#x3D;&#96;date &quot;+%Y-%m-%d-%H-%M-%S&quot;&#96;</span><br><span class="line">echo &quot;start check airflow worker&quot; $time</span><br><span class="line">ps -fe|grep &quot;celeryd: xxxx:MainProcess&quot; |grep -v grep |grep -v root</span><br><span class="line">#不存在</span><br><span class="line">if [ $? -ne 0 ]</span><br><span class="line">then</span><br><span class="line">    echo $time &quot;不存在&quot;</span><br><span class="line">    cd &#x2F;home&#x2F;work&#x2F;soft&#x2F;airflow</span><br><span class="line">    nohup airflow worker &gt; &#x2F;home&#x2F;work&#x2F;soft&#x2F;airflow&#x2F;logs&#x2F;airflow-worker_$&#123;time&#125;.log &amp;</span><br><span class="line">    ps -fe|grep &quot;celeryd: xx:MainProcess&quot; |grep -v grep |grep -v root</span><br><span class="line">    #存在</span><br><span class="line">else</span><br><span class="line">    echo $time &quot;存在&quot;</span><br><span class="line">fi</span><br></pre></td></tr></table></figure>





<h2 id="caddy安装"><a href="#caddy安装" class="headerlink" title="caddy安装"></a>caddy安装</h2><p>caddy在这里是为了自动部署代码，可以直接关联到git。每次git代码修改，都会自动部署到airflow上</p>
<p>下载linux版本，解压，然后配置 Caddyfile</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">0.0.0.0:6666 &#123;</span><br><span class="line">    root &#x2F;home&#x2F;work&#x2F;bin&#x2F;xx_scripts</span><br><span class="line">    log &#x2F;home&#x2F;work&#x2F;soft&#x2F;caddy&#x2F;access.log</span><br><span class="line">    git &#123;</span><br><span class="line">        repo git@git.xxx-scripts.git</span><br><span class="line">        path &#x2F;home&#x2F;work&#x2F;bin&#x2F;xx_scripts</span><br><span class="line">        key &#x2F;home&#x2F;work&#x2F;.ssh&#x2F;id_rsa</span><br><span class="line">        hook &#x2F;deploy</span><br><span class="line">        hook_type generic</span><br><span class="line">        then rm -v &#x2F;home&#x2F;work&#x2F;airflow&#x2F;dags</span><br><span class="line">        then rm -v &#x2F;home&#x2F;work&#x2F;airflow&#x2F;sqls</span><br><span class="line">        then rm -v &#x2F;home&#x2F;work&#x2F;airflow&#x2F;tests</span><br><span class="line">        then ln -sv &#x2F;home&#x2F;work&#x2F;bin&#x2F;xx_scripts&#x2F;dags &#x2F;home&#x2F;work&#x2F;airflow&#x2F;dags</span><br><span class="line">        then ln -sv &#x2F;home&#x2F;work&#x2F;bin&#x2F;xx_scripts&#x2F;sqls &#x2F;home&#x2F;work&#x2F;airflow&#x2F;sqls</span><br><span class="line">        then ln -sv &#x2F;home&#x2F;work&#x2F;bin&#x2F;xx_scripts&#x2F;tests &#x2F;home&#x2F;work&#x2F;airflow&#x2F;tests</span><br><span class="line">        interval 3600</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>启动caddy</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nohup .&#x2F;caddy -config Caddyfile &amp;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>other</category>
      </categories>
      <tags>
        <tag>airflow</tag>
        <tag>调度</tag>
      </tags>
  </entry>
  <entry>
    <title>RPC初探</title>
    <url>/2021/01/09/java/rpc%E5%88%9D%E6%8E%A2/</url>
    <content><![CDATA[<h1 id="RPC"><a href="#RPC" class="headerlink" title="RPC"></a>RPC</h1><p>RPC(Remote Procedure Call)，即远程过程调用，主要用于分布式情景下节点间的调用，可以让远程调用使用起来就像本地调用一样方便。<a id="more"></a></p>
<p>最简单RPC实现包括如下内容</p>
<ul>
<li>调用者发起调用</li>
<li>封装调用，打包方法名和参数，序列化</li>
<li>被调用端反序列化，得到方法名参数，使用反射并执行得到结果</li>
<li>将结果序列化返回给调用者</li>
<li>调用者反序列化得到结果</li>
</ul>
<p><img src="https://github.com/zcenao21/photos-blog/blob/main/rpc-start/rpc.png?raw=true" alt="rpc"></p>
<h1 id="简单实现"><a href="#简单实现" class="headerlink" title="简单实现"></a>简单实现</h1><p>为了更好理解RPC，参考知乎作者分享（见文末）的内容，自己敲了一遍（已上传<a href="https://github.com/zcenao21/rpc-learn" target="_blank" rel="noopener">github</a>）</p>
<p>首先是远程服务部分：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class ProviderApp &#123;</span><br><span class="line">    private Calculator calculator &#x3D; new CalculatorImpl();</span><br><span class="line">    static final int PORT&#x3D;9090;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws IOException &#123;</span><br><span class="line">        new ProviderApp().run();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private void run() throws IOException &#123;</span><br><span class="line">        ServerSocket listener &#x3D; new ServerSocket(PORT);</span><br><span class="line">        try &#123;</span><br><span class="line">            while (true) &#123;</span><br><span class="line">                log.info(&quot;rpc method waiting for call!&quot;);</span><br><span class="line">                Socket socket &#x3D; listener.accept();</span><br><span class="line">                try &#123;</span><br><span class="line">                    &#x2F;&#x2F; 将请求反序列化</span><br><span class="line">                    ObjectInputStream objectInputStream &#x3D; new ObjectInputStream(socket.getInputStream());</span><br><span class="line">                    Object object &#x3D; objectInputStream.readObject();</span><br><span class="line"></span><br><span class="line">                    log.info(&quot;request is &#123;&#125;&quot;, object);</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F; 调用服务</span><br><span class="line">                    int result &#x3D; 0;</span><br><span class="line">                    if (object instanceof CalculateRpcRequest) &#123;</span><br><span class="line">                        CalculateRpcRequest calculateRpcRequest &#x3D; (CalculateRpcRequest) object;</span><br><span class="line">                        if (&quot;add&quot;.equals(calculateRpcRequest.getMethod())) &#123;</span><br><span class="line">                            log.info(&quot;add service called!&quot;);</span><br><span class="line">                            result &#x3D; calculator.add(calculateRpcRequest.getA(), calculateRpcRequest.getB());</span><br><span class="line">                        &#125; else &#123;</span><br><span class="line">                            throw new UnsupportedOperationException();</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line"></span><br><span class="line">                    &#x2F;&#x2F; 返回结果</span><br><span class="line">                    ObjectOutputStream objectOutputStream &#x3D; new ObjectOutputStream(socket.getOutputStream());</span><br><span class="line">                    objectOutputStream.writeObject(new Integer(result));</span><br><span class="line">                    log.info(&quot;rpc method call finished!&quot;);</span><br><span class="line">                &#125; catch (Exception e) &#123;</span><br><span class="line">                    log.error(&quot;fail&quot;, e);</span><br><span class="line">                &#125; finally &#123;</span><br><span class="line">                    socket.close();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; finally &#123;</span><br><span class="line">            listener.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其中计算器接口：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public interface Calculator &#123;</span><br><span class="line">    public int add(int a, int b);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>计算器加法实现：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class CalculatorImpl implements Calculator&#123;</span><br><span class="line">    public int add(int a, int b) &#123;</span><br><span class="line">     return a+b;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>封装方法和参数的类：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class CalculateRpcRequest implements Serializable &#123;</span><br><span class="line">    int a;</span><br><span class="line">    int b;</span><br><span class="line"></span><br><span class="line">    public CalculateRpcRequest(int a,int b)&#123;</span><br><span class="line">        this.a&#x3D;a;</span><br><span class="line">        this.b&#x3D;b;</span><br><span class="line">    &#125;</span><br><span class="line">    public String getMethod()&#123;</span><br><span class="line">        return &quot;add&quot;;</span><br><span class="line">    &#125;</span><br><span class="line">    public int getA() &#123;</span><br><span class="line">        return a;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public int getB() &#123;</span><br><span class="line">        return b;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>另外一个是调用端：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class ConsumerApp &#123;</span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        Calculator calculator &#x3D; new CalculatorRemoteImpl();</span><br><span class="line">        int result &#x3D; calculator.add(6, 2);</span><br><span class="line">        log.info(&quot;get sum:&quot;+result);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>其中封装方法和参数以及接收返回结果的过程如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class CalculatorRemoteImpl implements Calculator &#123;</span><br><span class="line">    static final int PORT&#x3D;9090;</span><br><span class="line"></span><br><span class="line">    public int add(int a, int b) &#123;</span><br><span class="line">        String address &#x3D; &quot;127.0.0.1&quot;;</span><br><span class="line">        try &#123;</span><br><span class="line">            log.info(&quot;now call rpc method!&quot;);</span><br><span class="line">            Socket socket &#x3D; new Socket(address, PORT);</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; 将请求序列化</span><br><span class="line">            CalculateRpcRequest calculateRpcRequest &#x3D; generateRequest(a, b);</span><br><span class="line">            ObjectOutputStream objectOutputStream &#x3D; new ObjectOutputStream(socket.getOutputStream());</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; 将请求发给服务提供方</span><br><span class="line">            objectOutputStream.writeObject(calculateRpcRequest);</span><br><span class="line">            log.info(&quot;send request to service provider! then wait for result!&quot;);</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; 将响应体反序列化</span><br><span class="line">            ObjectInputStream objectInputStream &#x3D; new ObjectInputStream(socket.getInputStream());</span><br><span class="line">            Object response &#x3D; objectInputStream.readObject();</span><br><span class="line">            log.info(&quot;get result from service provider! response:&#123;&#125;&quot;,response);</span><br><span class="line"></span><br><span class="line">            if (response instanceof Integer) &#123;</span><br><span class="line">                return (Integer) response;</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                throw new InternalError();</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            log.error(&quot;fail&quot;, e);</span><br><span class="line">            throw new InternalError();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private CalculateRpcRequest generateRequest(int a, int b) &#123;</span><br><span class="line">        return new CalculateRpcRequest(a,b);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<p>在idea首先运行Provider，然后再运行Consumer。</p>
<p>Provider端日志：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[INFO ] 2021-01-09 21:00:43 rpc method waiting for call!</span><br><span class="line">[INFO ] 2021-01-09 21:00:53 request is com.will.rpc.CalculateRpcRequest@50134894</span><br><span class="line">[INFO ] 2021-01-09 21:00:53 add service called!</span><br><span class="line">[INFO ] 2021-01-09 21:00:53 rpc method call finished!</span><br><span class="line">[INFO ] 2021-01-09 21:00:53 rpc method waiting for call!</span><br></pre></td></tr></table></figure>

<p>Consumer端日志：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[INFO ] 2021-01-09 21:00:53 now call rpc method!</span><br><span class="line">[INFO ] 2021-01-09 21:00:53 send request to service provider! then wait for result!</span><br><span class="line">[INFO ] 2021-01-09 21:00:53 get result from service provider! response:8</span><br><span class="line">[INFO ] 2021-01-09 21:00:53 get sum:8</span><br></pre></td></tr></table></figure>



<blockquote>
<p>本文主要参考：<a href="https://zhuanlan.zhihu.com/p/36528189" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/36528189</a></p>
</blockquote>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Java</tag>
        <tag>RPC</tag>
      </tags>
  </entry>
  <entry>
    <title>hive使用</title>
    <url>/2020/08/12/hive/hive-dw/</url>
    <content><![CDATA[<p>hive在数仓中<a id="more"></a></p>
]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>使用</tag>
        <tag>常用功能</tag>
      </tags>
  </entry>
  <entry>
    <title>Git的使用</title>
    <url>/2020/07/26/other/git/</url>
    <content><![CDATA[<h3 id="Git是做什么的"><a href="#Git是做什么的" class="headerlink" title="Git是做什么的"></a>Git是做什么的</h3><p>Git（读音为/gɪt/）是一个开源的分布式版本控制系统，可以有效、高速地处理从很小到非常大的项目版本管理。Git 是 Linus Torvalds为了帮助管理 Linux 内核开发而开发的一个开放源码的版本控制软件[百度百科]。不得不感慨一下，Linus这位大佬是真的厉害，从0-1创造了没有的东西，还不止一个！据传言，这个软件是在两周完成，一个月投入使用的。。。</p>
<p>言归正传，通俗的说，Git的主要用途是用于版本控制，对于纯文本形式的文件尤其适合。非纯文本的比如视频就可以选择其他更好的工具来管理。而且Git的开发初衷是为了管理linux内核源码，也就是说是为代码版本控制量身定制的。</p>
<p>现在的互联网公司基本都用Git做代码版本控制，因为它可以高效完美地解决团队合作开发的问题。另外有各种代码仓库帮我们管理代码，比如Github，Gitlab。</p>
<p>Git的功能十分强大，本文对普通的开发者最常用的功能做了一些小总结。<a id="more"></a>列表如下：</p>
<ul>
<li><p>同步远程仓库到本地</p>
</li>
<li><p>修改并提交</p>
</li>
<li><p>回退</p>
</li>
<li><p>显示修改log</p>
</li>
<li><p>创建分支</p>
</li>
<li><p>合并分支</p>
</li>
<li><p>暂存修改及恢复</p>
</li>
<li><p>重放修改</p>
</li>
<li><p>打标签</p>
</li>
<li><p>命令简化</p>
</li>
<li><p>设置提交者信息</p>
</li>
<li><p>合并commit</p>
</li>
</ul>
<p>下面依次进行介绍。为了演示操作，我们选择github创建新项目进行操作。如果没有帐号，请登陆github网站或使用搜索引擎搜索相关注册教程。</p>
<h3 id="同步远程仓库到本地"><a href="#同步远程仓库到本地" class="headerlink" title="同步远程仓库到本地"></a>同步远程仓库到本地</h3><p>首先我们在github上新建一个仓库，然后同步到本地</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">% git clone git@github.com:zcenao21&#x2F;git-test.git</span><br><span class="line">正克隆到 &#39;git-test&#39;...</span><br><span class="line">remote: Enumerating objects: 3, done.</span><br><span class="line">remote: Counting objects: 100% (3&#x2F;3), done.</span><br><span class="line">remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0</span><br><span class="line">接收对象中: 100% (3&#x2F;3), 完成.</span><br><span class="line">检查连接... 完成。</span><br></pre></td></tr></table></figure>

<p>命令是git clone xxx.git，ssh串可以通过点击图中code绿色块得到</p>
<p><img src="https://i.loli.net/2020/07/26/3xsm2vdWutOEe1n.png" alt="ssh.png"></p>
<h3 id="修改并提交"><a href="#修改并提交" class="headerlink" title="修改并提交"></a>修改并提交</h3><p>克隆到本地后，可以通过git add添加修改到暂存区，git commit命令给修改增加注释信息并添加到本地git仓库中</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">% cd git-test</span><br><span class="line">% vim README.md</span><br></pre></td></tr></table></figure>

<p>vim对README.md文件进行修改，修改完成后内容为</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">% cat README.md </span><br><span class="line">	git test</span><br></pre></td></tr></table></figure>

<p>git status命令可以查看文件状态</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">% git status</span><br><span class="line">    位于分支 master</span><br><span class="line">    您的分支与上游分支 &#39;origin&#x2F;master&#39; 一致。</span><br><span class="line">    尚未暂存以备提交的变更：</span><br><span class="line">      （使用 &quot;git add &lt;文件&gt;...&quot; 更新要提交的内容）</span><br><span class="line">      （使用 &quot;git checkout -- &lt;文件&gt;...&quot; 丢弃工作区的改动）</span><br><span class="line"></span><br><span class="line">        修改：     README.md</span><br><span class="line"></span><br><span class="line">    修改尚未加入提交（使用 &quot;git add&quot; 和&#x2F;或 &quot;git commit -a&quot;）</span><br></pre></td></tr></table></figure>

<p>然后添加文件到本地仓库并注释</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">% git add README.md </span><br><span class="line">% git commit -m &quot;first commit&quot;</span><br><span class="line">    [master 98a32a9] first commit</span><br><span class="line">    1 file changed, 1 insertion(+), 2 deletions(-)</span><br></pre></td></tr></table></figure>

<p>然后推送到远程仓库，这样就完成了一次完整的提交过程</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">% git push origin master</span><br><span class="line">    对象计数中: 3, 完成.</span><br><span class="line">    写入对象中: 100% (3&#x2F;3), 260 bytes | 0 bytes&#x2F;s, 完成.</span><br><span class="line">    Total 3 (delta 0), reused 0 (delta 0)</span><br><span class="line">    To git@github.com:zcenao21&#x2F;git-test.git</span><br><span class="line">       04ca6fe..98a32a9  master -&gt; master</span><br></pre></td></tr></table></figure>



<h3 id="回退"><a href="#回退" class="headerlink" title="回退"></a>回退</h3><p>如果改错了怎么办？比如我们修改了README.md，然而想回到上一次的提交，可以用git reset命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">% vim README.md </span><br><span class="line">% git commit -am &quot;2nd line&quot;</span><br><span class="line">    [master 55ea9f4] 2nd line</span><br><span class="line">     1 file changed, 1 insertion(+)</span><br><span class="line">% cat README.md </span><br><span class="line">    git test</span><br><span class="line">    git test 2nd line</span><br></pre></td></tr></table></figure>

<p>首先我们修改了README.md，增加了 git test 2nd line这一行，然后提交到本地仓库，现在要回退到修改前的内容，即删掉新增第二行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">% git reset --hard HEAD^</span><br><span class="line">	HEAD 现在位于 d608dba 1st commit</span><br><span class="line">% cat README.md </span><br><span class="line"> 	git test</span><br></pre></td></tr></table></figure>

<p>这样就回到了第一次提交的时候。如果要回到上上次呢？</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git reset --hard HEAD^^</span><br></pre></td></tr></table></figure>

<p>三次到更多次依次类推，更简单的提交方式是</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git reset --hard HEAD~100</span><br></pre></td></tr></table></figure>

<p>表示回到一百次前，估计应该不会用到</p>
<h3 id="显示修改log"><a href="#显示修改log" class="headerlink" title="显示修改log"></a>显示修改log</h3><p>显示log需要用到git log命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">% git log</span><br><span class="line">    commit d608dbac936d917c5e9d5d6b7176d68cea98a0b1</span><br><span class="line">    Author: 张超 &lt;zhangchao29@xiaomi.com&gt;</span><br><span class="line">    Date:   Sun Jul 26 21:43:48 2020 +0800</span><br><span class="line"></span><br><span class="line">        1st commit</span><br><span class="line"></span><br><span class="line">    commit 04ca6fec277e4da0a142add5826cf64b49fd2756</span><br><span class="line">    Author: zcenao21 &lt;buct_zc@163.com&gt;</span><br><span class="line">    Date:   Sun Jul 26 21:18:00 2020 +0800</span><br><span class="line"></span><br><span class="line">        Initial commit</span><br></pre></td></tr></table></figure>

<p>但是这样很不好看，有一个输出好看的log的命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">% git log --graph --pretty&#x3D;oneline --abbrev-commit</span><br><span class="line">    * d608dba 1st commit</span><br><span class="line">    * 04ca6fe Initial commit</span><br></pre></td></tr></table></figure>

<p>可以看到，d608dba是d608dbac936d917c5e9d5d6b7176d68cea98a0b1这一长串的缩写，是这次commit的唯一标识，非常有用。比如我们想回到初始的版本</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">% git reset --hard 04ca6fe</span><br><span class="line">	HEAD 现在位于 04ca6fe Initial commit</span><br><span class="line">% cat README.md </span><br><span class="line">    # git-test</span><br><span class="line">    git测试</span><br></pre></td></tr></table></figure>

<p>那么如果我们想撤销回退的内容怎么办？提交的内容已经在log中消失了！</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">% git log --graph --pretty&#x3D;oneline --abbrev-commit</span><br><span class="line">	* 04ca6fe Initial commit</span><br></pre></td></tr></table></figure>

<p>git reflog命令可以看到所有的历史修改</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">% git reflog</span><br><span class="line">    04ca6fe HEAD@&#123;0&#125;: reset: moving to 04ca6fe</span><br><span class="line">    d608dba HEAD@&#123;1&#125;: reset: moving to HEAD^</span><br><span class="line">    b2e751e HEAD@&#123;2&#125;: commit: 2nd line</span><br><span class="line">    。。。</span><br><span class="line">    98a32a9 HEAD@&#123;11&#125;: commit: first commit</span><br><span class="line">    04ca6fe HEAD@&#123;12&#125;: clone: from git@github.com:zcenao21&#x2F;git-test.git</span><br></pre></td></tr></table></figure>

<p>然后我们执行如下命令</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">% git reset --hard b2e751e</span><br><span class="line">    HEAD 现在位于 b2e751e 2nd line</span><br><span class="line">    will@will-Lenovo-ideapad-720S-14IKB ~&#x2F;study&#x2F;projects&#x2F;git-test</span><br><span class="line">% cat README.md </span><br><span class="line">    git test</span><br><span class="line">    git test 2nd line</span><br></pre></td></tr></table></figure>

<p>可以看到，git reset可以回退到任意版本</p>
<h3 id="创建分支"><a href="#创建分支" class="headerlink" title="创建分支"></a>创建分支</h3><p>分支非常重要，是团队合作的利器。每个团队成员从主分支拉取一个分支，然后并行开发各自的模块，最终合并到主分支中去。这样既保持了开发的独立性，又实现了团队合作。创建分支示例如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">% git checkout -b dev</span><br></pre></td></tr></table></figure>

<p>dev为分支名。</p>
<h3 id="合并分支"><a href="#合并分支" class="headerlink" title="合并分支"></a>合并分支</h3><p>假如在分支上做了修改，如何合并到主分支呢？示例如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">% vim README.md </span><br><span class="line">% git commit -am &quot;3rd revise&quot;</span><br><span class="line">    [dev b4ed33e] 3rd revise</span><br><span class="line">     1 file changed, 1 insertion(+)</span><br><span class="line">% cat README.md </span><br><span class="line">    git test</span><br><span class="line">    git test 2nd line</span><br><span class="line">    git test 3rd line</span><br><span class="line">% git checkout master</span><br><span class="line">    切换到分支 &#39;master&#39;</span><br><span class="line">    您的分支和 &#39;origin&#x2F;master&#39; 出现了偏离，</span><br><span class="line">    并且分别有 2 和 1 处不同的提交。</span><br><span class="line">      （使用 &quot;git pull&quot; 来合并远程分支）</span><br><span class="line">% git merge dev </span><br><span class="line">    更新 b2e751e..b4ed33e</span><br><span class="line">    Fast-forward</span><br><span class="line">     README.md | 1 +</span><br><span class="line">     1 file changed, 1 insertion(+)</span><br></pre></td></tr></table></figure>



<h3 id="暂存修改及恢复"><a href="#暂存修改及恢复" class="headerlink" title="暂存修改及恢复"></a>暂存修改及恢复</h3><p>什么时候需要暂存呢？比如我们正在一个分支dev开发，来了一个新需求，这是我们在dev的开发还没做完，直接合并不行，会导致项目出错，其他人就没办法继续编译开发了。暂存可以将从上次提交完开始到现在做的修改暂时保存起来，之后完成了其他需求再恢复这个暂存项。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">% vim README.md </span><br><span class="line">% cat README.md </span><br><span class="line">    git test</span><br><span class="line">    git test 2nd line</span><br><span class="line">    git test 3rd line.</span><br><span class="line">% git status</span><br><span class="line">    位于分支 dev</span><br><span class="line">    尚未暂存以备提交的变更：</span><br><span class="line">      （使用 &quot;git add &lt;文件&gt;...&quot; 更新要提交的内容）</span><br><span class="line">      （使用 &quot;git checkout -- &lt;文件&gt;...&quot; 丢弃工作区的改动）</span><br><span class="line"></span><br><span class="line">        修改：     README.md</span><br><span class="line"></span><br><span class="line">    修改尚未加入提交（使用 &quot;git add&quot; 和&#x2F;或 &quot;git commit -a&quot;）</span><br><span class="line">% git stash</span><br><span class="line">    Saved working directory and index state WIP on dev: b4ed33e 3rd revise</span><br><span class="line">    HEAD 现在位于 b4ed33e 3rd revise</span><br><span class="line">% git status</span><br><span class="line">    位于分支 dev</span><br><span class="line">    无文件要提交，干净的工作区</span><br></pre></td></tr></table></figure>

<p>上面的操作是先修改dev分支的readme文件（最后一行新增.号），然后暂存操作，可以发现现在的dev分支为干净状态，恢复到了上次提交的状态。恢复暂回内容的命令为git stash pop</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">% cat README.md </span><br><span class="line">    git test</span><br><span class="line">    git test 2nd line</span><br><span class="line">    git test 3rd line</span><br><span class="line">% git stash pop</span><br><span class="line">    位于分支 dev</span><br><span class="line">    尚未暂存以备提交的变更：</span><br><span class="line">      （使用 &quot;git add &lt;文件&gt;...&quot; 更新要提交的内容）</span><br><span class="line">      （使用 &quot;git checkout -- &lt;文件&gt;...&quot; 丢弃工作区的改动）</span><br><span class="line"></span><br><span class="line">        修改：     README.md</span><br><span class="line"></span><br><span class="line">    修改尚未加入提交（使用 &quot;git add&quot; 和&#x2F;或 &quot;git commit -a&quot;）</span><br><span class="line">    丢弃了 refs&#x2F;stash@&#123;0&#125; (e47bf4e261b238728cefd414a1fa7948adec5265)</span><br><span class="line">% cat README.md </span><br><span class="line">    git test</span><br><span class="line">    git test 2nd line</span><br><span class="line">    git test 3rd line.</span><br></pre></td></tr></table></figure>

<p>我们也可以多次进行暂存，恢复的命令如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git stash apply xxx &#x2F;&#x2F;恢复某次stash，xxx可以用git stash list查看</span><br></pre></td></tr></table></figure>



<h3 id="重放修改"><a href="#重放修改" class="headerlink" title="重放修改"></a>重放修改</h3><p>如果在某个分支上做了修改，但是想把这些修改同时应用到其他分支上，即“重放”修改。下面的示例是dev领先master分支一个commit，要在master分支上重放dev分支上的最后一次commit，使用git cherry-pick操作即可</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">% git diff dev master </span><br><span class="line">    diff --git a&#x2F;README.md b&#x2F;README.md</span><br><span class="line">    index 88c9632..9cead72 100644</span><br><span class="line">    --- a&#x2F;README.md</span><br><span class="line">    +++ b&#x2F;README.md</span><br><span class="line">    @@ -1,3 +1,3 @@</span><br><span class="line">    -git test</span><br><span class="line">    + git test</span><br><span class="line">     git test 2nd line</span><br><span class="line">     git test 3rd line</span><br><span class="line">% git log --graph --pretty&#x3D;oneline --abbrev-commit</span><br><span class="line">    * a2b2c9a remove first blank</span><br><span class="line">    * b4ed33e 3rd revise</span><br><span class="line">    * b2e751e 2nd line</span><br><span class="line">    * d608dba 1st commit</span><br><span class="line">    * 04ca6fe Initial commit</span><br><span class="line">% git checkout master</span><br><span class="line">    切换到分支 &#39;master&#39;</span><br><span class="line">    您的分支和 &#39;origin&#x2F;master&#39; 出现了偏离，</span><br><span class="line">    并且分别有 3 和 1 处不同的提交。</span><br><span class="line">      （使用 &quot;git pull&quot; 来合并远程分支）</span><br><span class="line"> % git cherry-pick a2b2c9a</span><br><span class="line">    [master a8f25bd] remove first blank</span><br><span class="line">     Date: Mon Jul 27 00:46:02 2020 +0800</span><br><span class="line">     1 file changed, 1 insertion(+), 1 deletion(-)</span><br><span class="line">% git log --graph --pretty&#x3D;oneline --abbrev-commit</span><br><span class="line">    * a8f25bd remove first blank</span><br><span class="line">    * b4ed33e 3rd revise</span><br><span class="line">    * b2e751e 2nd line</span><br><span class="line">    * d608dba 1st commit</span><br><span class="line">    * 04ca6fe Initial commit</span><br><span class="line">% git diff a8f25bd b4ed33e</span><br><span class="line">    diff --git a&#x2F;README.md b&#x2F;README.md</span><br><span class="line">    index 88c9632..9cead72 100644</span><br><span class="line">    --- a&#x2F;README.md</span><br><span class="line">    +++ b&#x2F;README.md</span><br><span class="line">    @@ -1,3 +1,3 @@</span><br><span class="line">    -git test</span><br><span class="line">    + git test</span><br><span class="line">     git test 2nd line</span><br><span class="line">     git test 3rd line</span><br></pre></td></tr></table></figure>



<h3 id="打标签"><a href="#打标签" class="headerlink" title="打标签"></a>打标签</h3><p>标签和commit很类似，标签常用来管理版本号。commit到一定数量，一个功能开发完成，这时候就可以打一个tag。github还会贴心的为tag打包可以直接下载。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">% git tag v1.0</span><br><span class="line">% git tag</span><br><span class="line">	v1.0</span><br></pre></td></tr></table></figure>

<p>删除及其它操作如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git tag -a v2.0 -m &quot;move ahead!&quot; 4225b7d  &#x2F;&#x2F;给标签加上备注</span><br><span class="line">git tag -d v1.0 &#x2F;&#x2F;删除标签</span><br><span class="line">git push origin :refs&#x2F;tags&#x2F;v0.9 &#x2F;&#x2F;删除远程标签。先删除本地，git tag -d 命令，然后执行此命令删除远程</span><br></pre></td></tr></table></figure>



<h3 id="命令简化"><a href="#命令简化" class="headerlink" title="命令简化"></a>命令简化</h3><p>刚刚的git log后跟的一长串有没有让你留下深刻的印象？Git提供了一种快捷方式。比如缩写git log …</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git config --global alias.lg &quot;log --color --graph --pretty&#x3D;format:&#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset&#39; --abbrev-commit&quot;</span><br></pre></td></tr></table></figure>

<p>之后我们就可以使用git lg这个命令了</p>
<h3 id="设置提交者信息"><a href="#设置提交者信息" class="headerlink" title="设置提交者信息"></a>设置提交者信息</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git config --global user.name &quot;名字&quot;</span><br><span class="line">git config --global user.email &quot;邮箱&quot;</span><br></pre></td></tr></table></figure>

<p>这是针对全局的设置，如果对于某个仓库想单独设置，可以修改.git/config文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[user]   </span><br><span class="line">	name &#x3D; XXX(自己的名称英文)   </span><br><span class="line">	email &#x3D; XXXX(邮箱)</span><br></pre></td></tr></table></figure>



<h3 id="合并commit"><a href="#合并commit" class="headerlink" title="合并commit"></a>合并commit</h3><p>参考<a href="/2020/02/27/other/rebase%E5%90%88%E5%B9%B6commit/" title="git rebase">git rebase</a></p>
]]></content>
      <categories>
        <category>other</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>工具</tag>
        <tag>代码提交</tag>
        <tag>团队开发</tag>
      </tags>
  </entry>
  <entry>
    <title>文件及用户权限</title>
    <url>/2020/03/10/linux/linux-%E6%96%87%E4%BB%B6%E6%9D%83%E9%99%90/</url>
    <content><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>计算机最重要的两大部分：存储和计算。存储分永久性存储（例如文件）和短暂的存储（例如内存）。永久性存储我们接触最多的就是文件了。<a id="more"></a>大多数人都用过word，肯定都有过word没保存，工作白干了这种尴尬的事情，这就是临时的修改没有保存到文件中的缘故。文件作为重要信息载体，安全性、共享性都非常重要。我们是否可以通过linux的文件系统做到以下事情？</p>
<p>1.不同用户对同一文件有不同权限</p>
<p>2.是否可以分配某个权限给一群人</p>
<p>3.是否可以针对某个用户做权限限制</p>
<p>4.为了文件安全性，是否可以设置文件只能添加或者其他权限如不能删除改名</p>
<p>5.只让用户做指定的事情</p>
<p>带着这些问题，我们将分3个部分进行介绍。</p>
<h1 id="基础权限"><a href="#基础权限" class="headerlink" title="基础权限"></a>基础权限</h1><p>网上盗一张图：</p>
<p><img src="https://i.loli.net/2020/03/12/Nwa34uJdEZFgW2H.png" alt="image.png"></p>
<p>linux列出文件的命令<code>ls -l</code>执行一下，得到如下结果</p>
<p><img src="https://i.loli.net/2020/03/12/HvxWYgweRpmGf6l.png" alt=""></p>
<center>图1</center>
表示当前目录下只有一个test.txt文件。最前面的含义可以用第一张图来解释。第一个数字表示第几个符号

<p><code>-rw-rw-r--</code></p>
<p>1: -表示文件类型是文件。文件夹用d表示，l表示链接文件。最常用的就是这三个。接下来每3个一组</p>
<p>2: r对于will用户可读（<code>ll</code>显示的结果第三列表示所有者用户）</p>
<p>3: w对于will用户可写</p>
<p>4: -对于will用户不可执行</p>
<p>接下来的3个表示对于用户组will可读可写不可执行(<code>ll</code>显示的结果第四列表示所在用户组)</p>
<p>最后的3个表示对于其他用户可读不可写不可执行</p>
<blockquote>
<p>第一个问题就解决了。当以will用户登陆使用linux时，对于test.txt文件就有两个权限读和写; 若非will用户组的其他用户登陆linux时，对于test.txt文件就只有读的权限。</p>
</blockquote>
<blockquote>
<p>第二个问题也解决了。如何给一群人一个权限？拉到一个用户组就可以了。</p>
</blockquote>
<p>对于文件和文件夹，rwx的权限含义是不一样的，如下表</p>
<table>
<thead>
<tr>
<th></th>
<th>r</th>
<th>w</th>
<th>x</th>
</tr>
</thead>
<tbody><tr>
<td>文件</td>
<td>读取文件内容</td>
<td>修改文件内容</td>
<td>执行文件内容</td>
</tr>
<tr>
<td>目录</td>
<td>读到文件名</td>
<td>修改文件名</td>
<td>进入该目录</td>
</tr>
</tbody></table>
<p>对于目录权限的理解：若没有x权限，就无法进入该目录;若没有r权限，则文件夹中内容不可见。可以将文件夹理解成一个盒子，x权限相当于我们有了钥匙，但是是在黑夜中打开，看不见里面的内容，r权限就是一道光，照亮盒子，让我们看到里面的小盒子（子文件夹）和小糖果、小文具（文件）。</p>
<h1 id="ACL权限"><a href="#ACL权限" class="headerlink" title="ACL权限"></a>ACL权限</h1><p>如上一部分所介绍的，可以针对某个用户组设置权限，但是如果想针对某个用户做限制可以做到吗？比如一个项目组正在做一个项目，这些人都属于一个用户组。对于其他人，项目文件不可见。现在有个人非此项目人员，但又有查看这些项目文件的需求怎么办？传统的用户，用户组，其他用户的区分已经没办法做到了。ACL可以帮我们做到！</p>
<p>ACL是Access Control List的英文缩写，现在的unix-like系统一般都会装。如果要确认是否有这个功能，可以使用如下命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dmsg | grep -i acl</span><br></pre></td></tr></table></figure>

<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><p>以示例来说明。以当前用户创建一个文件，再写点东西进去</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">touch testfile</span><br><span class="line"></span><br><span class="line"> % ll</span><br><span class="line">总用量 0</span><br><span class="line">-rw-rw-r-- 1 will will 0 4月   1 00:40 testfile</span><br><span class="line"></span><br><span class="line">% echo &quot;big data&quot; &gt; testfile</span><br><span class="line">% cat testfile </span><br><span class="line">big data</span><br></pre></td></tr></table></figure>

<p>假设已经有了一个用户tom，这个用户经常恶作剧，所以我们想限制tom这个人不能对这个文件进行修改</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> % setfacl -m u:tom:r testfile </span><br><span class="line"> % ll</span><br><span class="line">-rw-rw-r--+ 1 will will 9 4月   1 00:43 testfile</span><br></pre></td></tr></table></figure>

<p>可以发现在九个字母表示的权限后面多了一个+号，表示有ACL权限设置了。设置ACL的命令为<code>setfacl</code>，-m表示设置参数给文件使用，-x表示删除ACL参数，-b表示删除所有的ACL参数。下面验证是否设置成功，即tom能不能修改这个文件</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">% su - tom</span><br><span class="line">$ ls -l</span><br><span class="line">-rw-rw-r--+ 1 will will 9 4月   1 00:43 testfile</span><br><span class="line">$ cat testfile </span><br><span class="line">big data</span><br><span class="line">$ vim testfile</span><br></pre></td></tr></table></figure>

<p>使用vim编辑文件就会提示文件只读，设置成功。</p>
<p>获取ACL设置可以用<code>getfacl</code></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ getfacl testfile </span><br><span class="line"># file: testfile</span><br><span class="line"># owner: will</span><br><span class="line"># group: will</span><br><span class="line">user::rw-</span><br><span class="line">user:tom:r--</span><br><span class="line">group::rw-</span><br><span class="line">mask::rw-</span><br><span class="line">other::r--</span><br></pre></td></tr></table></figure>

<p>ACL还可以针对用户组设置，还能设置继承时的文件权限，比如子文件对某用户/某用户组只读</p>
<p>针对用户组设置：<code>setfacl -m g:user:rwx filename</code></p>
<p>设置继承：<code>setfacl -m d:u:user:rx dirname</code></p>
<p>取消ACL：<code>setfacl -b fielname</code></p>
<blockquote>
<p>第三个问题圆满解决</p>
</blockquote>
<h1 id="文件隐藏属性"><a href="#文件隐藏属性" class="headerlink" title="文件隐藏属性"></a>文件隐藏属性</h1><p>隐藏属性是个啥？可以理解成基本属性的补充。比如我们需要设置某个文件只能添加怎么办？普通的权限设置已经无能为力了，这时候就用到了隐藏属性。</p>
<p>chatttr [+-=] [ai] 文件或目录名称</p>
<blockquote>
<p>a参数表示文件只能增加数据，不能删除也不能修改，只有root有权限设置此权限</p>
<p>i参数表示文件不能删除，改名，设置链接，无法写入或新增数据</p>
</blockquote>
<p>lsattr [-adR] 文件或目录</p>
<p>分别用于配置和显示隐藏属性</p>
<p>如下示例，表示设置文件只能添加，妄图写数据报错</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">% sudo chattr +a testfile</span><br><span class="line">% echo &quot;input&quot; &gt; testfile </span><br><span class="line">不允许的操作: testfile</span><br></pre></td></tr></table></figure>

<blockquote>
<p>第四个问题到这里也解决了！</p>
</blockquote>
<h1 id="文件特殊权限：SUID-SGID-SBIT"><a href="#文件特殊权限：SUID-SGID-SBIT" class="headerlink" title="文件特殊权限：SUID/SGID/SBIT"></a>文件特殊权限：SUID/SGID/SBIT</h1><p>终于来到了最后一类权限。这类权限能干嘛？比如一个普通用户，我们希望他只能执行某个操作。比如Linux修改密码的工作就是/usr/bin/passwd来做的，这个脚本修改的是/etc/shadow里面的东西。然而，对于/etc/shadow这个文件是高度保密的，不然就会出现密码泄露，而且文件权限是— — —，即只有root有权限修改。那么我们平时怎么修改自己的密码？答案就是在执行时获得root权限。</p>
<h3 id="Set-UID"><a href="#Set-UID" class="headerlink" title="Set UID"></a>Set UID</h3><p>简称SUID。这个就可以实现执行时获得权限的功能，它有如下特性</p>
<ul>
<li>SUID仅对二进制程序有效</li>
<li>执行者需要对改程序具有x的权限</li>
<li>本权限仅在执行程序过程中有效</li>
<li>执行者将具有该程序拥有者的权限</li>
</ul>
<p>拥有SUID权限时，用户权限x变成s。</p>
<h3 id="Set-GID"><a href="#Set-GID" class="headerlink" title="Set GID"></a>Set GID</h3><p>与SUID不同的是，SGID可以针对文件和目录设置，而且对文件和目录设置时功能不同。</p>
<p>对于文件来说：</p>
<ul>
<li>SGID对二进制程序有用</li>
<li>程序执行者需要对该程序具备x权限</li>
<li>执行者将在执行过程中获得该程序用户组支持</li>
</ul>
<p>对于目录来说：</p>
<ul>
<li>用户若对此目录具有r和x权限时，该用户能够进入该目录</li>
<li>用户在此目录下的有效用户组会变成该目录的用户组</li>
<li>用户：若用户在此目录下拥有w权限，则用户所建立的新文件用户组与此目录用户组相同</li>
</ul>
<p>对于目录的第三个特性非常有用！</p>
<p>当拥有SGID权限时，s会出现在用户组的x位置</p>
<h3 id="Sticky-Bit"><a href="#Sticky-Bit" class="headerlink" title="Sticky Bit"></a>Sticky Bit</h3><p>简称SBIT。仅针对目录有效</p>
<ul>
<li>当用户对此目录具有w、x权限</li>
<li>当用户在该目录下建立文件或目录时，仅有root和自己才有权利增删改该文件，其他人有r权限时可以读。</li>
</ul>
<blockquote>
<p>最后一个问题圆满解决！</p>
</blockquote>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>文件</tag>
        <tag>权限</tag>
      </tags>
  </entry>
  <entry>
    <title>计算机联网</title>
    <url>/2020/03/05/other/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%81%94%E7%BD%91/</url>
    <content><![CDATA[<p>本科计算机网络学的不扎实，一直对ip寻址一知半解。最近稍微研究了一下，有了新的认识，小结一下。</p>
<a id="more"></a>
<p>搜索引擎是我们经常用到的，就以百度为例。当我们在本机上输入<a href="http://www.baidu.com" target="_blank" rel="noopener">www.baidu.com</a>, 回车，就能在浏览器看到百度的搜索首页。看似一个简单的过程，包括了很多知识。</p>
<h3 id="域名解析"><a href="#域名解析" class="headerlink" title="域名解析"></a>域名解析</h3><p>TCP/IP是计算机通信的协议，计算机间是通过IP来进行连接的。IP（仅介绍IPV4，IPV6可以自己了解）的定义是32位来表示的，每个位可以为0或者1。由于二进制不好写且不好记，所以表示的时候8位为一组表示。有过一定计算机基础都知道8位表示为十进制，能表达的范围为0～255（无符号），所以32位IP可以表示为192.168.143.252这样的一串数字，比32位更方便书写和记忆了。但是对于人来说还是不容易记住那么多的IP地址，怎么办？解决办法是用我们容易记住的网址来替换IP，比如<a href="http://www.baidu.com，我们最终还是通过IP来访问百度服务器的。" target="_blank" rel="noopener">www.baidu.com，我们最终还是通过IP来访问百度服务器的。</a></p>
<p>这个对应关系存在哪里呢？它们存在域名解析服务器上，国际上有很多的域名解析服务器，而且是分等级的，最高级的是根域名解析服务器，各国家或组织来维护，这些可以自己了解。有了域名解析服务器，我们就可以将网址转化为对应的IP了。</p>
<h3 id="子网掩码"><a href="#子网掩码" class="headerlink" title="子网掩码"></a>子网掩码</h3><p>32位的IP供全球的人使用，最多可用个数是2的32次方，差不多42亿左右，现在全球人数早就超过了这个数字;而且申请IP不是个人来申请的，而是代理商或组织申请一段连续的IP，这样就更显得不够用了。怎么办？这就涉及到了子网的概念。比如一个小办公楼，如果就分配一个IP，作为统一的通信入口和出口。对于外面的计算机来说，只要负责找到这个入口，其他的事情交给办公楼内部解决。内部可以以代号来表示其中的计算机，这样就可以节省很多IP。这个代号在IP协议里也规定了，也是用IP表示，不过有范围：</p>
<p>A类地址：10.0.0.0 - 10.255.255.255<br>B类地址：172.16.0.0 - 172.31.255.255 C类地址：192.168.0.0 -192.168.255.255 </p>
<p>关于ABC类可以自己了解。这样内部的IP表示也解决了。但是还有一个问题，如何识别一个一个IP是要访问内部还是外部？这就是子网掩码发挥作用的时候了。之前也说了，IP本质上还是按位来表示的，要么0要么1，所以可以按位与，与的定义是如果都是1那么与的结果是1，其他情况都是0。这样与1就表示保持原来的位不变，与0就表示置0。用公式来表示：</p>
<p>1&amp;*=*</p>
<p>0&amp;*=0</p>
<p>默认的子网掩码是255.255.255.0，这个子网掩码的意思是保持32位的前24位不变，后面的置0。如果本机IP和目标IP都和子网掩码做与操作，如果结果相同说明在同一个子网内</p>
]]></content>
      <categories>
        <category>other</category>
      </categories>
      <tags>
        <tag>ip寻址</tag>
        <tag>上网</tag>
        <tag>计算机网络</tag>
      </tags>
  </entry>
  <entry>
    <title>git rebase合并commits</title>
    <url>/2020/02/27/other/rebase%E5%90%88%E5%B9%B6commit/</url>
    <content><![CDATA[<h1 id="rebase作用"><a href="#rebase作用" class="headerlink" title="rebase作用"></a>rebase作用</h1><p>rebase翻译过来是变基，主要作用有两个</p>
<ul>
<li><strong>合并代码不会有冲突，因为在rebase过程中已经解决了</strong>。程序员很多时候需要协同开发，假设有如下场景，分别有开发的程序猿A和程序员B，以及他们的leader来合并A和B的代码。程序猿A和B同时从master分支拉代码并新建为各自的分支，之后程序猿A和B同时对一个文件做了修改，这在日常coding中非常常见。这时如果A的代码由leader合并到了master分支，B在A合并之后提交自己的修改。由于对同一个文件做了修改，leader必须处理完冲突之后才能将B的代码合并。通过rebase，还是上述的过程，不过这次B这次做了rebase到master分支的操作，在这个过程中就已经处理完冲突。由于B对于自己的修改是很清楚的，由他来处理冲突更合适。leader就不需要处理冲突了。</li>
<li><strong>让提交信息更加清晰</strong>。在开发过程中，我们经常会保存修改的代码到程序仓库(如github, gitlab)。在保存之前会有一个commit信息，很多人都是随便写这个commit信息，而合并代码之后这些commit信息会现实在修改历史中，很多时候这些信息都是没必要的。假如有多个程序猿同时开发，这些commit信息就会多如牛毛，之后看修改信息就很困难了。其实只需要把所有修改的内容总结到一个commit信息中即可，这样提交的代码修改逻辑就十分清晰。rebase可以实现合并commit信息的功能。</li>
</ul>
<a id="more"></a>

<p>从网上盗张图，侵删</p>
<p><img src="https://i.loli.net/2020/04/12/alxyPthifAuvTb3.png" alt="rebase"></p>
<p>从图中很明显的可以看出，变基之后相当于你从master分支最后的修改处继续做了改动。</p>
<h1 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h1><h3 id="首先新建项目并从github上拉下来"><a href="#首先新建项目并从github上拉下来" class="headerlink" title="首先新建项目并从github上拉下来"></a>首先新建项目并从github上拉下来</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone git@github.com:zcenao21&#x2F;rebaseTest.git</span><br><span class="line">正克隆到 &#39;rebaseTest&#39;...</span><br><span class="line">remote: Enumerating objects: 3, done.</span><br><span class="line">remote: Counting objects: 100% (3&#x2F;3), done.</span><br><span class="line">remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0</span><br><span class="line">接收对象中: 100% (3&#x2F;3), 完成.</span><br><span class="line">检查连接... 完成。</span><br></pre></td></tr></table></figure>



<h3 id="新建分支"><a href="#新建分支" class="headerlink" title="新建分支"></a>新建分支</h3><p>分别为程序猿A，B和leader的分支，A和B分别做了修改然后提交到leader分支作为阶段性的分支。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">% git checkout -b apeA origin&#x2F;master</span><br><span class="line">分支 apeA 设置为跟踪来自 origin 的远程分支 master。</span><br><span class="line">切换到一个新分支 &#39;apeA&#39;</span><br><span class="line">will@will-Lenovo-ideapad-720S-14IKB ~&#x2F;study&#x2F;projects&#x2F;rebaseTest</span><br><span class="line"> % git checkout -b apeB origin&#x2F;master</span><br><span class="line">分支 apeB 设置为跟踪来自 origin 的远程分支 master。</span><br><span class="line">切换到一个新分支 &#39;apeB&#39;</span><br><span class="line">will@will-Lenovo-ideapad-720S-14IKB ~&#x2F;study&#x2F;projects&#x2F;rebaseTest</span><br><span class="line"> % git checkout -b leader origin&#x2F;master</span><br><span class="line">分支 leader 设置为跟踪来自 origin 的远程分支 master。</span><br><span class="line">切换到一个新分支 &#39;leader&#39;</span><br></pre></td></tr></table></figure>



<h3 id="修改并提交多次"><a href="#修改并提交多次" class="headerlink" title="修改并提交多次"></a>修改并提交多次</h3><p>程序猿A和B分别做了两次修改，而且都是修改的READ.md文件</p>
<p><img src="https://i.loli.net/2020/04/12/6ZE9Lkne4vfxisD.png" alt="image.png"></p>
<p><img src="https://i.loli.net/2020/04/12/CPpraVdR1lIAE7g.png" alt="image.png"></p>
<h3 id="合并程序猿A的修改"><a href="#合并程序猿A的修改" class="headerlink" title="合并程序猿A的修改"></a>合并程序猿A的修改</h3><p>leader合并完A的修改后（假装有三个人的样子），leader分支commit信息如下</p>
<p><img src="https://i.loli.net/2020/04/12/DwsLNIZdr6EBOYJ.png" alt="image.png"></p>
<h3 id="合并程序猿B的修改"><a href="#合并程序猿B的修改" class="headerlink" title="合并程序猿B的修改"></a>合并程序猿B的修改</h3><p>程序猿B提出merge到leader分支的请求，哦哟，有冲突！</p>
<p><img src="https://i.loli.net/2020/04/12/wQdYqLaGxEPsFe4.png" alt="image.png"></p>
<p>leader看了一下B修改的内容和B的是不同内容，需要解决冲突后合并</p>
<p>解决冲突前：</p>
<p><img src="https://i.loli.net/2020/04/12/k7eIldPu3Fwnh9Q.png" alt="image.png"></p>
<p>解决冲突后：</p>
<p><img src="https://i.loli.net/2020/04/12/QAomr4j5e9FWMBL.png" alt="image.png"></p>
<p>合并到leader分支之后</p>
<p><img src="https://i.loli.net/2020/04/12/FGXkKUIOL9wRzn3.png" alt="image.png"></p>
<p>哦哟，A和B只是修改了一点点代码（其实就是划水改了一下READ.md），就这么多commit信息了。leader过了几天发现代码有Bug，然后看看修改历史，头有点疼。。。而且这只是A和B各只commit两次的结果。</p>
<h3 id="rebase闪亮登场"><a href="#rebase闪亮登场" class="headerlink" title="rebase闪亮登场"></a>rebase闪亮登场</h3><p>rebase旁白：让我来干掉这些乱七八糟的修改历史吧！</p>
<p>一顿操作如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">% git rebase -i leader-rebase</span><br></pre></td></tr></table></figure>

<p>进入如下修改页面（默认nano编辑器，可以修改为vim，不赘述）。将提交信息8ffeb8e那行前面的pick改为s，表示第二行的信息合并到第一行中去，commit信息之后可以编辑。</p>
<p>ps: s是squash的缩写，表示使用commit信息但是合并到前一条commit信息中。其他的选项也都有解释，不赘述。</p>
<p><img src="https://i.loli.net/2020/04/12/DoeBx9jqRvy25hp.png" alt="image.png"></p>
<p>^x离开，选择保存修改信息，Y</p>
<p><img src="https://i.loli.net/2020/04/12/CxJTn6gi9VU4PIo.png" alt="image.png"></p>
<p>然后就是合并commit信息，就像编辑文本那样编辑就行</p>
<p><img src="https://i.loli.net/2020/04/12/AajJzqU8iXEmOTh.png" alt="image.png"></p>
<p>编辑完如下图，保存。A的rebase工作顺利完成。然后让leader合并。</p>
<p><img src="https://i.loli.net/2020/04/12/u2MPf8QYrhtDbGk.png" alt="image.png"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> % git rebase -i leader-rebase</span><br><span class="line">[分离头指针 c7d70d0] apeA: 1st commit</span><br><span class="line"> Date: Sun Apr 12 00:23:10 2020 +0800</span><br><span class="line"> 1 file changed, 5 insertions(+)</span><br><span class="line">Successfully rebased and updated refs&#x2F;heads&#x2F;apeA.</span><br></pre></td></tr></table></figure>

<p>切换到leader-rebase分支，然后merge</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> % git merge apeA</span><br><span class="line">更新 5966a99..c7d70d0</span><br><span class="line">Fast-forward</span><br><span class="line"> README.md | 5 +++++</span><br><span class="line"> 1 file changed, 5 insertions(+)</span><br></pre></td></tr></table></figure>

<p>下面是B的rebase操作了，和A类似的一顿操作。然鹅，出现了如下错误：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">% git rebase -i leader-rebase </span><br><span class="line">error: 不能应用 3a09941... apeB: first commit</span><br><span class="line"></span><br><span class="line">当您解决了此问题后，执行 &quot;git rebase --continue&quot;。</span><br><span class="line">如果您想跳过此补丁，则执行 &quot;git rebase --skip&quot;。</span><br><span class="line">要恢复原分支并停止变基，执行 &quot;git rebase --abort&quot;。</span><br><span class="line">Could not apply 3a09941bc634a59c6f3c238c8352fc7175834a63... apeB: first commit</span><br></pre></td></tr></table></figure>

<p>这是因为和A的代码冲突了。稳住，解决冲突。根据提示信息</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">% git rebase --continue</span><br><span class="line">README.md: needs merge</span><br><span class="line">您必须编辑所有的合并冲突，然后通过 git add</span><br><span class="line">命令将它们标记为已解决</span><br></pre></td></tr></table></figure>

<p>然后使用编辑器编辑README.md文件，修改完成后如下：</p>
<p><img src="https://i.loli.net/2020/04/12/chXH6Fdy5AziRu4.png" alt="image.png"></p>
<p>这只是处理了apeB第一次的commit信息，接下来处理第二次的commit信息</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git add README.md</span><br></pre></td></tr></table></figure>

<p><img src="https://i.loli.net/2020/04/12/OnpoFCRrtLANckD.png" alt="image.png"></p>
<p>修改完commit信息后如下：</p>
<p><img src="https://i.loli.net/2020/04/12/WtB3lLXKiko7mRY.png" alt="image.png"></p>
<p>保存之后，出现如下信息！rebase成功</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">% git rebase --continue</span><br><span class="line">[分离头指针 abb911a] apeB: first commit</span><br><span class="line"> 1 file changed, 4 insertions(+)</span><br><span class="line">[分离头指针 5deef87] apeB: 1st commit</span><br><span class="line"> Date: Sun Apr 12 00:28:26 2020 +0800</span><br><span class="line"> 1 file changed, 7 insertions(+)</span><br><span class="line">Successfully rebased and updated refs&#x2F;heads&#x2F;apeB.</span><br></pre></td></tr></table></figure>

<p>切换到leader-rebase分支，合并</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">% git merge apeB</span><br><span class="line">更新 c7d70d0..5deef87</span><br><span class="line">Fast-forward</span><br><span class="line"> README.md | 7 +++++++</span><br><span class="line"> 1 file changed, 7 insertions(+)</span><br></pre></td></tr></table></figure>

<p><img src="https://i.loli.net/2020/04/12/Q7f5v4XF2bAtPCZ.png" alt="image.png"></p>
<p>perfect！清爽！和谐！</p>
]]></content>
      <categories>
        <category>other</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>rebase</tag>
        <tag>commits合并</tag>
      </tags>
  </entry>
  <entry>
    <title>github个人网站替换自定义域名</title>
    <url>/2020/02/24/other/github%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99%E6%9B%B4%E6%94%B9%E5%9F%9F%E5%90%8D/</url>
    <content><![CDATA[<p>首先非常感谢github提供的个人网站功能，然后感谢hexo提供了方便的写博客手段。<br>github个人建站，一般访问网站名为xxx.github.io，这种名字对于强迫症患者来说真的是无法忍受。但是github也提供了更改为自定义域名功能，非常人性化。</p>
<a id="more"></a>
<p>所以如果你没有把博客迁到国内服务器的需求，只需要替换一下域名即可。下面说一下我的操作步骤，主要还在申请域名上。</p>
<h3 id="域名申请"><a href="#域名申请" class="headerlink" title="域名申请"></a>域名申请</h3><p>经过我的一番调研，最终选用了godaddy的域名服务。国内的也有很多，大家根据需要选择。<br>接下来就是申请域名了。先选择一个自己喜欢的域名前缀，最好比较好记，方便自己也方便他人。域名的后缀有很多，比如.com,.top，.net等等，价格也不一样，便宜的只需要几元人民币。选完域名付完钱，在使用域名前还需要一个认证的过程，中国大陆可以选择的只有身份证。<br><strong>这里必须吐槽一下，一般需要身份证复印件的都是正反面，然后我就上传了正反面。第一次给拒绝了，理由是空白边缘留的不够宽。这次我认认真真，拍了正反面在照片中间再次上传，等了一天，又给我拒绝了，还是相同的理由！我就很纳闷了。打电话给客服问了才知道，只需要正面，但是他们审核的时候没有这个选项，就给我随便找了个理由。。。</strong>第三次终于申请成功了。</p>
<p>接下来就是域名解析服务器替换，好像他们也提供，不替换是否可行没试。我用国内的DNSpod。</p>
<p>首先需要在godaddy上更改域名服务器。DNSpod对于在godaddy上更改域名服务器的说明已经过时了</p>
<p><img src="https://i.loli.net/2020/02/24/Tdk3JMGxInWoLS2.png" alt="选区_017.png"></p>
<p>替换后如上图</p>
<p>等一段时间，我等了半小时吧，等DNSpod上显示状态为正常就可以了</p>
<p><img src="https://i.loli.net/2020/02/24/sv1zMHZrExTOlAa.png" alt="选区_018.png"></p>
<p>现在再进去添加两条纪录</p>
<p><img src="https://i.loli.net/2020/02/24/1d65DsLzUfHqRPW.png" alt="选区_020.png"></p>
<p>上面的记录值是用下面的命令得到的。这两条表示不输入www和输入www都导向记录值</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ping xxx.github.io</span><br></pre></td></tr></table></figure>

<p>最后，在github上填写替换的域名。我用的hexo部署的，所以在source目录下新建CNAME文件，内容为will21.cn。部署到服务器。</p>
<p>访问will21.cn，搞定！</p>
]]></content>
      <categories>
        <category>other</category>
      </categories>
      <tags>
        <tag>github个人网站</tag>
        <tag>域名修改</tag>
      </tags>
  </entry>
  <entry>
    <title>hive udf&amp;udaf</title>
    <url>/2020/02/21/hive/hive-udf/</url>
    <content><![CDATA[<p>hive为我们定义了很多函数，大多数情况下是能够满足我们的需求的。但是在有些情况下，很有必要自己定义一些函数，这样使用起来非常方便。这就是自定义函数(udf)和自定义聚合函数(udaf,user defined aggregation function)。udf是每行返回一个结果，而udaf则是聚合的结果，多行产生一个结果。另外还有udtf，是一行产生多个结果。目前我还没有用到过udtf，就先不介绍它了。下面分别用两个示例介绍udf和udaf<a id="more"></a></p>
<h1 id="udf"><a href="#udf" class="headerlink" title="udf"></a>udf</h1><h3 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h3><p>查找array中是否包含被查询值</p>
<h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><ul>
<li><p>测试数据准备</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">zhangsan        beijing,shanghai,tianjin,hangzhou</span><br><span class="line">lisi    changchu,chengdu,wuhan</span><br></pre></td></tr></table></figure>
</li>
<li><p>hive建表与导入</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Create table users(name string, worklocations array&lt;string&gt; ) row format delimited fields terminated by &#39;\t&#39; collection items terminated by &#39;,&#39;; </span><br><span class="line"></span><br><span class="line">load data local inpath &#39;&#x2F;root&#x2F;person.txt &#39; OVERWRITE INTO TABLE users;</span><br></pre></td></tr></table></figure>
</li>
<li><p>udf包生成与导入</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">package com.will;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line">import java.util.ArrayList;</span><br><span class="line"></span><br><span class="line">public class FindInArray extends UDF &#123;</span><br><span class="line">    public ArrayList&lt;String&gt; evaluate(String keywords, ArrayList&lt;String&gt; column)&#123;</span><br><span class="line">        &#x2F;&#x2F;参数类型使用arraylist&lt;String&gt;对应hive中的array&lt;string&gt;,而不是String[]</span><br><span class="line">        if(column.contains(keywords))&#123;</span><br><span class="line">            return column;</span><br><span class="line">        &#125;else&#123;</span><br><span class="line">            return null;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    public String evaluate(String keywords,ArrayList&lt;String&gt; column,String name)&#123;</span><br><span class="line">        &#x2F;&#x2F;重载evaluate，另一种查询方式，返回name值</span><br><span class="line">        if(column.contains(keywords))&#123;</span><br><span class="line">            return name;</span><br><span class="line">        &#125;else&#123;</span><br><span class="line">            return null;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>使用mvn 打包</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mvn clean package</span><br></pre></td></tr></table></figure>
</li>
<li><p>导入hive</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">add jar &#x2F;home&#x2F;will&#x2F;work&#x2F;projects&#x2F;hive_udf_test&#x2F;target&#x2F;hive_udf_test-1.0-SNAPSHOT.jar;</span><br><span class="line">create temporary function find_in_array as &#39;com.will.FindInArray&#39;;</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive&gt; select find_in_array(&#39;beijing&#39;,worklocations) from users;</span><br><span class="line">OK</span><br><span class="line">[&quot;beijing&quot;,&quot;shanghai&quot;,&quot;tianjin&quot;,&quot;hangzhou&quot;]</span><br><span class="line">NULL</span><br><span class="line">Time taken: 0.424 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<blockquote>
<p>参考：<a href="https://blog.csdn.net/Nougats/article/details/71158318" target="_blank" rel="noopener">https://blog.csdn.net/Nougats/article/details/71158318</a></p>
</blockquote>
<h1 id="udaf"><a href="#udaf" class="headerlink" title="udaf"></a>udaf</h1><blockquote>
<p>参考： </p>
<p><a href="https://blog.51cto.com/xiaolanlan/2397771" target="_blank" rel="noopener">https://blog.51cto.com/xiaolanlan/2397771</a></p>
<p><a href="https://www.cnblogs.com/Rudd/p/5137612.html" target="_blank" rel="noopener">https://www.cnblogs.com/Rudd/p/5137612.html</a></p>
</blockquote>
<h3 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h3><p>hive的udtf有两种，Simple和Generic。这是由于历史问题，Simple先出现，但是因为反射问题效率较低，所以现在推荐使用Generic的写法。</p>
<ul>
<li>Simple。即继承<code>org.apache.hadoop.hive.ql.exec.UDAF</code>类，并在派生类中以静态内部类的方式实现<code>org.apache.hadoop.hive.ql.exec.UDAFEvaluator</code>接口。在Hive源码包<code>org.apache.hadoop.hive.contrib.udaf.example</code>中包含几个示例。可以直接参阅。但是这些接口已经被注解为Deprecated，建议不要使用这种方式开发新的UDAF函数。</li>
<li>Generic。这是Hive社区推荐的新的写法，以抽象类代替原有的接口。新的抽象类<code>org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver</code>替代老的UDAF接口，新的抽象类<code>org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator</code>替代老的UDAFEvaluator接口。</li>
</ul>
<p>hive是立足于hadoop之上，也就是hive基于mapreduce，hive sql最终还是会转化为mapreduce执行。为了实现mapreduce，udaf中用model来表示mapreduce各个阶段。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public static enum Mode &#123;</span><br><span class="line">        PARTIAL1,</span><br><span class="line">        PARTIAL2,</span><br><span class="line">        FINAL,</span><br><span class="line">        COMPLETE;</span><br><span class="line"></span><br><span class="line">        private Mode() &#123;&#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>PARTIAL1: 这个是mapreduce的map阶段:从原始数据到部分数据聚合，将会调用<strong>iterate()</strong>和*<em>terminatePartial() *</em></li>
<li>PARTIAL2: 这个是mapreduce的map端的Combiner阶段，负责在map端合并map的数据::从部分数据聚合到部分数据聚合，将会调用<strong>merge()</strong> 和 <strong>terminatePartial()</strong> </li>
<li>FINAL: mapreduce的reduce阶段，从部分数据的聚合到完全聚合，将会调用<strong>merge()</strong>和*<em>terminate() *</em></li>
</ul>
<hr>
<ul>
<li><p>COMPLETE: 如果出现了这个阶段，表示mapreduce只有map，没有reduce，所以map端就直接出结果了，从原始数据直接到完全聚合，将会调用 <strong>iterate()</strong>和<strong>terminate()</strong></p>
<p><img src="https://i.loli.net/2020/02/23/DlNqFRHnZ6zLuoA.png" alt="image.png"></p>
</li>
</ul>
<p><img src="https://i.loli.net/2020/02/23/IKn3Vv57jPScxs8.png" alt="image.png"></p>
<p>udaf骨架示例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class GenericUDAFHistogramNumeric extends AbstractGenericUDAFResolver &#123;</span><br><span class="line">  static final Log LOG &#x3D; LogFactory.getLog(GenericUDAFHistogramNumeric.class.getName());</span><br><span class="line"> </span><br><span class="line">  @Override</span><br><span class="line">  public GenericUDAFEvaluator getEvaluator(GenericUDAFParameterInfo info) throws SemanticException &#123;</span><br><span class="line">    &#x2F;&#x2F; 这里主要做类型检查</span><br><span class="line"> </span><br><span class="line">    return new GenericUDAFHistogramNumericEvaluator();</span><br><span class="line">  &#125;</span><br><span class="line"> </span><br><span class="line">  public static class GenericUDAFHistogramNumericEvaluator extends GenericUDAFEvaluator &#123;</span><br><span class="line">         &#x2F;&#x2F; 确定各个阶段输入输出参数的数据格式ObjectInspectors</span><br><span class="line">         public  ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException&#123;</span><br><span class="line">             return  null;</span><br><span class="line">         &#125;</span><br><span class="line"></span><br><span class="line">         &#x2F;&#x2F; 保存数据聚集结果的类</span><br><span class="line">         public AggregationBuffer getNewAggregationBuffer() throws HiveException &#123;</span><br><span class="line">             return null;</span><br><span class="line">         &#125;</span><br><span class="line"></span><br><span class="line">		 &#x2F;&#x2F; 重置聚集结果</span><br><span class="line">         public void reset(AggregationBuffer aggregationBuffer) throws HiveException &#123;&#125;</span><br><span class="line"></span><br><span class="line">         &#x2F;&#x2F; map阶段，迭代处理输入sql传过来的列数据 </span><br><span class="line">         public void iterate(AggregationBuffer aggregationBuffer, Object[] objects) throws HiveException &#123;&#125;</span><br><span class="line"></span><br><span class="line">         &#x2F;&#x2F; map与combiner结束返回结果，得到部分数据聚集结果</span><br><span class="line">         public Object terminatePartial(AggregationBuffer aggregationBuffer) throws HiveException &#123;</span><br><span class="line">             return null;</span><br><span class="line">         &#125;</span><br><span class="line"></span><br><span class="line">         &#x2F;&#x2F; combiner合并map返回的结果，还有reducer合并mapper或combiner返回的结果。</span><br><span class="line">         public void merge(AggregationBuffer aggregationBuffer, Object o) throws HiveException &#123;&#125;</span><br><span class="line"></span><br><span class="line">		 &#x2F;&#x2F; reducer阶段，输出最终结果 </span><br><span class="line">         public Object terminate(AggregationBuffer aggregationBuffer) throws HiveException &#123;</span><br><span class="line">             return null;</span><br><span class="line">         &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>





<h3 id="功能-1"><a href="#功能-1" class="headerlink" title="功能"></a>功能</h3><p>统计字符数</p>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">package com.will;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;</span><br><span class="line">import org.apache.hadoop.hive.ql.metadata.HiveException;</span><br><span class="line">import org.apache.hadoop.hive.ql.parse.SemanticException;</span><br><span class="line">import org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver;</span><br><span class="line">import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;</span><br><span class="line">import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;</span><br><span class="line">import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;</span><br><span class="line"></span><br><span class="line">public class TotalNumOfLetttersGenericUDAF extends AbstractGenericUDAFResolver &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticException &#123;</span><br><span class="line"></span><br><span class="line">        if (parameters.length !&#x3D; 1) &#123;</span><br><span class="line">            throw new UDFArgumentTypeException(parameters.length - 1,&quot;Exactly one argument is expected.&quot;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        ObjectInspector oi &#x3D; TypeInfoUtils.getStandardJavaObjectInspectorFromTypeInfo(parameters[0]);</span><br><span class="line"></span><br><span class="line">        if (oi.getCategory() !&#x3D; ObjectInspector.Category.PRIMITIVE)&#123;</span><br><span class="line">            throw new UDFArgumentTypeException(0,</span><br><span class="line">                    &quot;Argument must be PRIMITIVE, but &quot;</span><br><span class="line">                    + oi.getCategory().name()</span><br><span class="line">                    + &quot; was passed.&quot;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        PrimitiveObjectInspector inputOI &#x3D; (PrimitiveObjectInspector) oi;</span><br><span class="line">        if (inputOI.getPrimitiveCategory() !&#x3D; PrimitiveObjectInspector.PrimitiveCategory.STRING)&#123;</span><br><span class="line">            throw new UDFArgumentTypeException(0, &quot;Argument must be String, but &quot;</span><br><span class="line">                     + inputOI.getPrimitiveCategory().name()</span><br><span class="line">                     + &quot; was passed.&quot;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        return new TotalNumOfLettersEvaluator();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static class TotalNumOfLettersEvaluator extends GenericUDAFEvaluator&#123;</span><br><span class="line">        PrimitiveObjectInspector inputOI;</span><br><span class="line">        ObjectInspector outputOI;</span><br><span class="line">        PrimitiveObjectInspector integerOI;</span><br><span class="line"></span><br><span class="line">        int total &#x3D; 0;</span><br><span class="line">        private boolean warned &#x3D; false;</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">         public  ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException&#123;</span><br><span class="line">            assert (parameters.length &#x3D;&#x3D; 1);</span><br><span class="line">            super.init(m, parameters);</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F;map阶段读取sql列，输入为String基础数据格式</span><br><span class="line">            if (m &#x3D;&#x3D; Mode.PARTIAL1 || m &#x3D;&#x3D; Mode.COMPLETE) &#123;</span><br><span class="line">                inputOI &#x3D; (PrimitiveObjectInspector) parameters[0];</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                &#x2F;&#x2F;其余阶段，输入为Integer基础数据格式</span><br><span class="line">                integerOI &#x3D; (PrimitiveObjectInspector) parameters[0];</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; 指定各个阶段输出数据格式都为Integer类型</span><br><span class="line">            outputOI &#x3D; ObjectInspectorFactory.getReflectionObjectInspector(Integer.class,</span><br><span class="line">                                        ObjectInspectorFactory.ObjectInspectorOptions.JAVA);</span><br><span class="line"></span><br><span class="line">            return outputOI;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;存储当前字符总数的类</span><br><span class="line">        static class LetterSumAgg implements AggregationBuffer &#123;</span><br><span class="line">            int sum &#x3D; 0;</span><br><span class="line">            void add(int num)&#123;</span><br><span class="line">                sum +&#x3D; num;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">         public AggregationBuffer getNewAggregationBuffer() throws HiveException &#123;</span><br><span class="line">             LetterSumAgg result &#x3D; new LetterSumAgg();</span><br><span class="line">             return result;</span><br><span class="line">         &#125;</span><br><span class="line"></span><br><span class="line">         public void reset(AggregationBuffer aggregationBuffer) throws HiveException &#123;</span><br><span class="line">             LetterSumAgg myagg &#x3D; new LetterSumAgg();</span><br><span class="line">         &#125;</span><br><span class="line"></span><br><span class="line">         public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException &#123;</span><br><span class="line">             assert (parameters.length &#x3D;&#x3D; 1);</span><br><span class="line">             if (parameters[0] !&#x3D; null) &#123;</span><br><span class="line">                 LetterSumAgg myagg &#x3D; (LetterSumAgg) agg;</span><br><span class="line">                 Object p1 &#x3D; ((PrimitiveObjectInspector) inputOI).getPrimitiveJavaObject(parameters[0]);</span><br><span class="line">                 myagg.add(String.valueOf(p1).length());</span><br><span class="line">             &#125;</span><br><span class="line">         &#125;</span><br><span class="line"></span><br><span class="line">         public Object terminatePartial(AggregationBuffer agg) throws HiveException &#123;</span><br><span class="line">             LetterSumAgg myagg &#x3D; (LetterSumAgg) agg;</span><br><span class="line">             total +&#x3D; myagg.sum;</span><br><span class="line">             return total;</span><br><span class="line">         &#125;</span><br><span class="line"></span><br><span class="line">         public void merge(AggregationBuffer agg, Object partial) throws HiveException &#123;</span><br><span class="line">             if (partial !&#x3D; null) &#123;</span><br><span class="line">                 LetterSumAgg myagg1 &#x3D; (LetterSumAgg) agg;</span><br><span class="line">                 Integer partialSum &#x3D; (Integer) integerOI.getPrimitiveJavaObject(partial);</span><br><span class="line">                 LetterSumAgg myagg2 &#x3D; new LetterSumAgg();</span><br><span class="line">                 myagg2.add(partialSum);</span><br><span class="line">                 myagg1.add(myagg2.sum);</span><br><span class="line">             &#125;</span><br><span class="line">         &#125;</span><br><span class="line"></span><br><span class="line">         public Object terminate(AggregationBuffer agg) throws HiveException &#123;</span><br><span class="line">             LetterSumAgg myagg &#x3D; (LetterSumAgg) agg;</span><br><span class="line">             total &#x3D; myagg.sum;</span><br><span class="line">             return myagg.sum;</span><br><span class="line">         &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h3><p>首先准备数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive&gt; select * from users;</span><br><span class="line">OK</span><br><span class="line">zhangsan	[&quot;beijing&quot;,&quot;shanghai&quot;,&quot;tianjin&quot;,&quot;hangzhou&quot;]</span><br><span class="line">lisi	[&quot;changchu&quot;,&quot;chengdu&quot;,&quot;wuhan&quot;]</span><br></pre></td></tr></table></figure>



<p>然后添加jar包</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; ADD JAR &#x2F;home&#x2F;will&#x2F;work&#x2F;projects&#x2F;hive_udf_test&#x2F;target&#x2F;hive_udf_test-1.0-SNAPSHOT.jar;</span><br><span class="line">Added [&#x2F;home&#x2F;will&#x2F;work&#x2F;projects&#x2F;hive_udf_test&#x2F;target&#x2F;hive_udf_test-1.0-SNAPSHOT.jar] to class path</span><br><span class="line">Added resources: [&#x2F;home&#x2F;will&#x2F;work&#x2F;projects&#x2F;hive_udf_test&#x2F;target&#x2F;hive_udf_test-1.0-SNAPSHOT.jar]</span><br></pre></td></tr></table></figure>



<p>定义函数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive&gt;  CREATE TEMPORARY FUNCTION letters as &#39;com.will.TotalNumOfLetttersGenericUDAF&#39;;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.049 seconds</span><br></pre></td></tr></table></figure>



<p>执行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive&gt; select letters(name) from users;</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1</span><br><span class="line">2020-02-23 13:25:06,087 Stage-1 map &#x3D; 0%,  reduce &#x3D; 0%</span><br><span class="line">2020-02-23 13:25:11,426 Stage-1 map &#x3D; 100%,  reduce &#x3D; 0%, Cumulative CPU 2.03 sec</span><br><span class="line">2020-02-23 13:25:16,607 Stage-1 map &#x3D; 100%,  reduce &#x3D; 100%, Cumulative CPU 4.01 sec</span><br><span class="line">MapReduce Total cumulative CPU time: 4 seconds 10 msec</span><br><span class="line">Total MapReduce CPU Time Spent: 4 seconds 10 msec</span><br><span class="line">OK</span><br><span class="line">12</span><br><span class="line">Time taken: 23.819 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure>



<h1 id="my-own-udaf"><a href="#my-own-udaf" class="headerlink" title="my own udaf"></a>my own udaf</h1><h3 id="要解决的问题"><a href="#要解决的问题" class="headerlink" title="要解决的问题"></a>要解决的问题</h3><p>有一个hive表，其中两列分别代表时间戳和事件。目标是得到指定时间范围的所有事件。</p>
<h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p>根据上一部分的介绍，要实现聚合首先要设计如何存储，传输的问题。在这个过程中我仔细研究了hive udaf示例的histogram设计，然后设计了自己的udaf。聚合存储使用hashmap，初步解析结果使用string的list保存。</p>
<p>代码</p>
<p><a href="https://github.com/zcenao21/hive-udaf" target="_blank" rel="noopener">github 演示项目</a></p>
]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>udf</tag>
        <tag>udaf</tag>
      </tags>
  </entry>
  <entry>
    <title>hive源码调试入门</title>
    <url>/2020/02/21/hive/hive-source-modification/</url>
    <content><![CDATA[<p>在windows上折腾安装好了hadoop，hive因为文件路径太长不支持等各种奇奇怪怪的问题始终运行不起来，下了大决心放弃windows系统，unbuntu走起！重装系统还算顺利，几个小时搞定。然后就是ubuntu上运行hadoop,成功; ubuntu上运行hive，成功！下一步hive源码走起。<a id="more"></a></p>
<p>hive依赖hadoop，看hive启动脚本，最终是调用hadoop启动，而hadoop最终还是执行的java -jar xxx.jar形式，所以决定先从hive java源码入口看。先试一试改动源码打印个信息。</p>
<p>首先在hive的main函数入口增加一行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">System.out.println(&quot;Will&#39;s first hive source code modification: test err print info&quot;);</span><br></pre></td></tr></table></figure>

<img src="https://i.loli.net/2020/02/21/bQOyidqXgsMR3Ca.png" alt="启动" style="zoom:80%;" />

<p>打包，然后替换lib目录下hive-cli-xxx.jar。</p>
<p>运行hive</p>
<p><img src="https://i.loli.net/2020/02/21/LZ16gVt4rfdXqyu.png" alt="选区_001.png"></p>
<p>好啦，正式开启与hive源码的斗争！</p>
]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>源码</tag>
        <tag>开发</tag>
      </tags>
  </entry>
  <entry>
    <title>hadoop在windows 10下安装步骤</title>
    <url>/2020/02/15/hadoop/hadoop-hadoop%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<h1 id="序言"><a href="#序言" class="headerlink" title="序言"></a>序言</h1><p>首先劝大家在有条件情况下能用mac就用mac，再不行用linux系统，在windows上运行hadoop不是一个好主意！真的好麻烦。。。</p>
<a id="more"></a>



<h1 id="准备文件"><a href="#准备文件" class="headerlink" title="准备文件"></a>准备文件</h1><ul>
<li>在官网上下载hadoop的压缩包</li>
<li>然后有个github项目专门做windows下配置的包，具体链接需要自己搜一下</li>
</ul>
<p>我用的是hadoop 2.10.0，然后配置文件和hadoop包都在里面，需要自己下载</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">链接：https:&#x2F;&#x2F;pan.baidu.com&#x2F;s&#x2F;18ZVB89xOUq43gJ7cqlZUGA </span><br><span class="line">提取码：wj3v</span><br></pre></td></tr></table></figure>



<h1 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h1><ul>
<li>安装好java环境，这是基础，网上一堆教程</li>
<li>解压缩hadoop压缩包，然后解压下载的另一个配置文件，直接拷贝覆盖即可</li>
</ul>
<p>这里卡了好久，因为文件路径太长经常解压后部分文件无法解压成功，这个可以参考链接：<a href="https://knowledge.autodesk.com/zh-hans/search-result/caas/sfdcarticles/sfdcarticles/CHS/The-Windows-10-default-path-length-limitation-MAX-PATH-is-256-characters.html" target="_blank" rel="noopener">点这里</a>，最好放在盘的第一层，我就放在C:\下面</p>
<ul>
<li><p>配置hadoop环境变量</p>
<p>我的电脑-&gt;属性-&gt;高级系统设置-&gt;环境变量-&gt;系统变量</p>
<p>新建HADOOP_HOME, 我的配置：C:\hadoop-2.10.0\bin</p>
<p><img src="https://i.loli.net/2020/02/20/T9MjvyiPe8rcE5H.jpg" alt="b146837bly1gbxf3b0kv0j20s9071dfu.jpg"></p>
</li>
</ul>
<p>​     在PATH变量中添加：%HADOOP_HOME%</p>
<ul>
<li><p>编辑 hadoop安装目录下 etc/hadoop/hadoop-env.cmd, 替换JAVA_HOME路径，”D:\program files\Java\jdk1.8.0_171”为JAVA安装路径。</p>
<p>set JAVA_HOME=”D:\program files\Java\jdk1.8.0_171”</p>
<p>然后编辑在hadoop根目录下创建data目录，目录中再创建两个空文件夹datanode和namenode，之后编辑 etc/hadoop/hdfs-site.xml，替换路径/hadoop-2.10.0/为你的hadoop安装根目录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.name.dir&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;&#x2F;hadoop-2.10.0&#x2F;data&#x2F;namenode&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.data.dir&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;&#x2F;hadoop-2.10.0&#x2F;data&#x2F;datanode&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>格式化namenode</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在任意目录执行 hdfs namenode -format</span><br></pre></td></tr></table></figure>
</li>
<li><p>到安装根目录下的sbin目录，执行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">start-all.cmd</span><br></pre></td></tr></table></figure>

<p><img src="https://i.loli.net/2020/02/20/Lupda64fCRmJEHo.jpg" alt="b146837bly1gbxfmuf908j20z50li7gm.jpg"></p>
<p>验证是否成功：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure>

<p>会有以下进程在运行：</p>
<p>NodeManager<br>DataNode<br>ResourceManager<br>NameNode</p>
</li>
</ul>
<h1 id="问题及解决方法"><a href="#问题及解决方法" class="headerlink" title="问题及解决方法"></a>问题及解决方法</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">java.lang.NoClassDefFoundError: org&#x2F;apache&#x2F;hadoop&#x2F;yarn&#x2F;server&#x2F;timelineservice&#x2F;collector&#x2F;TimelineCollectorManager</span><br><span class="line">        at java.lang.ClassLoader.defineClass1(Native Method)</span><br><span class="line">        at java.lang.ClassLoader.defineClass(ClassLoader.java:763)</span><br><span class="line">        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)</span><br><span class="line">        at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)</span><br><span class="line">        at java.net.URLClassLoader.access$100(URLClassLoader.java:73)</span><br><span class="line">        at java.net.URLClassLoader$1.run(URLClassLoader.java:368)</span><br><span class="line">        at java.net.URLClassLoader$1.run(URLClassLoader.java:362)</span><br><span class="line">        at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">        at java.net.URLClassLoader.findClass(URLClassLoader.java:361)</span><br><span class="line">        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</span><br><span class="line">        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)</span><br><span class="line">        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</span><br><span class="line">        at java.lang.Class.getDeclaredMethods0(Native Method)</span><br><span class="line">        at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)</span><br><span class="line">        at java.lang.Class.getDeclaredMethods(Class.java:1975)</span><br><span class="line">        at com.google.inject.spi.InjectionPoint.getInjectionPoints(InjectionPoint.java:688)</span><br><span class="line">        at com.google.inject.spi.InjectionPoint.forInstanceMethodsAndFields(InjectionPoint.java:380)</span><br><span class="line">        at com.google.inject.spi.InjectionPoint.forInstanceMethodsAndFields(InjectionPoint.java:399)</span><br><span class="line">        at com.google.inject.internal.BindingBuilder.toInstance(BindingBuilder.java:84)</span><br><span class="line">        at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.setup(RMWebApp.java:56)</span><br><span class="line">        at org.apache.hadoop.yarn.webapp.WebApp.configureServlets(WebApp.java:160)</span><br><span class="line">        at com.google.inject.servlet.ServletModule.configure(ServletModule.java:55)</span><br><span class="line">        at com.google.inject.AbstractModule.configure(AbstractModule.java:62)</span><br><span class="line">        at com.google.inject.spi.Elements$RecordingBinder.install(Elements.java:340)</span><br><span class="line">        at com.google.inject.spi.Elements.getElements(Elements.java:110)</span><br><span class="line">        at com.google.inject.internal.InjectorShell$Builder.build(InjectorShell.java:138)</span><br><span class="line">        at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:104)</span><br><span class="line">        at com.google.inject.Guice.createInjector(Guice.java:96)</span><br><span class="line">        at com.google.inject.Guice.createInjector(Guice.java:73)</span><br><span class="line">        at com.google.inject.Guice.createInjector(Guice.java:62)</span><br><span class="line">        at org.apache.hadoop.yarn.webapp.WebApps$Builder.build(WebApps.java:356)</span><br><span class="line">        at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:401)</span><br><span class="line">        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:1137)</span><br><span class="line">        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1245)</span><br><span class="line">        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)</span><br><span class="line">        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1446)</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorManager</span><br><span class="line">        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)</span><br><span class="line">        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</span><br><span class="line">        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)</span><br><span class="line">        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</span><br><span class="line">        ... 36 more</span><br></pre></td></tr></table></figure>

<p>*<em>解决方法： *</em>share\hadoop\yarn\timelineservice 下 hadoop-yarn-server-timelineservice-3.0.3.jar copy 到share\hadoop\yarn目录下 </p>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hadoop安装</tag>
        <tag>windows 10</tag>
      </tags>
  </entry>
  <entry>
    <title>hadoop在ubuntu16.0.4的源码调试</title>
    <url>/2020/02/15/hadoop/hadoop-hadoop%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95/</url>
    <content><![CDATA[<h1 id="序言"><a href="#序言" class="headerlink" title="序言"></a>序言</h1><h1 id="问题及解决方法"><a href="#问题及解决方法" class="headerlink" title="问题及解决方法"></a>问题及解决方法</h1><h3 id="找不到或者无法加载主类xxx-xxx"><a href="#找不到或者无法加载主类xxx-xxx" class="headerlink" title="找不到或者无法加载主类xxx.xxx"></a>找不到或者无法加载主类xxx.xxx</h3><p><strong>解决方法：</strong> 点击菜单栏<code>Run--&gt;Edit configuration</code>，在出现的窗口中，勾选<code>Include dependencies with &#39;Provided&#39; scope</code>，点击OK，重新运行</p>
<h3 id="打日志问题"><a href="#打日志问题" class="headerlink" title="打日志问题"></a>打日志问题</h3><p>添加log4j到项目中，我的内容</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">log4j.rootLogger&#x3D;info, ServerDailyRollingFile, stdout</span><br><span class="line">log4j.appender.ServerDailyRollingFile&#x3D;org.apache.log4j.DailyRollingFileAppender</span><br><span class="line">log4j.appender.ServerDailyRollingFile.DatePattern&#x3D;&#39;.&#39;yyyy-MM-dd</span><br><span class="line">log4j.appender.ServerDailyRollingFile.File&#x3D;&#x2F;tmp&#x2F;log-hadoop-will&#x2F;hadoop.log</span><br><span class="line">log4j.appender.ServerDailyRollingFile.layout&#x3D;org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.ServerDailyRollingFile.layout.ConversionPattern&#x3D;%d - %m%n</span><br><span class="line">log4j.appender.ServerDailyRollingFile.Append&#x3D;true</span><br><span class="line">log4j.appender.stdout&#x3D;org.apache.log4j.ConsoleAppender</span><br><span class="line">log4j.appender.stdout.layout&#x3D;org.apache.log4j.PatternLayout</span><br><span class="line">log4j.appender.stdout.layout.ConversionPattern&#x3D;%d&#123;yyyy-MM-dd HH\:mm\:ss&#125; %p [%c] %m%n</span><br></pre></td></tr></table></figure>



<h3 id="resourceManager启动报错"><a href="#resourceManager启动报错" class="headerlink" title="resourceManager启动报错"></a>resourceManager启动报错</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">java.lang.IllegalStateException: Queue configuration missing child queue names for root</span><br><span class="line">    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.parseQueue(CapacityScheduler.java:558)</span><br><span class="line">    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.initializeQueues(CapacityScheduler.java:463)</span><br><span class="line">    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.initScheduler(CapacityScheduler.java:295)</span><br><span class="line">    at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.serviceInit(CapacityScheduler.java:324)</span><br><span class="line">    at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)</span><br></pre></td></tr></table></figure>

<p>hadoop-yarn-server-resourcemanagerde的子目录java/resouces下新建的capacity-scheduler.xml，内容如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type&#x3D;&quot;text&#x2F;xsl&quot; href&#x3D;&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;!--</span><br><span class="line">  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span><br><span class="line">  you may not use this file except in compliance with the License.</span><br><span class="line">  You may obtain a copy of the License at</span><br><span class="line"></span><br><span class="line">    http:&#x2F;&#x2F;www.apache.org&#x2F;licenses&#x2F;LICENSE-2.0</span><br><span class="line"></span><br><span class="line">  Unless required by applicable law or agreed to in writing, software</span><br><span class="line">  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line">  See the License for the specific language governing permissions and</span><br><span class="line">  limitations under the License. See accompanying LICENSE file.</span><br><span class="line">--&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.queues&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;alpha,beta,default&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;description&gt;</span><br><span class="line">      The queues at the this level (root is the root queue).</span><br><span class="line">    &lt;&#x2F;description&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.alpha.capacity&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;50&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;description&gt;Default queue target capacity.&lt;&#x2F;description&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.beta.capacity&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;30&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;description&gt;Default queue target capacity.&lt;&#x2F;description&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.default.capacity&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;20&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;description&gt;Default queue target capacity.&lt;&#x2F;description&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.alpha.state&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;RUNNING&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;description&gt;</span><br><span class="line">      The state of the default queue. State can be one of RUNNING or STOPPED.</span><br><span class="line">    &lt;&#x2F;description&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.beta.state&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;RUNNING&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;description&gt;</span><br><span class="line">      The state of the default queue. State can be one of RUNNING or STOPPED.</span><br><span class="line">    &lt;&#x2F;description&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.default.state&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;RUNNING&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;description&gt;</span><br><span class="line">      The state of the default queue. State can be one of RUNNING or STOPPED.</span><br><span class="line">    &lt;&#x2F;description&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.capacity.root.acl_submit_applications&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;hadoop,yarn,mapred,hdfs&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;description&gt;</span><br><span class="line">      The ACL of who can submit jobs to the root queue.</span><br><span class="line">    &lt;&#x2F;description&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>

<p>然后设置idea<img src="https://github.com/zcenao21/photos-blog/blob/main/hadoop-debug/resouceManeger%E9%85%8D%E7%BD%AE.png?raw=true" alt="resourceManager设置"></p>
<h3 id="not"><a href="#not" class="headerlink" title="not"></a>not</h3><p><img src="https://raw.githubusercontent.com/zcenao21/photos-blog/main/hadoop-debug/namenode-webapp-problem.png" alt="idea配置"></p>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>源码</tag>
        <tag>hadoop调试</tag>
      </tags>
  </entry>
  <entry>
    <title>基础知识</title>
    <url>/2020/02/12/linux/linux-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
    <content><![CDATA[<h1 id="计算机"><a href="#计算机" class="headerlink" title="计算机"></a>计算机</h1><h3 id="组成部分"><a href="#组成部分" class="headerlink" title="组成部分"></a>组成部分</h3><ul>
<li>输入单元</li>
<li>CPU<a id="more"></a></li>
<li>内存</li>
<li>外部存储设备</li>
<li>输出单元</li>
</ul>
<h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><ul>
<li>超级计算机</li>
<li>大型计算机</li>
<li>迷你计算机</li>
<li>工作站</li>
<li>微电脑</li>
</ul>
<h3 id="文件大小"><a href="#文件大小" class="headerlink" title="文件大小"></a>文件大小</h3><p>B=&gt;K=&gt;M=&gt;G=&gt;T=&gt;P=&gt;E</p>
<p>关系都是1024的倍数，如1M=1024K</p>
<h3 id="结构层次"><a href="#结构层次" class="headerlink" title="结构层次"></a>结构层次</h3><p>网络图片，侵删</p>
<p><img src="https://i.loli.net/2020/03/10/kmq52KJte6GyuzW.png" alt="image.png"></p>
<p>普通用户熟悉的是操作系统和应用程序。linux操作系统的核心为linux内核。在linux中，常常用到命令行工具，称为shell。shell是一个用户调用接口，包含各种命令可以和系统交互。内核和系统硬件进行交互。</p>
<h1 id="帮助"><a href="#帮助" class="headerlink" title="帮助"></a>帮助</h1><p>因为传统linux系统只有命令行模式，所以需要使用各种命令，但是记住所有的命令和参数这是几乎不可能的，所以知道如何获取帮助非常重要。</p>
<p>获取帮助的命令主要有三个，help, man, info。这三个得到的查询结果详细程度依次递增。</p>
<p><strong>help命令</strong></p>
<p>help 命令经常使用，可以简洁的列出命令使用方法</p>
<p>示例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">help echo</span><br></pre></td></tr></table></figure>



<p><img src="https://i.loli.net/2020/02/20/jNaxOQpHEnBAWZy.jpg" alt="b146837bly1gbsy3grssnj21fd09qwff.jpg"></p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>基础知识</tag>
      </tags>
  </entry>
  <entry>
    <title>linux---目录</title>
    <url>/2020/02/09/linux/Linux%E7%9B%AE%E5%BD%95/</url>
    <content><![CDATA[<p>linux系统是一种自由和开放源码的类UNIX操作系统。该操作系统的内核由林纳斯·托瓦兹在1991年10月5日首次发布，在加上用户空间的应用程序之后，成为Linux操作系统[1]。linux系统作为当前最流行的系统之一，有各种版本，如centos, ubuntu等，当前移动互联网终端之王—手机—的系统之一Android也是基于linux，可以说随处可以见到linux的身影。</p>
<p>在大数据开发中，linux系统必然会涉及，是必备的基础技能。下面将分为以下一系列文章进行介绍：</p>
<ul>
<li><a href="/2020/02/12/linux/linux-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" title="基础知识">基础知识</a></li>
<li>常用命令</li>
<li><a href="/2020/03/10/linux/linux-%E6%96%87%E4%BB%B6%E6%9D%83%E9%99%90/" title="文件及用户权限">文件及用户权限</a></li>
<li>shell脚本</li>
</ul>
<blockquote>
<p>[1] 维基百科</p>
</blockquote>
]]></content>
      <categories>
        <category>目录</category>
      </categories>
      <tags>
        <tag>linux目录</tag>
      </tags>
  </entry>
  <entry>
    <title>RDD转换</title>
    <url>/2019/11/02/spark/RDD%E8%BD%AC%E6%8D%A2/</url>
    <content><![CDATA[<h1 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h1><p>RDD(Resilient Distributed Datasets，弹性分布式数据集)，是一个惰性计算、静态类型的分布式数据集合，是Spark实现并发计算的基础数据结构。<a id="more"></a></p>
<p>RDD包含以下特性（前3个必须有，后两个可选）：</p>
<ol>
<li><p>partitions()</p>
<p>返回组成分布式数据集的分区对象数组。</p>
</li>
<li><p>itearator(p, parentIters)</p>
<p>为每个父分区计算分区p的iteartors。</p>
</li>
<li><p>dependencies</p>
<p>返回依赖对象序列。</p>
</li>
<li><p>partitioner()—可选</p>
<p>若RDD有相关元素与分区信息，则返回Scala option type的分区对象。</p>
</li>
<li><p>prefferedLocations(p)—可选</p>
<p>返回数据分区的存储位置信息。</p>
</li>
</ol>
<p>针对RDD的处理有丰富的操作，包括Action算子和Transformation算子。Action操作返回对象非RDD，Transformation操作返回仍然是RDD。Action算子是触发行动的算子，<strong>Action算子数量等于Spark Job的数量</strong>；Transformation算子会对RDD进行变换，根据父RDD和子RDD的依赖关系不同，Transformation又分为宽依赖和窄依赖。宽依赖的示意图如下：</p>
<p><img src="https://i.loli.net/2019/11/03/5EpJ6In4DhlVSxj.png" alt="image.png"></p>
<p>窄依赖的严格定义：<strong>each partition of the parrent RDD is used by at most one partition of the child RDD（译：每个父RDD的分区最多被一个子RDD分区使用）</strong>。</p>
<p>这是区分宽窄依赖最严格的定义，还有一个并不是非常严格的说法，但是便于理解：</p>
<p>需要进行shuffle的为宽依赖，不需要的为窄依赖。</p>
<p><strong>Spark Job中的Stage个数就等于宽依赖个数。</strong></p>
<p>常用的Action算子有reduce,collect,count,first,take,aggregate, fold, foreach, saveAsTextFile, countByKey等; 常用的Transformation算子有map, filter, flatmap, mapPartitions, sample, union, intersection, distinct, groupByKey, countByKey, sortBy, join, coalesce, repartition, repartitionAndSortWithinPatitions, mapValues等。</p>
<h1 id="Spark-Job阶段划分"><a href="#Spark-Job阶段划分" class="headerlink" title="Spark Job阶段划分"></a>Spark Job阶段划分</h1><p><img src="https://i.loli.net/2019/11/02/Iw4YD79qiK2h6kp.jpg" alt=""></p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>High Performance Spark</tag>
        <tag>RDD</tag>
      </tags>
  </entry>
  <entry>
    <title>spark---目录</title>
    <url>/2019/11/02/spark/spark%E7%9B%AE%E5%BD%95/</url>
    <content><![CDATA[<p>Spark作为一个开源大数据分析引擎，分布式、计算速度快是其显著的优势，特别适用于机器学习（ML）等计算中需要迭代的算法。下面将主要针对Spark Core部分，分为以下一系列文章进行介绍：</p>
<ul>
<li><a href="/2019/10/24/spark/spark/" title="Spark绪论">Spark绪论</a></li>
<li><a href="/2019/11/02/spark/spark%E6%9E%B6%E6%9E%84/" title="Spark架构">Spark架构</a></li>
<li><a href="/2019/11/02/spark/RDD%E8%BD%AC%E6%8D%A2/" title="RDD转换">RDD转换</a></li>
<li>键值对处理</li>
<li><a href="/2021/04/17/spark/RDD%E5%86%85%E5%B9%95/" title="RDD内幕">RDD内幕</a>



</li>
</ul>
<blockquote>
<p>参考书目：high performance spark, Holden karau &amp; Rachel Warren</p>
</blockquote>
]]></content>
      <categories>
        <category>目录</category>
      </categories>
      <tags>
        <tag>spark目录</tag>
      </tags>
  </entry>
  <entry>
    <title>spark架构</title>
    <url>/2019/11/02/spark/spark%E6%9E%B6%E6%9E%84/</url>
    <content><![CDATA[<h1 id="Spark架构"><a href="#Spark架构" class="headerlink" title="Spark架构"></a>Spark架构</h1><p><img src="https://github.com/zcenao21/photos-blog/blob/main/spark/spark.png?raw=true" alt=""></p>
<a id="more"></a>

<p>一个sparkApplication对应 Driver程序中一个SparkContext，而SparkContext由一系列Spark job组成。一个Worker Node可以由多个Executer组成，但是Executer不能跨Worker Node。</p>
<h1 id="spark数据处理系统"><a href="#spark数据处理系统" class="headerlink" title="spark数据处理系统"></a>spark数据处理系统</h1><p> Spark可以在仅有单个JVM的单台机器上运行，但更常与分布式存储系统和集群管理器串联组成如下数据处理系统。分布式存储系统用来存放数据，集群管理器用来协调管理集群spark任务。Spark目前支持4种集群管理器：Standalone集群管理器，Apache Mesos，Hadoop YARN，EC2。</p>
<p><img src="https://github.com/zcenao21/photos-blog/blob/main/spark/spark-storage.png?raw=true" alt="Spark"></p>
<h1 id="spark生态系统"><a href="#spark生态系统" class="headerlink" title="spark生态系统"></a>spark生态系统</h1><p><img src="https://github.com/zcenao21/photos-blog/blob/main/spark/spark-env.png?raw=true" alt="spark生态系统"></p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>High Performance Spark</tag>
        <tag>spark架构</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark绪论</title>
    <url>/2019/10/24/spark/spark/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><ul>
<li><p>为什么会有spark</p>
<p>现在的计算机性能越来越高，单台机器处理M级别的数据简直小菜一碟。但是如果数据量更大，达到G、T甚至P、E级别，这时单台机器的性能可能就有点不够了。也许你会反驳，目前世界上最快的超级计算<a id="more"></a> 机“Summit”浮点运算速度峰值可达20亿亿次/秒，这相当快了吧！确实很快，但是它的使用和维护费用都很高呀，一般人或者说公司根本负担不起。所以聪明的人类就发明了大数据工具。虽然单台机器的计算性能可能并不是很好，但是如果多台机器联合起来一起做计算，那么不就可以实现相对廉价的机器实现超大数据规模的计算吗？所以诞生了大数据工具Hadoop。Hadoop基于HDFS和Map-Reduce奠定了大数据领域的霸主地位。随后各种工具层出不穷。Spark针对Hadoop计算中慢的缺点，即每次计算结果都要读写到磁盘导致速度变慢的设计，作了如下改进：</p>
<ol>
<li>计算中间结果保存在内存中。Hadoop每次的计算结果都会放回HDFS，这种设计严重影响了计算速度。spark基于内存的设计则改进了这一点。</li>
<li>惰性计算。Spark使用有向无环图（DAG）调度机制，遇到action操作算子才进行实际计算，所有就有了优化空间。</li>
</ol>
<blockquote>
<p>可参考对比：<a href="https://www.zhihu.com/question/26568496" target="_blank" rel="noopener">https://www.zhihu.com/question/26568496</a></p>
</blockquote>
</li>
<li><p>Spark是什么</p>
<p>官方定义：<strong>Apache Spark™</strong> is a unified analytics engine for large-scale data processing。翻译过来就是：用于大规模数据处理的分析引擎。再直白一点就是说：spark是用于快速处理大量数据的工具。</p>
<img src="https://ericfu.me/images/2018/06/spark-banner.png" width="700" hegiht="113" align=center />



</li>
</ul>
<h1 id="和其他工具对比"><a href="#和其他工具对比" class="headerlink" title="和其他工具对比"></a>和其他工具对比</h1><blockquote>
<p>引用自：<a href="https://www.boxuegu.com/news/458.html" target="_blank" rel="noopener">https://www.boxuegu.com/news/458.html</a></p>
</blockquote>
<ul>
<li><p><strong>Hadoop框架</strong></p>
<p>提起大数据，第一个想起的肯定是Hadoop，因为Hadoop是目前世界上应用最广泛的大数据工具，它凭借极高的容错率和极低的硬件价格，在大数据市场上风生水起。Hadoop还是第一个在开源社区上引发高度关注的批处理框架，他提出的Map和Reduce的计算模式简洁而优雅。迄今为止，Hadoop已经成为了一个广阔的生态圈，实现了大量算法和组件。由于Hadoop的计算任务需要在集群的多个节点上多次读写，因此在速度上会稍显劣势，但是其吞吐量也同样是其他框架所不能匹敌的。</p>
</li>
</ul>
<ul>
<li><strong>Storm框架</strong><br>与Hadoop的批处理模式不同，Storm采用的是流计算框架，由Twitter开源并且托管在GitHub上。与Hadoop类似的是，Storm也提出了两个计算角色，分别为Spout和Bolt。如果说Hadoop是水桶，只能一桶一桶的去井里扛，那么Storm就是水龙头，只要打开就可以源源不断的出水。Storm支持的语言也比较多，Java、Ruby、Python等语言都能很好的支持。由于Storm是流计算框架，因此使用的是内存，延迟上有极大的优势，但是Storm不会持久化数据。</li>
</ul>
<ul>
<li><strong>Samza框架</strong><br>Smaza也是一种流计算框架，但他目前只支持JVM语言，灵活度上略显不足，并且Samza必须和Kafka共同使用。但是响应的，其也继承了Kafka的低延时、分区、避免回压等优势。对于已经有Hadoop+Kafka工作环境的团队来说，Samza是一个不错的选择，并且Samza在多个团队使用的时候能体现良好的性能。</li>
</ul>
<ul>
<li><strong>Spark框架</strong><br>Spark属于前两种框架形式的集合体，是一种混合式的计算框架。它既有自带的实时流处理工具，也可以和Hadoop集成，代替其中的MapReduce，甚至Spark还可以单独拿出来部署集群，但是还得借助HDFS等分布式存储系统。Spark的强大之处在于其运算速度，与Storm类似，Spark也是基于内存的，并且在内存满负载的时候，硬盘也能运算，运算结果表示，Spark的速度大约为Hadoop的一百倍，并且其成本可能比Hadoop更低。但是Spark目前还没有像Hadoop哪有拥有上万级别的集群，因此现阶段的Spark和Hadoop搭配起来使用更加合适。</li>
</ul>
<ul>
<li><strong>Flink框架</strong><br>Flink也是一种混合式的计算框架，但是在设计初始，Fink的侧重点在于处理流式数据，这与Spark的设计初衷恰恰相反，而在市场需求的驱使下，两者都在朝着更多的兼容性发展。Flink目前不是很成熟，更多情况下Flink还是起到一个借鉴的作用。</li>
</ul>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>High Performance Spark</tag>
        <tag>大数据工具</tag>
      </tags>
  </entry>
</search>
