<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>github个人网站替换自定义域名</title>
    <url>/2020/02/24/other/github%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99%E6%9B%B4%E6%94%B9%E5%9F%9F%E5%90%8D/</url>
    <content><![CDATA[<p>首先非常感谢github提供的个人网站功能，然后感谢hexo提供了方便的写博客手段。<br>github个人建站，一般访问网站名为xxx.github.io，这种名字对于强迫症患者来说真的是无法忍受。但是github也提供了更改为自定义域名功能，非常人性化。</p>
<a id="more"></a>
<p>所以如果你没有把博客迁到国内服务器的需求，只需要替换一下域名即可。下面说一下我的操作步骤，主要还在申请域名上。</p>
<h3 id="域名申请"><a href="#域名申请" class="headerlink" title="域名申请"></a>域名申请</h3><p>经过我的一番调研，最终选用了godaddy的域名服务。国内的也有很多，大家根据需要选择。<br>接下来就是申请域名了。先选择一个自己喜欢的域名前缀，最好比较好记，方便自己也方便他人。域名的后缀有很多，比如.com,.top，.net等等，价格也不一样，便宜的只需要几元人民币。选完域名付完钱，在使用域名前还需要一个认证的过程，中国大陆可以选择的只有身份证。<br><strong>这里必须吐槽一下，一般需要身份证复印件的都是正反面，然后我就上传了正反面。第一次给拒绝了，理由是空白边缘留的不够宽。这次我认认真真，拍了正反面在照片中间再次上传，等了一天，又给我拒绝了，还是相同的理由！我就很纳闷了。打电话给客服问了才知道，只需要正面，但是他们审核的时候没有这个选项，就给我随便找了个理由。。。</strong>第三次终于申请成功了。</p>
<p>接下来就是域名解析服务器替换，好像他们也提供，不替换是否可行没试。我用国内的DNSpod。</p>
<p>首先需要在godaddy上更改域名服务器。DNSpod对于在godaddy上更改域名服务器的说明已经过时了</p>
<p><img src="https://i.loli.net/2020/02/24/Tdk3JMGxInWoLS2.png" alt="选区_017.png"></p>
<p>替换后如上图</p>
<p>等一段时间，我等了半小时吧，等DNSpod上显示状态为正常就可以了</p>
<p><img src="https://i.loli.net/2020/02/24/sv1zMHZrExTOlAa.png" alt="选区_018.png"></p>
<p>现在再进去添加两条纪录</p>
<p><img src="https://i.loli.net/2020/02/24/1d65DsLzUfHqRPW.png" alt="选区_020.png"></p>
<p>上面的记录值是用下面的命令得到的。这两条表示不输入www和输入www都导向记录值</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ping xxx.github.io</span><br></pre></td></tr></table></figure>

<p>最后，在github上填写替换的域名。我用的hexo部署的，所以在source目录下新建CNAME文件，内容为will21.cn。部署到服务器。</p>
<p>访问will21.cn，搞定！</p>
]]></content>
      <categories>
        <category>other</category>
      </categories>
      <tags>
        <tag>github个人网站</tag>
        <tag>域名修改</tag>
      </tags>
  </entry>
  <entry>
    <title>github个人网站替换自定义域名</title>
    <url>/2020/02/24/github%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99%E6%9B%B4%E6%94%B9%E5%9F%9F%E5%90%8D/</url>
    <content><![CDATA[<p>首先非常感谢github提供的个人网站功能，然后感谢hexo提供了方便的写博客手段。<br>github个人建站，一般访问网站名为xxx.github.io，这种名字对于强迫症患者来说真的是无法忍受。但是github也提供了更改为自定义域名功能，非常人性化。</p>
<a id="more"></a>
<p>所以如果你没有把博客迁到国内服务器的需求，只需要替换一下域名即可。下面说一下我的操作步骤，主要还在申请域名上。</p>
<h3 id="域名申请"><a href="#域名申请" class="headerlink" title="域名申请"></a>域名申请</h3><p>经过我的一番调研，最终选用了godaddy的域名服务。国内的也有很多，大家根据需要选择。<br>接下来就是申请域名了。先选择一个自己喜欢的域名前缀，最好比较好记，方便自己也方便他人。域名的后缀有很多，比如.com,.top，.net等等，价格也不一样，便宜的只需要几元人民币。选完域名付完钱，在使用域名前还需要一个认证的过程，中国大陆可以选择的只有身份证。<br><strong>这里必须吐槽一下，一般需要身份证复印件的都是正反面，然后我就上传了正反面。第一次给拒绝了，理由是空白边缘留的不够宽。这次我认认真真，拍了正反面在照片中间再次上传，等了一天，又给我拒绝了，还是相同的理由！我就很纳闷了。打电话给客服问了才知道，只需要正面，但是他们审核的时候没有这个选项，就给我随便找了个理由。。。</strong>第三次终于申请成功了。</p>
<p>接下来就是域名解析服务器替换，好像他们也提供，不替换是否可行没试。我用国内的DNSpod。</p>
<p>首先需要在godaddy上更改域名服务器。DNSpod对于在godaddy上更改域名服务器的说明已经过时了</p>
<p><img src="https://i.loli.net/2020/02/24/Tdk3JMGxInWoLS2.png" alt="选区_017.png"></p>
<p>替换后如上图</p>
<p>等一段时间，我等了半小时吧，等DNSpod上显示状态为正常就可以了</p>
<p><img src="https://i.loli.net/2020/02/24/sv1zMHZrExTOlAa.png" alt="选区_018.png"></p>
<p>现在再进去添加两条纪录</p>
<p><img src="https://i.loli.net/2020/02/24/1d65DsLzUfHqRPW.png" alt="选区_020.png"></p>
<p>上面的记录值是用下面的命令得到的。这两条表示不输入www和输入www都导向记录值</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ping xxx.github.io</span><br></pre></td></tr></table></figure>

<p>最后，在github上填写替换的域名。我用的hexo部署的，所以在source目录下新建CNAME文件，内容为will21.cn。部署到服务器。</p>
<p>访问will21.cn，搞定！</p>
]]></content>
      <categories>
        <category>other</category>
      </categories>
      <tags>
        <tag>github个人网站</tag>
        <tag>域名修改</tag>
      </tags>
  </entry>
  <entry>
    <title>hive udf&amp;udaf</title>
    <url>/2020/02/21/hive/hive-udf/</url>
    <content><![CDATA[<p>hive为我们定义了很多函数，大多数情况下是能够满足我们的需求的。但是在有些情况下，很有必要自己定义一些函数，这样使用起来非常方便。这就是自定义函数(udf)和自定义聚合函数(udaf,user defined aggregation function)。udf是每行返回一个结果，而udaf则是聚合的结果，多行产生一个结果。另外还有udtf，是一行产生多个结果。目前我还没有用到过udtf，就先不介绍它了。下面分别用两个示例介绍udf和udaf<a id="more"></a></p>
<h1 id="udf"><a href="#udf" class="headerlink" title="udf"></a>udf</h1><h3 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h3><p>查找array中是否包含被查询值</p>
<h3 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h3><ul>
<li><p>测试数据准备</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">zhangsan        beijing,shanghai,tianjin,hangzhou</span><br><span class="line">lisi    changchu,chengdu,wuhan</span><br></pre></td></tr></table></figure>
</li>
<li><p>hive建表与导入</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Create table users(name string, worklocations array&lt;string&gt; ) row format delimited fields terminated by &#39;\t&#39; collection items terminated by &#39;,&#39;; </span><br><span class="line"></span><br><span class="line">load data local inpath &#39;&#x2F;root&#x2F;person.txt &#39; OVERWRITE INTO TABLE users;</span><br></pre></td></tr></table></figure>
</li>
<li><p>udf包生成与导入</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">package com.will;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDF;</span><br><span class="line">import java.util.ArrayList;</span><br><span class="line"></span><br><span class="line">public class FindInArray extends UDF &#123;</span><br><span class="line">    public ArrayList&lt;String&gt; evaluate(String keywords, ArrayList&lt;String&gt; column)&#123;</span><br><span class="line">        &#x2F;&#x2F;参数类型使用arraylist&lt;String&gt;对应hive中的array&lt;string&gt;,而不是String[]</span><br><span class="line">        if(column.contains(keywords))&#123;</span><br><span class="line">            return column;</span><br><span class="line">        &#125;else&#123;</span><br><span class="line">            return null;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    public String evaluate(String keywords,ArrayList&lt;String&gt; column,String name)&#123;</span><br><span class="line">        &#x2F;&#x2F;重载evaluate，另一种查询方式，返回name值</span><br><span class="line">        if(column.contains(keywords))&#123;</span><br><span class="line">            return name;</span><br><span class="line">        &#125;else&#123;</span><br><span class="line">            return null;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>使用mvn 打包</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mvn clean package</span><br></pre></td></tr></table></figure>
</li>
<li><p>导入hive</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">add jar &#x2F;home&#x2F;will&#x2F;work&#x2F;projects&#x2F;hive_udf_test&#x2F;target&#x2F;hive_udf_test-1.0-SNAPSHOT.jar;</span><br><span class="line">create temporary function find_in_array as &#39;com.will.FindInArray&#39;;</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive&gt; select find_in_array(&#39;beijing&#39;,worklocations) from users;</span><br><span class="line">OK</span><br><span class="line">[&quot;beijing&quot;,&quot;shanghai&quot;,&quot;tianjin&quot;,&quot;hangzhou&quot;]</span><br><span class="line">NULL</span><br><span class="line">Time taken: 0.424 seconds, Fetched: 2 row(s)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<blockquote>
<p>参考：<a href="https://blog.csdn.net/Nougats/article/details/71158318" target="_blank" rel="noopener">https://blog.csdn.net/Nougats/article/details/71158318</a></p>
</blockquote>
<h1 id="udaf"><a href="#udaf" class="headerlink" title="udaf"></a>udaf</h1><blockquote>
<p>参考： </p>
<p><a href="https://blog.51cto.com/xiaolanlan/2397771" target="_blank" rel="noopener">https://blog.51cto.com/xiaolanlan/2397771</a></p>
<p><a href="https://www.cnblogs.com/Rudd/p/5137612.html" target="_blank" rel="noopener">https://www.cnblogs.com/Rudd/p/5137612.html</a></p>
</blockquote>
<h3 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h3><p>hive的udtf有两种，Simple和Generic。这是由于历史问题，Simple先出现，但是因为反射问题效率较低，所以现在推荐使用Generic的写法。</p>
<ul>
<li>Simple。即继承<code>org.apache.hadoop.hive.ql.exec.UDAF</code>类，并在派生类中以静态内部类的方式实现<code>org.apache.hadoop.hive.ql.exec.UDAFEvaluator</code>接口。在Hive源码包<code>org.apache.hadoop.hive.contrib.udaf.example</code>中包含几个示例。可以直接参阅。但是这些接口已经被注解为Deprecated，建议不要使用这种方式开发新的UDAF函数。</li>
<li>Generic。这是Hive社区推荐的新的写法，以抽象类代替原有的接口。新的抽象类<code>org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver</code>替代老的UDAF接口，新的抽象类<code>org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator</code>替代老的UDAFEvaluator接口。</li>
</ul>
<p>hive是立足于hadoop之上，也就是hive基于mapreduce，hive sql最终还是会转化为mapreduce执行。为了实现mapreduce，udaf中用model来表示mapreduce各个阶段。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public static enum Mode &#123;</span><br><span class="line">        PARTIAL1,</span><br><span class="line">        PARTIAL2,</span><br><span class="line">        FINAL,</span><br><span class="line">        COMPLETE;</span><br><span class="line"></span><br><span class="line">        private Mode() &#123;&#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>PARTIAL1: 这个是mapreduce的map阶段:从原始数据到部分数据聚合，将会调用<strong>iterate()</strong>和*<em>terminatePartial() *</em></li>
<li>PARTIAL2: 这个是mapreduce的map端的Combiner阶段，负责在map端合并map的数据::从部分数据聚合到部分数据聚合，将会调用<strong>merge()</strong> 和 <strong>terminatePartial()</strong> </li>
<li>FINAL: mapreduce的reduce阶段，从部分数据的聚合到完全聚合，将会调用<strong>merge()</strong>和*<em>terminate() *</em></li>
</ul>
<hr>
<ul>
<li><p>COMPLETE: 如果出现了这个阶段，表示mapreduce只有map，没有reduce，所以map端就直接出结果了，从原始数据直接到完全聚合，将会调用 <strong>iterate()</strong>和<strong>terminate()</strong></p>
<p><img src="https://i.loli.net/2020/02/23/DlNqFRHnZ6zLuoA.png" alt="image.png"></p>
</li>
</ul>
<p><img src="https://i.loli.net/2020/02/23/IKn3Vv57jPScxs8.png" alt="image.png"></p>
<p>udaf骨架示例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public class GenericUDAFHistogramNumeric extends AbstractGenericUDAFResolver &#123;</span><br><span class="line">  static final Log LOG &#x3D; LogFactory.getLog(GenericUDAFHistogramNumeric.class.getName());</span><br><span class="line"> </span><br><span class="line">  @Override</span><br><span class="line">  public GenericUDAFEvaluator getEvaluator(GenericUDAFParameterInfo info) throws SemanticException &#123;</span><br><span class="line">    &#x2F;&#x2F; 这里主要做类型检查</span><br><span class="line"> </span><br><span class="line">    return new GenericUDAFHistogramNumericEvaluator();</span><br><span class="line">  &#125;</span><br><span class="line"> </span><br><span class="line">  public static class GenericUDAFHistogramNumericEvaluator extends GenericUDAFEvaluator &#123;</span><br><span class="line">         &#x2F;&#x2F; 确定各个阶段输入输出参数的数据格式ObjectInspectors</span><br><span class="line">         public  ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException&#123;</span><br><span class="line">             return  null;</span><br><span class="line">         &#125;</span><br><span class="line"></span><br><span class="line">         &#x2F;&#x2F; 保存数据聚集结果的类</span><br><span class="line">         public AggregationBuffer getNewAggregationBuffer() throws HiveException &#123;</span><br><span class="line">             return null;</span><br><span class="line">         &#125;</span><br><span class="line"></span><br><span class="line">		 &#x2F;&#x2F; 重置聚集结果</span><br><span class="line">         public void reset(AggregationBuffer aggregationBuffer) throws HiveException &#123;&#125;</span><br><span class="line"></span><br><span class="line">         &#x2F;&#x2F; map阶段，迭代处理输入sql传过来的列数据 </span><br><span class="line">         public void iterate(AggregationBuffer aggregationBuffer, Object[] objects) throws HiveException &#123;&#125;</span><br><span class="line"></span><br><span class="line">         &#x2F;&#x2F; map与combiner结束返回结果，得到部分数据聚集结果</span><br><span class="line">         public Object terminatePartial(AggregationBuffer aggregationBuffer) throws HiveException &#123;</span><br><span class="line">             return null;</span><br><span class="line">         &#125;</span><br><span class="line"></span><br><span class="line">         &#x2F;&#x2F; combiner合并map返回的结果，还有reducer合并mapper或combiner返回的结果。</span><br><span class="line">         public void merge(AggregationBuffer aggregationBuffer, Object o) throws HiveException &#123;&#125;</span><br><span class="line"></span><br><span class="line">		 &#x2F;&#x2F; reducer阶段，输出最终结果 </span><br><span class="line">         public Object terminate(AggregationBuffer aggregationBuffer) throws HiveException &#123;</span><br><span class="line">             return null;</span><br><span class="line">         &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>





<h3 id="功能-1"><a href="#功能-1" class="headerlink" title="功能"></a>功能</h3><p>统计字符数</p>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">package com.will;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;</span><br><span class="line">import org.apache.hadoop.hive.ql.metadata.HiveException;</span><br><span class="line">import org.apache.hadoop.hive.ql.parse.SemanticException;</span><br><span class="line">import org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver;</span><br><span class="line">import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;</span><br><span class="line">import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;</span><br><span class="line">import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;</span><br><span class="line">import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;</span><br><span class="line"></span><br><span class="line">public class TotalNumOfLetttersGenericUDAF extends AbstractGenericUDAFResolver &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public GenericUDAFEvaluator getEvaluator(TypeInfo[] parameters) throws SemanticException &#123;</span><br><span class="line"></span><br><span class="line">        if (parameters.length !&#x3D; 1) &#123;</span><br><span class="line">            throw new UDFArgumentTypeException(parameters.length - 1,&quot;Exactly one argument is expected.&quot;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        ObjectInspector oi &#x3D; TypeInfoUtils.getStandardJavaObjectInspectorFromTypeInfo(parameters[0]);</span><br><span class="line"></span><br><span class="line">        if (oi.getCategory() !&#x3D; ObjectInspector.Category.PRIMITIVE)&#123;</span><br><span class="line">            throw new UDFArgumentTypeException(0,</span><br><span class="line">                    &quot;Argument must be PRIMITIVE, but &quot;</span><br><span class="line">                    + oi.getCategory().name()</span><br><span class="line">                    + &quot; was passed.&quot;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        PrimitiveObjectInspector inputOI &#x3D; (PrimitiveObjectInspector) oi;</span><br><span class="line">        if (inputOI.getPrimitiveCategory() !&#x3D; PrimitiveObjectInspector.PrimitiveCategory.STRING)&#123;</span><br><span class="line">            throw new UDFArgumentTypeException(0, &quot;Argument must be String, but &quot;</span><br><span class="line">                     + inputOI.getPrimitiveCategory().name()</span><br><span class="line">                     + &quot; was passed.&quot;);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        return new TotalNumOfLettersEvaluator();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static class TotalNumOfLettersEvaluator extends GenericUDAFEvaluator&#123;</span><br><span class="line">        PrimitiveObjectInspector inputOI;</span><br><span class="line">        ObjectInspector outputOI;</span><br><span class="line">        PrimitiveObjectInspector integerOI;</span><br><span class="line"></span><br><span class="line">        int total &#x3D; 0;</span><br><span class="line">        private boolean warned &#x3D; false;</span><br><span class="line"></span><br><span class="line">        @Override</span><br><span class="line">         public  ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveException&#123;</span><br><span class="line">            assert (parameters.length &#x3D;&#x3D; 1);</span><br><span class="line">            super.init(m, parameters);</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F;map阶段读取sql列，输入为String基础数据格式</span><br><span class="line">            if (m &#x3D;&#x3D; Mode.PARTIAL1 || m &#x3D;&#x3D; Mode.COMPLETE) &#123;</span><br><span class="line">                inputOI &#x3D; (PrimitiveObjectInspector) parameters[0];</span><br><span class="line">            &#125; else &#123;</span><br><span class="line">                &#x2F;&#x2F;其余阶段，输入为Integer基础数据格式</span><br><span class="line">                integerOI &#x3D; (PrimitiveObjectInspector) parameters[0];</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            &#x2F;&#x2F; 指定各个阶段输出数据格式都为Integer类型</span><br><span class="line">            outputOI &#x3D; ObjectInspectorFactory.getReflectionObjectInspector(Integer.class,</span><br><span class="line">                                        ObjectInspectorFactory.ObjectInspectorOptions.JAVA);</span><br><span class="line"></span><br><span class="line">            return outputOI;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;存储当前字符总数的类</span><br><span class="line">        static class LetterSumAgg implements AggregationBuffer &#123;</span><br><span class="line">            int sum &#x3D; 0;</span><br><span class="line">            void add(int num)&#123;</span><br><span class="line">                sum +&#x3D; num;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">         public AggregationBuffer getNewAggregationBuffer() throws HiveException &#123;</span><br><span class="line">             LetterSumAgg result &#x3D; new LetterSumAgg();</span><br><span class="line">             return result;</span><br><span class="line">         &#125;</span><br><span class="line"></span><br><span class="line">         public void reset(AggregationBuffer aggregationBuffer) throws HiveException &#123;</span><br><span class="line">             LetterSumAgg myagg &#x3D; new LetterSumAgg();</span><br><span class="line">         &#125;</span><br><span class="line"></span><br><span class="line">         public void iterate(AggregationBuffer agg, Object[] parameters) throws HiveException &#123;</span><br><span class="line">             assert (parameters.length &#x3D;&#x3D; 1);</span><br><span class="line">             if (parameters[0] !&#x3D; null) &#123;</span><br><span class="line">                 LetterSumAgg myagg &#x3D; (LetterSumAgg) agg;</span><br><span class="line">                 Object p1 &#x3D; ((PrimitiveObjectInspector) inputOI).getPrimitiveJavaObject(parameters[0]);</span><br><span class="line">                 myagg.add(String.valueOf(p1).length());</span><br><span class="line">             &#125;</span><br><span class="line">         &#125;</span><br><span class="line"></span><br><span class="line">         public Object terminatePartial(AggregationBuffer agg) throws HiveException &#123;</span><br><span class="line">             LetterSumAgg myagg &#x3D; (LetterSumAgg) agg;</span><br><span class="line">             total +&#x3D; myagg.sum;</span><br><span class="line">             return total;</span><br><span class="line">         &#125;</span><br><span class="line"></span><br><span class="line">         public void merge(AggregationBuffer agg, Object partial) throws HiveException &#123;</span><br><span class="line">             if (partial !&#x3D; null) &#123;</span><br><span class="line">                 LetterSumAgg myagg1 &#x3D; (LetterSumAgg) agg;</span><br><span class="line">                 Integer partialSum &#x3D; (Integer) integerOI.getPrimitiveJavaObject(partial);</span><br><span class="line">                 LetterSumAgg myagg2 &#x3D; new LetterSumAgg();</span><br><span class="line">                 myagg2.add(partialSum);</span><br><span class="line">                 myagg1.add(myagg2.sum);</span><br><span class="line">             &#125;</span><br><span class="line">         &#125;</span><br><span class="line"></span><br><span class="line">         public Object terminate(AggregationBuffer agg) throws HiveException &#123;</span><br><span class="line">             LetterSumAgg myagg &#x3D; (LetterSumAgg) agg;</span><br><span class="line">             total &#x3D; myagg.sum;</span><br><span class="line">             return myagg.sum;</span><br><span class="line">         &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h3><p>首先准备数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive&gt; select * from users;</span><br><span class="line">OK</span><br><span class="line">zhangsan	[&quot;beijing&quot;,&quot;shanghai&quot;,&quot;tianjin&quot;,&quot;hangzhou&quot;]</span><br><span class="line">lisi	[&quot;changchu&quot;,&quot;chengdu&quot;,&quot;wuhan&quot;]</span><br></pre></td></tr></table></figure>



<p>然后添加jar包</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt; ADD JAR &#x2F;home&#x2F;will&#x2F;work&#x2F;projects&#x2F;hive_udf_test&#x2F;target&#x2F;hive_udf_test-1.0-SNAPSHOT.jar;</span><br><span class="line">Added [&#x2F;home&#x2F;will&#x2F;work&#x2F;projects&#x2F;hive_udf_test&#x2F;target&#x2F;hive_udf_test-1.0-SNAPSHOT.jar] to class path</span><br><span class="line">Added resources: [&#x2F;home&#x2F;will&#x2F;work&#x2F;projects&#x2F;hive_udf_test&#x2F;target&#x2F;hive_udf_test-1.0-SNAPSHOT.jar]</span><br></pre></td></tr></table></figure>



<p>定义函数</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive&gt;  CREATE TEMPORARY FUNCTION letters as &#39;com.will.TotalNumOfLetttersGenericUDAF&#39;;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.049 seconds</span><br></pre></td></tr></table></figure>



<p>执行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive&gt; select letters(name) from users;</span><br><span class="line">Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1</span><br><span class="line">2020-02-23 13:25:06,087 Stage-1 map &#x3D; 0%,  reduce &#x3D; 0%</span><br><span class="line">2020-02-23 13:25:11,426 Stage-1 map &#x3D; 100%,  reduce &#x3D; 0%, Cumulative CPU 2.03 sec</span><br><span class="line">2020-02-23 13:25:16,607 Stage-1 map &#x3D; 100%,  reduce &#x3D; 100%, Cumulative CPU 4.01 sec</span><br><span class="line">MapReduce Total cumulative CPU time: 4 seconds 10 msec</span><br><span class="line">Total MapReduce CPU Time Spent: 4 seconds 10 msec</span><br><span class="line">OK</span><br><span class="line">12</span><br><span class="line">Time taken: 23.819 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>udf</tag>
        <tag>udaf</tag>
      </tags>
  </entry>
  <entry>
    <title>hive源码调试入门</title>
    <url>/2020/02/21/hive/hive-source-modification/</url>
    <content><![CDATA[<p>在windows上折腾安装好了hadoop，hive因为文件路径太长不支持等各种奇奇怪怪的问题始终运行不起来，下了大决心放弃windows系统，unbuntu走起！重装系统还算顺利，几个小时搞定。然后就是ubuntu上运行hadoop,成功; ubuntu上运行hive，成功！下一步hive源码走起。<a id="more"></a></p>
<p>hive依赖hadoop，看hive启动脚本，最终是调用hadoop启动，而hadoop最终还是执行的java -jar xxx.jar形式，所以决定先从hive java源码入口看。先试一试改动源码打印个信息。</p>
<p>首先在hive的main函数入口增加一行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">System.out.println(&quot;Will&#39;s first hive source code modification: test err print info&quot;);</span><br></pre></td></tr></table></figure>

<img src="https://i.loli.net/2020/02/21/bQOyidqXgsMR3Ca.png" alt="启动" style="zoom:80%;" />

<p>打包，然后替换lib目录下hive-cli-xxx.jar。</p>
<p>运行hive</p>
<p><img src="https://i.loli.net/2020/02/21/LZ16gVt4rfdXqyu.png" alt="选区_001.png"></p>
<p>好啦，正式开启与hive源码的斗争！</p>
]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>hive</tag>
        <tag>源码</tag>
        <tag>开发</tag>
      </tags>
  </entry>
  <entry>
    <title>hadoop在windows 10下安装步骤</title>
    <url>/2020/02/15/hadoop/hadoop-hadoop%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<h1 id="序言"><a href="#序言" class="headerlink" title="序言"></a>序言</h1><p>首先劝大家在有条件情况下能用mac就用mac，再不行用linux系统，在windows上运行hadoop不是一个好主意！真的好麻烦。。。</p>
<a id="more"></a>



<h1 id="准备文件"><a href="#准备文件" class="headerlink" title="准备文件"></a>准备文件</h1><ul>
<li>在官网上下载hadoop的压缩包</li>
<li>然后有个github项目专门做windows下配置的包，具体链接需要自己搜一下</li>
</ul>
<p>我用的是hadoop 2.10.0，然后配置文件和hadoop包都在里面，需要自己下载</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">链接：https:&#x2F;&#x2F;pan.baidu.com&#x2F;s&#x2F;18ZVB89xOUq43gJ7cqlZUGA </span><br><span class="line">提取码：wj3v</span><br></pre></td></tr></table></figure>



<h1 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h1><ul>
<li>安装好java环境，这是基础，网上一堆教程</li>
<li>解压缩hadoop压缩包，然后解压下载的另一个配置文件，直接拷贝覆盖即可</li>
</ul>
<p>这里卡了好久，因为文件路径太长经常解压后部分文件无法解压成功，这个可以参考链接：<a href="https://knowledge.autodesk.com/zh-hans/search-result/caas/sfdcarticles/sfdcarticles/CHS/The-Windows-10-default-path-length-limitation-MAX-PATH-is-256-characters.html" target="_blank" rel="noopener">点这里</a>，最好放在盘的第一层，我就放在C:\下面</p>
<ul>
<li><p>配置hadoop环境变量</p>
<p>我的电脑-&gt;属性-&gt;高级系统设置-&gt;环境变量-&gt;系统变量</p>
<p>新建HADOOP_HOME, 我的配置：C:\hadoop-2.10.0\bin</p>
<p><img src="https://i.loli.net/2020/02/20/T9MjvyiPe8rcE5H.jpg" alt="b146837bly1gbxf3b0kv0j20s9071dfu.jpg"></p>
</li>
</ul>
<p>​     在PATH变量中添加：%HADOOP_HOME%</p>
<ul>
<li><p>编辑 hadoop安装目录下 etc/hadoop/hadoop-env.cmd, 替换JAVA_HOME路径，”D:\program files\Java\jdk1.8.0_171”为JAVA安装路径。</p>
<p>set JAVA_HOME=”D:\program files\Java\jdk1.8.0_171”</p>
<p>然后编辑在hadoop根目录下创建data目录，目录中再创建两个空文件夹datanode和namenode，之后编辑 etc/hadoop/hdfs-site.xml，替换路径/hadoop-2.10.0/为你的hadoop安装根目录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.name.dir&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;&#x2F;hadoop-2.10.0&#x2F;data&#x2F;namenode&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.data.dir&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;&#x2F;hadoop-2.10.0&#x2F;data&#x2F;datanode&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>格式化namenode</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在任意目录执行 hdfs namenode -format</span><br></pre></td></tr></table></figure>
</li>
<li><p>到安装根目录下的sbin目录，执行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">start-all.cmd</span><br></pre></td></tr></table></figure>

<p><img src="https://i.loli.net/2020/02/20/Lupda64fCRmJEHo.jpg" alt="b146837bly1gbxfmuf908j20z50li7gm.jpg"></p>
<p>验证是否成功：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure>

<p>会有以下进程在运行：</p>
<p>NodeManager<br>DataNode<br>ResourceManager<br>NameNode</p>
</li>
</ul>
<h1 id="问题及解决方法"><a href="#问题及解决方法" class="headerlink" title="问题及解决方法"></a>问题及解决方法</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">java.lang.NoClassDefFoundError: org&#x2F;apache&#x2F;hadoop&#x2F;yarn&#x2F;server&#x2F;timelineservice&#x2F;collector&#x2F;TimelineCollectorManager</span><br><span class="line">        at java.lang.ClassLoader.defineClass1(Native Method)</span><br><span class="line">        at java.lang.ClassLoader.defineClass(ClassLoader.java:763)</span><br><span class="line">        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)</span><br><span class="line">        at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)</span><br><span class="line">        at java.net.URLClassLoader.access$100(URLClassLoader.java:73)</span><br><span class="line">        at java.net.URLClassLoader$1.run(URLClassLoader.java:368)</span><br><span class="line">        at java.net.URLClassLoader$1.run(URLClassLoader.java:362)</span><br><span class="line">        at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">        at java.net.URLClassLoader.findClass(URLClassLoader.java:361)</span><br><span class="line">        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</span><br><span class="line">        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)</span><br><span class="line">        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</span><br><span class="line">        at java.lang.Class.getDeclaredMethods0(Native Method)</span><br><span class="line">        at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)</span><br><span class="line">        at java.lang.Class.getDeclaredMethods(Class.java:1975)</span><br><span class="line">        at com.google.inject.spi.InjectionPoint.getInjectionPoints(InjectionPoint.java:688)</span><br><span class="line">        at com.google.inject.spi.InjectionPoint.forInstanceMethodsAndFields(InjectionPoint.java:380)</span><br><span class="line">        at com.google.inject.spi.InjectionPoint.forInstanceMethodsAndFields(InjectionPoint.java:399)</span><br><span class="line">        at com.google.inject.internal.BindingBuilder.toInstance(BindingBuilder.java:84)</span><br><span class="line">        at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.setup(RMWebApp.java:56)</span><br><span class="line">        at org.apache.hadoop.yarn.webapp.WebApp.configureServlets(WebApp.java:160)</span><br><span class="line">        at com.google.inject.servlet.ServletModule.configure(ServletModule.java:55)</span><br><span class="line">        at com.google.inject.AbstractModule.configure(AbstractModule.java:62)</span><br><span class="line">        at com.google.inject.spi.Elements$RecordingBinder.install(Elements.java:340)</span><br><span class="line">        at com.google.inject.spi.Elements.getElements(Elements.java:110)</span><br><span class="line">        at com.google.inject.internal.InjectorShell$Builder.build(InjectorShell.java:138)</span><br><span class="line">        at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:104)</span><br><span class="line">        at com.google.inject.Guice.createInjector(Guice.java:96)</span><br><span class="line">        at com.google.inject.Guice.createInjector(Guice.java:73)</span><br><span class="line">        at com.google.inject.Guice.createInjector(Guice.java:62)</span><br><span class="line">        at org.apache.hadoop.yarn.webapp.WebApps$Builder.build(WebApps.java:356)</span><br><span class="line">        at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:401)</span><br><span class="line">        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:1137)</span><br><span class="line">        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1245)</span><br><span class="line">        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)</span><br><span class="line">        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1446)</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorManager</span><br><span class="line">        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)</span><br><span class="line">        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</span><br><span class="line">        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)</span><br><span class="line">        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</span><br><span class="line">        ... 36 more</span><br></pre></td></tr></table></figure>

<p>*<em>解决方法： *</em>share\hadoop\yarn\timelineservice 下 hadoop-yarn-server-timelineservice-3.0.3.jar copy 到share\hadoop\yarn目录下 </p>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hadoop安装</tag>
        <tag>windows 10</tag>
      </tags>
  </entry>
  <entry>
    <title>基础知识</title>
    <url>/2020/02/12/linux/linux-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
    <content><![CDATA[<h1 id="计算机"><a href="#计算机" class="headerlink" title="计算机"></a>计算机</h1><h3 id="组成部分"><a href="#组成部分" class="headerlink" title="组成部分"></a>组成部分</h3><ul>
<li>输入单元</li>
<li>CPU<a id="more"></a></li>
<li>内存</li>
<li>外部存储设备</li>
<li>输出单元</li>
</ul>
<h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><ul>
<li>超级计算机</li>
<li>大型计算机</li>
<li>迷你计算机</li>
<li>工作站</li>
<li>微电脑</li>
</ul>
<h3 id="文件大小"><a href="#文件大小" class="headerlink" title="文件大小"></a>文件大小</h3><p>B=&gt;K=&gt;M=&gt;G=&gt;T=&gt;P=&gt;E</p>
<p>关系都是1024的倍数，如1M=1024K</p>
<h1 id="帮助"><a href="#帮助" class="headerlink" title="帮助"></a>帮助</h1><p>因为传统linux系统只有命令行模式，所以需要使用各种命令，但是记住所有的命令和参数这是几乎不可能的，所以知道如何获取帮助非常重要。</p>
<p>获取帮助的命令主要有三个，help, man, info。这三个得到的查询结果详细程度依次递增。</p>
<p><strong>help命令</strong></p>
<p>help 命令经常使用，可以简洁的列出命令使用方法</p>
<p>示例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">help echo</span><br></pre></td></tr></table></figure>



<p><img src="https://i.loli.net/2020/02/20/jNaxOQpHEnBAWZy.jpg" alt="b146837bly1gbsy3grssnj21fd09qwff.jpg"></p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>基础知识</tag>
      </tags>
  </entry>
  <entry>
    <title>linux---目录</title>
    <url>/2020/02/09/linux/Linux%E7%9B%AE%E5%BD%95/</url>
    <content><![CDATA[<p>linux系统是一种自由和开放源码的类UNIX操作系统。该操作系统的内核由林纳斯·托瓦兹在1991年10月5日首次发布，在加上用户空间的应用程序之后，成为Linux操作系统[1]。linux系统作为当前最流行的系统之一，有各种版本，如centos, ubuntu等，当前移动互联网终端之王—手机—的系统之一Android也是基于linux，可以说随处可以见到linux的身影。</p>
<p>在大数据开发中，linux系统必然会涉及，是必备的基础技能。下面将分为以下一系列文章进行介绍：</p>
<ul>
<li><a href="/2020/02/12/linux/linux-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" title="基础知识">基础知识</a></li>
<li>常用命令</li>
<li>文件权限</li>
<li>shell脚本</li>
</ul>
<blockquote>
<p>[1] 维基百科</p>
</blockquote>
]]></content>
      <categories>
        <category>目录</category>
      </categories>
      <tags>
        <tag>linux目录</tag>
      </tags>
  </entry>
  <entry>
    <title>RDD转换</title>
    <url>/2019/11/02/spark/RDD%E8%BD%AC%E6%8D%A2/</url>
    <content><![CDATA[<h1 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h1><p>RDD(Resilient Distributed Datasets，弹性分布式数据集)，是一个惰性计算、静态类型的分布式数据集合，是Spark实现并发计算的基础数据结构。<a id="more"></a></p>
<p>RDD包含以下特性（前3个必须有，后两个可选）：</p>
<ol>
<li><p>partitions()</p>
<p>返回组成分布式数据集的分区对象数组。</p>
</li>
<li><p>itearator(p, parentIters)</p>
<p>为每个父分区计算分区p的iteartors。</p>
</li>
<li><p>dependencies</p>
<p>返回依赖对象序列。</p>
</li>
<li><p>partitioner()—可选</p>
<p>若RDD有相关元素与分区信息，则返回Scala option type的分区对象。</p>
</li>
<li><p>prefferedLocations(p)—可选</p>
<p>返回数据分区的存储位置信息。</p>
</li>
</ol>
<p>针对RDD的处理有丰富的操作，包括Action算子和Transformation算子。Action操作返回对象非RDD，Transformation操作返回仍然是RDD。Action算子是触发行动的算子，<strong>Action算子数量等于Spark Job的数量</strong>；Transformation算子会对RDD进行变换，根据父RDD和子RDD的依赖关系不同，Transformation又分为宽依赖和窄依赖。宽依赖的示意图如下：</p>
<p><img src="https://i.loli.net/2019/11/03/5EpJ6In4DhlVSxj.png" alt="image.png"></p>
<p>窄依赖的严格定义：<strong>each partition of the parrent RDD is used by at most one partition of the child RDD（译：每个父RDD的分区最多被一个子RDD分区使用）</strong>。</p>
<p>这是区分宽窄依赖最严格的定义，还有一个并不是非常严格的说法，但是便于理解：</p>
<p>需要进行shuffle的为宽依赖，不需要的为窄依赖。</p>
<p><strong>Spark Job中的Stage个数就等于宽依赖个数。</strong></p>
<p>常用的Action算子有reduce,collect,count,first,take,aggregate, fold, foreach, saveAsTextFile, countByKey等; 常用的Transformation算子有map, filter, flatmap, mapPartitions, sample, union, intersection, distinct, groupByKey, countByKey, sortBy, join, coalesce, repartition, repartitionAndSortWithinPatitions, mapValues等。</p>
<h1 id="Spark-Job阶段划分"><a href="#Spark-Job阶段划分" class="headerlink" title="Spark Job阶段划分"></a>Spark Job阶段划分</h1><p><img src="https://i.loli.net/2019/11/02/Iw4YD79qiK2h6kp.jpg" alt=""></p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>High Performance Spark</tag>
        <tag>RDD</tag>
      </tags>
  </entry>
  <entry>
    <title>spark---目录</title>
    <url>/2019/11/02/spark/spark%E7%9B%AE%E5%BD%95/</url>
    <content><![CDATA[<p>Spark作为一个开源大数据分析引擎，分布式、计算速度快是其显著的优势，特别适用于机器学习（ML）等计算中需要迭代的算法。下面将主要针对Spark Core部分，分为以下一系列文章进行介绍：</p>
<ul>
<li><a href="/2019/10/24/spark/spark/" title="Spark绪论">Spark绪论</a></li>
<li><a href="/2019/11/02/spark/spark%E6%9E%B6%E6%9E%84/" title="Spark架构">Spark架构</a></li>
<li><a href="/2019/11/02/spark/RDD%E8%BD%AC%E6%8D%A2/" title="RDD转换">RDD转换</a></li>
<li>键值对处理</li>
</ul>
<blockquote>
<p>参考书目：high performance spark, Holden karau &amp; Rachel Warren</p>
</blockquote>
]]></content>
      <categories>
        <category>目录</category>
      </categories>
      <tags>
        <tag>spark目录</tag>
      </tags>
  </entry>
  <entry>
    <title>spark架构</title>
    <url>/2019/11/02/spark/spark%E6%9E%B6%E6%9E%84/</url>
    <content><![CDATA[<h1 id="Spark架构"><a href="#Spark架构" class="headerlink" title="Spark架构"></a>Spark架构</h1><p><img src="https://i.loli.net/2019/11/02/TLMwH9aV1xStu5Y.png" alt=""></p>
<a id="more"></a>

<p>一个sparkApplication对应 Driver程序中一个SparkContext，而SparkContext由一系列Spark job组成。一个Worker Node可以由多个Executer组成，但是Executer不能跨Worker Node。</p>
<h1 id="spark数据处理系统"><a href="#spark数据处理系统" class="headerlink" title="spark数据处理系统"></a>spark数据处理系统</h1><p> Spark可以在仅有单个JVM的单台机器上运行，但更常与分布式存储系统和集群管理器串联组成如下数据处理系统。分布式存储系统用来存放数据，集群管理器用来协调管理集群spark任务。Spark目前支持4种集群管理器：Standalone集群管理器，Apache Mesos，Hadoop YARN，EC2。</p>
<p><img src="https://i.loli.net/2019/11/02/5ijlposeYMDScIU.png" alt="Spark"></p>
<h1 id="spark生态系统"><a href="#spark生态系统" class="headerlink" title="spark生态系统"></a>spark生态系统</h1><p><img src="https://i.loli.net/2019/11/02/B2JjgU7hczEba1r.png" alt="spark生态系统"></p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>High Performance Spark</tag>
        <tag>spark架构</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark绪论</title>
    <url>/2019/10/24/spark/spark/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><ul>
<li><p>为什么会有spark</p>
<p>现在的计算机性能越来越高，单台机器处理M级别的数据简直小菜一碟。但是如果数据量更大，达到G、T甚至P、E级别，这时单台机器的性能可能就有点不够了。也许你会反驳，目前世界上最快的超级计算<a id="more"></a> 机“Summit”浮点运算速度峰值可达20亿亿次/秒，这相当快了吧！确实很快，但是它的使用和维护费用都很高呀，一般人或者说公司根本负担不起。所以聪明的人类就发明了大数据工具。虽然单台机器的计算性能可能并不是很好，但是如果多台机器联合起来一起做计算，那么不就可以实现相对廉价的机器实现超大数据规模的计算吗？所以诞生了大数据工具Hadoop。Hadoop基于HDFS和Map-Reduce奠定了大数据领域的霸主地位。随后各种工具层出不穷。Spark针对Hadoop计算中慢的缺点，即每次计算结果都要读写到磁盘导致速度变慢的设计，作了如下改进：</p>
<ol>
<li>计算中间结果保存在内存中。Hadoop每次的计算结果都会放回HDFS，这种设计严重影响了计算速度。spark基于内存的设计则改进了这一点。</li>
<li>惰性计算。Spark使用有向无环图（DAG）调度机制，遇到action操作算子才进行实际计算，所有就有了优化空间。</li>
</ol>
<blockquote>
<p>可参考对比：<a href="https://www.zhihu.com/question/26568496" target="_blank" rel="noopener">https://www.zhihu.com/question/26568496</a></p>
</blockquote>
</li>
<li><p>Spark是什么</p>
<p>官方定义：<strong>Apache Spark™</strong> is a unified analytics engine for large-scale data processing。翻译过来就是：用于大规模数据处理的分析引擎。再直白一点就是说：spark是用于快速处理大量数据的工具。</p>
<img src="https://ericfu.me/images/2018/06/spark-banner.png" width="700" hegiht="113" align=center />



</li>
</ul>
<h1 id="和其他工具对比"><a href="#和其他工具对比" class="headerlink" title="和其他工具对比"></a>和其他工具对比</h1><blockquote>
<p>引用自：<a href="https://www.boxuegu.com/news/458.html" target="_blank" rel="noopener">https://www.boxuegu.com/news/458.html</a></p>
</blockquote>
<ul>
<li><p><strong>Hadoop框架</strong></p>
<p>提起大数据，第一个想起的肯定是Hadoop，因为Hadoop是目前世界上应用最广泛的大数据工具，它凭借极高的容错率和极低的硬件价格，在大数据市场上风生水起。Hadoop还是第一个在开源社区上引发高度关注的批处理框架，他提出的Map和Reduce的计算模式简洁而优雅。迄今为止，Hadoop已经成为了一个广阔的生态圈，实现了大量算法和组件。由于Hadoop的计算任务需要在集群的多个节点上多次读写，因此在速度上会稍显劣势，但是其吞吐量也同样是其他框架所不能匹敌的。</p>
</li>
</ul>
<ul>
<li><strong>Storm框架</strong><br>与Hadoop的批处理模式不同，Storm采用的是流计算框架，由Twitter开源并且托管在GitHub上。与Hadoop类似的是，Storm也提出了两个计算角色，分别为Spout和Bolt。如果说Hadoop是水桶，只能一桶一桶的去井里扛，那么Storm就是水龙头，只要打开就可以源源不断的出水。Storm支持的语言也比较多，Java、Ruby、Python等语言都能很好的支持。由于Storm是流计算框架，因此使用的是内存，延迟上有极大的优势，但是Storm不会持久化数据。</li>
</ul>
<ul>
<li><strong>Samza框架</strong><br>Smaza也是一种流计算框架，但他目前只支持JVM语言，灵活度上略显不足，并且Samza必须和Kafka共同使用。但是响应的，其也继承了Kafka的低延时、分区、避免回压等优势。对于已经有Hadoop+Kafka工作环境的团队来说，Samza是一个不错的选择，并且Samza在多个团队使用的时候能体现良好的性能。</li>
</ul>
<ul>
<li><strong>Spark框架</strong><br>Spark属于前两种框架形式的集合体，是一种混合式的计算框架。它既有自带的实时流处理工具，也可以和Hadoop集成，代替其中的MapReduce，甚至Spark还可以单独拿出来部署集群，但是还得借助HDFS等分布式存储系统。Spark的强大之处在于其运算速度，与Storm类似，Spark也是基于内存的，并且在内存满负载的时候，硬盘也能运算，运算结果表示，Spark的速度大约为Hadoop的一百倍，并且其成本可能比Hadoop更低。但是Spark目前还没有像Hadoop哪有拥有上万级别的集群，因此现阶段的Spark和Hadoop搭配起来使用更加合适。</li>
</ul>
<ul>
<li><strong>Flink框架</strong><br>Flink也是一种混合式的计算框架，但是在设计初始，Fink的侧重点在于处理流式数据，这与Spark的设计初衷恰恰相反，而在市场需求的驱使下，两者都在朝着更多的兼容性发展。Flink目前不是很成熟，更多情况下Flink还是起到一个借鉴的作用。</li>
</ul>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>High Performance Spark</tag>
        <tag>大数据工具</tag>
      </tags>
  </entry>
</search>
