<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>hadoop在windows 10下安装步骤</title>
    <url>/2020/02/15/hadoop-hadoop%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<h1 id="序言"><a href="#序言" class="headerlink" title="序言"></a>序言</h1><p>首先劝大家在有条件情况下能用mac就用mac，再不行用linux系统，在windows上运行hadoop不是一个好主意！真的好麻烦。。。</p>
<a id="more"></a>



<h1 id="准备文件"><a href="#准备文件" class="headerlink" title="准备文件"></a>准备文件</h1><ul>
<li>在官网上下载hadoop的压缩包</li>
<li>然后有个github项目专门做windows下配置的包，具体链接需要自己搜一下</li>
</ul>
<p>我用的是hadoop 2.10.0，然后配置文件和hadoop包都在里面，需要自己下载</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">链接：https:&#x2F;&#x2F;pan.baidu.com&#x2F;s&#x2F;18ZVB89xOUq43gJ7cqlZUGA </span><br><span class="line">提取码：wj3v</span><br></pre></td></tr></table></figure>



<h1 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h1><ul>
<li>安装好java环境，这是基础，网上一堆教程</li>
<li>解压缩hadoop压缩包，然后解压下载的另一个配置文件，直接拷贝覆盖即可</li>
</ul>
<p>这里卡了好久，因为文件路径太长经常解压后部分文件无法解压成功，这个可以参考链接：<a href="https://knowledge.autodesk.com/zh-hans/search-result/caas/sfdcarticles/sfdcarticles/CHS/The-Windows-10-default-path-length-limitation-MAX-PATH-is-256-characters.html" target="_blank" rel="noopener">点这里</a>，最好放在盘的第一层，我就放在C:\下面</p>
<ul>
<li><p>配置hadoop环境变量</p>
<p>我的电脑-&gt;属性-&gt;高级系统设置-&gt;环境变量-&gt;系统变量</p>
<p>新建HADOOP_HOME, 我的配置：C:\hadoop-2.10.0\bin</p>
<p><img src="https://i.loli.net/2020/02/20/T9MjvyiPe8rcE5H.jpg" alt="b146837bly1gbxf3b0kv0j20s9071dfu.jpg"></p>
</li>
</ul>
<p>​     在PATH变量中添加：%HADOOP_HOME%</p>
<ul>
<li><p>编辑 hadoop安装目录下 etc/hadoop/hadoop-env.cmd, 替换JAVA_HOME路径，”D:\program files\Java\jdk1.8.0_171”为JAVA安装路径。</p>
<p>set JAVA_HOME=”D:\program files\Java\jdk1.8.0_171”</p>
<p>然后编辑在hadoop根目录下创建data目录，目录中再创建两个空文件夹datanode和namenode，之后编辑 etc/hadoop/hdfs-site.xml，替换路径/hadoop-2.10.0/为你的hadoop安装根目录</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.namenode.name.dir&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;&#x2F;hadoop-2.10.0&#x2F;data&#x2F;namenode&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.datanode.data.dir&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;&#x2F;hadoop-2.10.0&#x2F;data&#x2F;datanode&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>格式化namenode</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在任意目录执行 hdfs namenode -format</span><br></pre></td></tr></table></figure>
</li>
<li><p>到安装根目录下的sbin目录，执行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">start-all.cmd</span><br></pre></td></tr></table></figure>

<p><img src="https://i.loli.net/2020/02/20/Lupda64fCRmJEHo.jpg" alt="b146837bly1gbxfmuf908j20z50li7gm.jpg"></p>
<p>验证是否成功：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">jps</span><br></pre></td></tr></table></figure>

<p>会有以下进程在运行：</p>
<p>NodeManager<br>DataNode<br>ResourceManager<br>NameNode</p>
</li>
</ul>
<h1 id="问题及解决方法"><a href="#问题及解决方法" class="headerlink" title="问题及解决方法"></a>问题及解决方法</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">java.lang.NoClassDefFoundError: org&#x2F;apache&#x2F;hadoop&#x2F;yarn&#x2F;server&#x2F;timelineservice&#x2F;collector&#x2F;TimelineCollectorManager</span><br><span class="line">        at java.lang.ClassLoader.defineClass1(Native Method)</span><br><span class="line">        at java.lang.ClassLoader.defineClass(ClassLoader.java:763)</span><br><span class="line">        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)</span><br><span class="line">        at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)</span><br><span class="line">        at java.net.URLClassLoader.access$100(URLClassLoader.java:73)</span><br><span class="line">        at java.net.URLClassLoader$1.run(URLClassLoader.java:368)</span><br><span class="line">        at java.net.URLClassLoader$1.run(URLClassLoader.java:362)</span><br><span class="line">        at java.security.AccessController.doPrivileged(Native Method)</span><br><span class="line">        at java.net.URLClassLoader.findClass(URLClassLoader.java:361)</span><br><span class="line">        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</span><br><span class="line">        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)</span><br><span class="line">        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</span><br><span class="line">        at java.lang.Class.getDeclaredMethods0(Native Method)</span><br><span class="line">        at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)</span><br><span class="line">        at java.lang.Class.getDeclaredMethods(Class.java:1975)</span><br><span class="line">        at com.google.inject.spi.InjectionPoint.getInjectionPoints(InjectionPoint.java:688)</span><br><span class="line">        at com.google.inject.spi.InjectionPoint.forInstanceMethodsAndFields(InjectionPoint.java:380)</span><br><span class="line">        at com.google.inject.spi.InjectionPoint.forInstanceMethodsAndFields(InjectionPoint.java:399)</span><br><span class="line">        at com.google.inject.internal.BindingBuilder.toInstance(BindingBuilder.java:84)</span><br><span class="line">        at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebApp.setup(RMWebApp.java:56)</span><br><span class="line">        at org.apache.hadoop.yarn.webapp.WebApp.configureServlets(WebApp.java:160)</span><br><span class="line">        at com.google.inject.servlet.ServletModule.configure(ServletModule.java:55)</span><br><span class="line">        at com.google.inject.AbstractModule.configure(AbstractModule.java:62)</span><br><span class="line">        at com.google.inject.spi.Elements$RecordingBinder.install(Elements.java:340)</span><br><span class="line">        at com.google.inject.spi.Elements.getElements(Elements.java:110)</span><br><span class="line">        at com.google.inject.internal.InjectorShell$Builder.build(InjectorShell.java:138)</span><br><span class="line">        at com.google.inject.internal.InternalInjectorCreator.build(InternalInjectorCreator.java:104)</span><br><span class="line">        at com.google.inject.Guice.createInjector(Guice.java:96)</span><br><span class="line">        at com.google.inject.Guice.createInjector(Guice.java:73)</span><br><span class="line">        at com.google.inject.Guice.createInjector(Guice.java:62)</span><br><span class="line">        at org.apache.hadoop.yarn.webapp.WebApps$Builder.build(WebApps.java:356)</span><br><span class="line">        at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:401)</span><br><span class="line">        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startWepApp(ResourceManager.java:1137)</span><br><span class="line">        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1245)</span><br><span class="line">        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)</span><br><span class="line">        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1446)</span><br><span class="line">Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.yarn.server.timelineservice.collector.TimelineCollectorManager</span><br><span class="line">        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)</span><br><span class="line">        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</span><br><span class="line">        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)</span><br><span class="line">        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</span><br><span class="line">        ... 36 more</span><br></pre></td></tr></table></figure>

<p>*<em>解决方法： *</em>share\hadoop\yarn\timelineservice 下 hadoop-yarn-server-timelineservice-3.0.3.jar copy 到share\hadoop\yarn目录下 </p>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
        <tag>hadoop安装</tag>
        <tag>windows 10</tag>
      </tags>
  </entry>
  <entry>
    <title>基础知识</title>
    <url>/2020/02/12/linux-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
    <content><![CDATA[<h1 id="计算机"><a href="#计算机" class="headerlink" title="计算机"></a>计算机</h1><h3 id="组成部分"><a href="#组成部分" class="headerlink" title="组成部分"></a>组成部分</h3><ul>
<li>输入单元</li>
<li>CPU<a id="more"></a></li>
<li>内存</li>
<li>外部存储设备</li>
<li>输出单元</li>
</ul>
<h3 id="分类"><a href="#分类" class="headerlink" title="分类"></a>分类</h3><ul>
<li>超级计算机</li>
<li>大型计算机</li>
<li>迷你计算机</li>
<li>工作站</li>
<li>微电脑</li>
</ul>
<h3 id="文件大小"><a href="#文件大小" class="headerlink" title="文件大小"></a>文件大小</h3><p>B=&gt;K=&gt;M=&gt;G=&gt;T=&gt;P=&gt;E</p>
<p>关系都是1024的倍数，如1M=1024K</p>
<h1 id="帮助"><a href="#帮助" class="headerlink" title="帮助"></a>帮助</h1><p>因为传统linux系统只有命令行模式，所以需要使用各种命令，但是记住所有的命令和参数这是几乎不可能的，所以知道如何获取帮助非常重要。</p>
<p>获取帮助的命令主要有三个，help, man, info。这三个得到的查询结果详细程度依次递增。</p>
<p><strong>help命令</strong></p>
<p>help 命令经常使用，可以简洁的列出命令使用方法</p>
<p>示例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">help echo</span><br></pre></td></tr></table></figure>



<p><img src="https://i.loli.net/2020/02/20/jNaxOQpHEnBAWZy.jpg" alt="b146837bly1gbsy3grssnj21fd09qwff.jpg"></p>
]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>基础知识</tag>
      </tags>
  </entry>
  <entry>
    <title>linux---目录</title>
    <url>/2020/02/09/Linux%E7%9B%AE%E5%BD%95/</url>
    <content><![CDATA[<p>linux系统是一种自由和开放源码的类UNIX操作系统。该操作系统的内核由林纳斯·托瓦兹在1991年10月5日首次发布，在加上用户空间的应用程序之后，成为Linux操作系统[1]。linux系统作为当前最流行的系统之一，有各种版本，如centos, ubuntu等，当前移动互联网终端之王—手机—的系统之一Android也是基于linux，可以说随处可以见到linux的身影。</p>
<p>在大数据开发中，linux系统必然会涉及，是必备的基础技能。下面将分为以下一系列文章进行介绍：</p>
<ul>
<li><a href="/2020/02/12/linux-%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/" title="基础知识">基础知识</a></li>
<li>常用命令</li>
<li>文件权限</li>
<li>shell脚本</li>
</ul>
<blockquote>
<p>[1] 维基百科</p>
</blockquote>
]]></content>
      <categories>
        <category>目录</category>
      </categories>
      <tags>
        <tag>linux目录</tag>
      </tags>
  </entry>
  <entry>
    <title>RDD转换</title>
    <url>/2019/11/02/RDD%E8%BD%AC%E6%8D%A2/</url>
    <content><![CDATA[<h1 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h1><p>RDD(Resilient Distributed Datasets，弹性分布式数据集)，是一个惰性计算、静态类型的分布式数据集合，是Spark实现并发计算的基础数据结构。<a id="more"></a></p>
<p>RDD包含以下特性（前3个必须有，后两个可选）：</p>
<ol>
<li><p>partitions()</p>
<p>返回组成分布式数据集的分区对象数组。</p>
</li>
<li><p>itearator(p, parentIters)</p>
<p>为每个父分区计算分区p的iteartors。</p>
</li>
<li><p>dependencies</p>
<p>返回依赖对象序列。</p>
</li>
<li><p>partitioner()—可选</p>
<p>若RDD有相关元素与分区信息，则返回Scala option type的分区对象。</p>
</li>
<li><p>prefferedLocations(p)—可选</p>
<p>返回数据分区的存储位置信息。</p>
</li>
</ol>
<p>针对RDD的处理有丰富的操作，包括Action算子和Transformation算子。Action操作返回对象非RDD，Transformation操作返回仍然是RDD。Action算子是触发行动的算子，<strong>Action算子数量等于Spark Job的数量</strong>；Transformation算子会对RDD进行变换，根据父RDD和子RDD的依赖关系不同，Transformation又分为宽依赖和窄依赖。宽依赖的示意图如下：</p>
<p><img src="https://i.loli.net/2019/11/03/5EpJ6In4DhlVSxj.png" alt="image.png"></p>
<p>窄依赖的严格定义：<strong>each partition of the parrent RDD is used by at most one partition of the child RDD（译：每个父RDD的分区最多被一个子RDD分区使用）</strong>。</p>
<p>这是区分宽窄依赖最严格的定义，还有一个并不是非常严格的说法，但是便于理解：</p>
<p>需要进行shuffle的为宽依赖，不需要的为窄依赖。</p>
<p><strong>Spark Job中的Stage个数就等于宽依赖个数。</strong></p>
<p>常用的Action算子有reduce,collect,count,first,take,aggregate, fold, foreach, saveAsTextFile, countByKey等; 常用的Transformation算子有map, filter, flatmap, mapPartitions, sample, union, intersection, distinct, groupByKey, countByKey, sortBy, join, coalesce, repartition, repartitionAndSortWithinPatitions, mapValues等。</p>
<h1 id="Spark-Job阶段划分"><a href="#Spark-Job阶段划分" class="headerlink" title="Spark Job阶段划分"></a>Spark Job阶段划分</h1><p><img src="https://i.loli.net/2019/11/02/Iw4YD79qiK2h6kp.jpg" alt=""></p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>High Performance Spark</tag>
        <tag>RDD</tag>
      </tags>
  </entry>
  <entry>
    <title>spark---目录</title>
    <url>/2019/11/02/spark%E7%9B%AE%E5%BD%95/</url>
    <content><![CDATA[<p>Spark作为一个开源大数据分析引擎，分布式、计算速度快是其显著的优势，特别适用于机器学习（ML）等计算中需要迭代的算法。下面将主要针对Spark Core部分，分为以下一系列文章进行介绍：</p>
<ul>
<li><a href="/2019/10/24/spark/" title="Spark绪论">Spark绪论</a></li>
<li><a href="/2019/11/02/spark%E6%9E%B6%E6%9E%84/" title="Spark架构">Spark架构</a></li>
<li><a href="/2019/11/02/RDD%E8%BD%AC%E6%8D%A2/" title="RDD转换">RDD转换</a></li>
<li>键值对处理</li>
</ul>
<blockquote>
<p>参考书目：high performance spark, Holden karau &amp; Rachel Warren</p>
</blockquote>
]]></content>
      <categories>
        <category>目录</category>
      </categories>
      <tags>
        <tag>spark目录</tag>
      </tags>
  </entry>
  <entry>
    <title>spark架构</title>
    <url>/2019/11/02/spark%E6%9E%B6%E6%9E%84/</url>
    <content><![CDATA[<h1 id="Spark架构"><a href="#Spark架构" class="headerlink" title="Spark架构"></a>Spark架构</h1><p><img src="https://i.loli.net/2019/11/02/TLMwH9aV1xStu5Y.png" alt=""></p>
<a id="more"></a>

<p>一个sparkApplication对应 Driver程序中一个SparkContext，而SparkContext由一系列Spark job组成。一个Worker Node可以由多个Executer组成，但是Executer不能跨Worker Node。</p>
<h1 id="spark数据处理系统"><a href="#spark数据处理系统" class="headerlink" title="spark数据处理系统"></a>spark数据处理系统</h1><p> Spark可以在仅有单个JVM的单台机器上运行，但更常与分布式存储系统和集群管理器串联组成如下数据处理系统。分布式存储系统用来存放数据，集群管理器用来协调管理集群spark任务。Spark目前支持4种集群管理器：Standalone集群管理器，Apache Mesos，Hadoop YARN，EC2。</p>
<p><img src="https://i.loli.net/2019/11/02/5ijlposeYMDScIU.png" alt="Spark"></p>
<h1 id="spark生态系统"><a href="#spark生态系统" class="headerlink" title="spark生态系统"></a>spark生态系统</h1><p><img src="https://i.loli.net/2019/11/02/B2JjgU7hczEba1r.png" alt="spark生态系统"></p>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>High Performance Spark</tag>
        <tag>spark架构</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark绪论</title>
    <url>/2019/10/24/spark/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><ul>
<li><p>为什么会有spark</p>
<p>现在的计算机性能越来越高，单台机器处理M级别的数据简直小菜一碟。但是如果数据量更大，达到G、T甚至P、E级别，这时单台机器的性能可能就有点不够了。也许你会反驳，目前世界上最快的超级计算<a id="more"></a> 机“Summit”浮点运算速度峰值可达20亿亿次/秒，这相当快了吧！确实很快，但是它的使用和维护费用都很高呀，一般人或者说公司根本负担不起。所以聪明的人类就发明了大数据工具。虽然单台机器的计算性能可能并不是很好，但是如果多台机器联合起来一起做计算，那么不就可以实现相对廉价的机器实现超大数据规模的计算吗？所以诞生了大数据工具Hadoop。Hadoop基于HDFS和Map-Reduce奠定了大数据领域的霸主地位。随后各种工具层出不穷。Spark针对Hadoop计算中慢的缺点，即每次计算结果都要读写到磁盘导致速度变慢的设计，作了如下改进：</p>
<ol>
<li>计算中间结果保存在内存中。Hadoop每次的计算结果都会放回HDFS，这种设计严重影响了计算速度。spark基于内存的设计则改进了这一点。</li>
<li>惰性计算。Spark使用有向无环图（DAG）调度机制，遇到action操作算子才进行实际计算，所有就有了优化空间。</li>
</ol>
<blockquote>
<p>可参考对比：<a href="https://www.zhihu.com/question/26568496" target="_blank" rel="noopener">https://www.zhihu.com/question/26568496</a></p>
</blockquote>
</li>
<li><p>Spark是什么</p>
<p>官方定义：<strong>Apache Spark™</strong> is a unified analytics engine for large-scale data processing。翻译过来就是：用于大规模数据处理的分析引擎。再直白一点就是说：spark是用于快速处理大量数据的工具。</p>
<img src="https://ericfu.me/images/2018/06/spark-banner.png" width="700" hegiht="113" align=center />



</li>
</ul>
<h1 id="和其他工具对比"><a href="#和其他工具对比" class="headerlink" title="和其他工具对比"></a>和其他工具对比</h1><blockquote>
<p>引用自：<a href="https://www.boxuegu.com/news/458.html" target="_blank" rel="noopener">https://www.boxuegu.com/news/458.html</a></p>
</blockquote>
<ul>
<li><p><strong>Hadoop框架</strong></p>
<p>提起大数据，第一个想起的肯定是Hadoop，因为Hadoop是目前世界上应用最广泛的大数据工具，它凭借极高的容错率和极低的硬件价格，在大数据市场上风生水起。Hadoop还是第一个在开源社区上引发高度关注的批处理框架，他提出的Map和Reduce的计算模式简洁而优雅。迄今为止，Hadoop已经成为了一个广阔的生态圈，实现了大量算法和组件。由于Hadoop的计算任务需要在集群的多个节点上多次读写，因此在速度上会稍显劣势，但是其吞吐量也同样是其他框架所不能匹敌的。</p>
</li>
</ul>
<ul>
<li><strong>Storm框架</strong><br>与Hadoop的批处理模式不同，Storm采用的是流计算框架，由Twitter开源并且托管在GitHub上。与Hadoop类似的是，Storm也提出了两个计算角色，分别为Spout和Bolt。如果说Hadoop是水桶，只能一桶一桶的去井里扛，那么Storm就是水龙头，只要打开就可以源源不断的出水。Storm支持的语言也比较多，Java、Ruby、Python等语言都能很好的支持。由于Storm是流计算框架，因此使用的是内存，延迟上有极大的优势，但是Storm不会持久化数据。</li>
</ul>
<ul>
<li><strong>Samza框架</strong><br>Smaza也是一种流计算框架，但他目前只支持JVM语言，灵活度上略显不足，并且Samza必须和Kafka共同使用。但是响应的，其也继承了Kafka的低延时、分区、避免回压等优势。对于已经有Hadoop+Kafka工作环境的团队来说，Samza是一个不错的选择，并且Samza在多个团队使用的时候能体现良好的性能。</li>
</ul>
<ul>
<li><strong>Spark框架</strong><br>Spark属于前两种框架形式的集合体，是一种混合式的计算框架。它既有自带的实时流处理工具，也可以和Hadoop集成，代替其中的MapReduce，甚至Spark还可以单独拿出来部署集群，但是还得借助HDFS等分布式存储系统。Spark的强大之处在于其运算速度，与Storm类似，Spark也是基于内存的，并且在内存满负载的时候，硬盘也能运算，运算结果表示，Spark的速度大约为Hadoop的一百倍，并且其成本可能比Hadoop更低。但是Spark目前还没有像Hadoop哪有拥有上万级别的集群，因此现阶段的Spark和Hadoop搭配起来使用更加合适。</li>
</ul>
<ul>
<li><strong>Flink框架</strong><br>Flink也是一种混合式的计算框架，但是在设计初始，Fink的侧重点在于处理流式数据，这与Spark的设计初衷恰恰相反，而在市场需求的驱使下，两者都在朝着更多的兼容性发展。Flink目前不是很成熟，更多情况下Flink还是起到一个借鉴的作用。</li>
</ul>
]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>High Performance Spark</tag>
        <tag>大数据工具</tag>
      </tags>
  </entry>
</search>
